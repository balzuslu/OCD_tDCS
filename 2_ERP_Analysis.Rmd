---
title: "ERP Analysis"
output: 
  html_document

---

<!-- Set general settings -->

```{r setup, include = FALSE}

# Set general settings for markdown file
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  comment = "",
  results = "hold"
)


# Clear environment
rm(list = ls())


# Enable/disable caching of time-consuming code chunks
knitr_cache_enabled = TRUE


# Load packages
library(dplyr)      # for data manipulation
library(knitr)      # for integrating computing and reporting in markdown
library(kableExtra) # for customizing appearance of tables
library(ggplot2)    # for plotting
library(cowplot)    # for arranging plots
library(e1071)      # for functions skewness and kurtosis
library(MASS)       # for boxcox function and contrast definition
library(lme4)       # for (G)LMMs
library(lmerTest)   # for LMM p values (Satterthwaite's method for approximating dfs for the t and F tests)
library(sjPlot)     # for tab_model function to display (G)LMM results
library(performance)# for check of model various assumptions
library(emmeans)    # for pairwise comparisons
library(afex)       # for ANOVAs (convenience functions, e.g. for nice display)
library(effectsize) # for effect sizes (t_to_d function)
library(tidyr)      # for reshape function
library(readr)      # for raincloud plots
library(lavaan)     # for raincloud plots
library(splithalf)  # for permutation-based split-half reliability
library(TOSTER)     # for equivalence test on t test


# Load functions
source("./functions/summarySEwithinO.R")  # Function provided by R-cookbook: http://www.cookbook-r.com/Graphs/Plotting_means_and_error_bars_(ggplot2)/
source("./functions/my_table_template.R") # Function to create table template
source("./functions/R_rainclouds.R")      # Function to create raincloud plots


# Turn off scientific notation
options(scipen = 999)


# Set figure theme and colors
my_figure_theme <- theme_classic(base_size = 11) +
  theme(legend.position = "bottom", 
        strip.background = element_rect(fill="grey95", linetype = "blank"),
        axis.ticks.x = element_blank(), 
        plot.title = element_text(hjust = 0.5)) 
# instad of theme_classic: + theme_apa(base_size = 11)

my_figure_colors <- c("#8ea6b4", "#465369")
```
<br><br> 

## Data Cleaning
***

```{r load-and-clean-data}

# Load data
load(file = "./data/Single_Trial_Data.rda")
load(file = "./data/Feedback_Infos.rda")


# Exclude P_02 (due to retainer) and C_02 (as preregistered: patients are excluded with their match)
single_trial_data <- single_trial_data[single_trial_data$participant_id != "P_02" & single_trial_data$participant_id != "C_02",]
feedback_infos    <- feedback_infos[feedback_infos$participant_id != "P_02" & feedback_infos$participant_id != "C_02",]


# Exclude missing responses, RT outliers and trials with ERP artifacts
single_trial_data_clean <- single_trial_data %>%
  dplyr::filter(
      response_type != "miss" &
      rt_invalid  == FALSE &
      !is.na(MFN_0_100_FCz)
  ) # (53093 of 53760 trials left)


# Add column for (grand mean standardized) number of errors (needed as covariate later) - this variable contains total number of errors, not only those entering analysis (preferred according to JK)
single_trial_data_clean <- single_trial_data %>% 
  dplyr::group_by(participant_id, session) %>% 
  dplyr::summarize(number_errors = sum(response_type == "incorrect")) %>%
  dplyr::ungroup(.) %>%
  dplyr::mutate(number_errors_standardized = scale(number_errors, center = TRUE, scale = TRUE)) %>%
  dplyr::left_join(single_trial_data_clean, ., by = c("participant_id", "session"))


# Add column for (grand mean standardized) number of speeding (needed as covariate later) 
single_trial_data_clean <- feedback_infos[feedback_infos$block != 6,] %>% 
  dplyr::group_by(participant_id, session, feedback) %>%
  dplyr::count(feedback, .drop = FALSE) %>%
  dplyr::filter(feedback == " schneller") %>%
  dplyr::rename(number_feedback_faster = n) %>%
  dplyr::ungroup(.) %>%  
  dplyr::select(-feedback) %>%
  dplyr::mutate(number_feedback_faster_standardized = scale(number_feedback_faster, center = TRUE, scale = TRUE)) %>%
  dplyr::left_join(single_trial_data_clean, ., by = c("participant_id", "session")) 


# Add column for medication (needed as covariate later) 
single_trial_data_clean <- single_trial_data_clean %>% 
  dplyr::mutate(medication = as.factor(ifelse(participant_id == "P_02" | participant_id == "P_04" | participant_id == "P_05" |
                                              participant_id == "P_06" | participant_id == "P_08" | participant_id == "P_10" |
                                              participant_id == "P_15" | participant_id == "P_16" | participant_id == "P_18" |
                                              participant_id == "P_22" | participant_id == "P_25" | participant_id == "P_26" |
                                              participant_id == "P_28" | participant_id == "P_30", "yes", "no")))


# Add column for (within-participant standardized) baseline EEG, N2, P3 (needed as covariate later) 
single_trial_data_clean <- single_trial_data_clean %>% 
  dplyr::group_by(participant_id, session) %>%
  dplyr::mutate(
    MFN_baseline_pre_200_0_FCz_standardized = scale(MFN_baseline_pre_200_0_FCz, center = TRUE, scale = TRUE),
    Pe_baseline_pre_200_0_Pz_standardized   = scale(Pe_baseline_pre_200_0_Pz, center = TRUE, scale = TRUE),
    N2_200_300_FCz_standardized             = scale(N2_200_300_FCz, center = TRUE, scale = TRUE),
    P3_300_500_CPz_standardized             = scale(P3_300_500_CPz, center = TRUE, scale = TRUE))  %>%
  dplyr::ungroup()


# Calculate aggregated data per participant for boxplots, ANOVAs, t tests, and outlier detection
df_aggregated_per_subject_MFN <- single_trial_data_clean %>%
  dplyr::group_by(participant_id, group, response_type, stimulation, session) %>%
  dplyr::summarize(
    MFN = mean(MFN_0_100_FCz, na.rm = TRUE)
   )  %>%
  dplyr::ungroup()


# Calculate aggregated data per participant for ANOVA 
df_aggregated_per_subject_ANOVA <- single_trial_data_clean %>%
  dplyr::group_by(participant_id, group, stimulation, session) %>%
  dplyr::summarize(
    CRN_mean_amplitude_0_100_FCz = mean(MFN_0_100_FCz[response_type == "correct"],   na.rm = TRUE),
    ERN_mean_amplitude_0_100_FCz = mean(MFN_0_100_FCz[response_type == "incorrect"], na.rm = TRUE)
    )  %>%
  dplyr::mutate(
    difference_ERN_CRN_mean_amplitude_0_100_FCz = ERN_mean_amplitude_0_100_FCz - CRN_mean_amplitude_0_100_FCz
  ) %>%
  dplyr::ungroup() 


# Make categorical variables factors
single_trial_data_clean$participant_id      <- as.factor(single_trial_data_clean$participant_id) 
single_trial_data_clean$group               <- as.factor(single_trial_data_clean$group)
single_trial_data_clean$session             <- as.factor(single_trial_data_clean$session)
single_trial_data_clean$stimulation         <- as.factor(single_trial_data_clean$stimulation)
single_trial_data_clean$stimulus_type       <- as.factor(single_trial_data_clean$stimulus_type)
single_trial_data_clean$response_type       <- as.factor(single_trial_data_clean$response_type)


# Load files with ERN quantified as area amplitude (replicating Reinhart & Woodman, 2014)
# Note that these files also contain P_02 and C_02 but these are dropped when joining with df_aggregated_per_subject_ANOVA
integral_area_amplitude <- read.table("./data/integral_area_amplitude_ERN_and_difference_ERN_CRN_-50_to_150_ms_at_FCz.txt", header = TRUE, stringsAsFactors = TRUE)
negative_area_amplitude <- read.table("./data/negative_area_amplitude_ERN_and_difference_ERN_CRN_-50_to_150_ms_at_FCz.txt", header = TRUE, stringsAsFactors = TRUE)


# Clean df for ERN quantified as area amplitude: Rename columns, create columns for merging, select relevant columns
df_aggregated_per_subject_ANOVA <- left_join(integral_area_amplitude, negative_area_amplitude, by = "ERPset") %>% 
  dplyr::rename(ERN_integral_area_amplitude_pre_50_150_FCz                 = bin2_incorrect__FCz.x,
                difference_ERN_CRN_integral_area_amplitude_pre_50_150_FCz  = bin10_difference_incorrect_._correct_FCz.x,
                ERN_negative_area_amplitude_pre_50_150_FCz                 = bin2_incorrect__FCz.y,
                difference_ERN_CRN_negative_area_amplitude_pre_50_150_FCz  = bin10_difference_incorrect_._correct_FCz.y) %>% 
  dplyr::mutate(session        = as.factor(ifelse(substr(ERPset, 6, 7) == "T1", "T1", "T2")),
                participant_id = as.factor(substr(ERPset, 1, 4))) %>%
  dplyr::select(c("participant_id", "session", 
                  "ERN_integral_area_amplitude_pre_50_150_FCz", "difference_ERN_CRN_integral_area_amplitude_pre_50_150_FCz",
                  "ERN_negative_area_amplitude_pre_50_150_FCz", "difference_ERN_CRN_negative_area_amplitude_pre_50_150_FCz")) %>%
  # Merge with df_aggregated_per_subject_ANOVA
  dplyr::left_join(df_aggregated_per_subject_ANOVA, ., by = c("participant_id", "session"))

```

Trials were excluded from all analyses if RT was shorter than 100 ms or longer than 800 ms or if the response in a trial was missing. We further discarded trials containing artifacts in the EEG, i.e., a voltage difference exceeding 50 μV between two consecutive sampling points or 200 μV within an epoch. 

```{r excluded-trials}

# Calculate percentage of excluded trials per participant
excluded_trials_per_participant <- single_trial_data %>%
  dplyr::group_by(group, participant_id, session) %>%
  dplyr::summarize(
    invalid_rt   = sum(!is.na(rt_invalid) & rt_invalid != FALSE) / length(participant_id) * 100,
    misses       = sum(response_type == "miss") / length(participant_id) * 100,
    EEG_artifact = sum(is.nan(MFN_0_100_FCz)) / length(participant_id) * 100 # is.nan -> trials lost due trigger miss have NA and thus do not count here
   ) %>%
  dplyr::ungroup() 


# Summarize percentage of excluded trials per participant over groups
excluded_trials_per_participant_over_groups <- excluded_trials_per_participant %>%
  # Calculate M and SD of the variables
  dplyr::summarize(across(-c(group, participant_id, session), list(mean,sd,min,max)))  


# Summarize percentage of excluded trials per participant per group 
excluded_trials_per_participant_per_group <- excluded_trials_per_participant %>%
  # Calculate M and SD of the variables
  dplyr::group_by(group) %>%
  dplyr::summarize(across(-c(participant_id, session), list(mean,sd,min,max))) %>%
  dplyr::ungroup()   


# Display percentage of excluded trials per participant over groups
my_table_template(excluded_trials_per_participant_over_groups,
  caption = "Excluded Trials (M and SD in %) per Participant", 
  col_names = c("M", "SD", "min", "max", "M", "SD", "min", "max", "M", "SD", "min", "max"),
  header_above_config = c("RT < 100 / > 800 ms" = 4, "Misses" = 4, "EEG artifact" = 4)
)


# Display percentage of excluded trials per participant per group
my_table_template(excluded_trials_per_participant_per_group,
  caption = "Excluded Trials (M and SD in %) per Participant per Group", 
  col_names = c("Group", "M", "SD", "min", "max", "M", "SD", "min", "max", "M", "SD", "min", "max"),
  header_above_config = c(" " = 1, "RT < 100 / > 800 ms" = 4, "Misses" = 4, "EEG artifact" = 4)
)


# Detect ERP outliers (ERP deviates more than 2/3 SD below/above group mean per condition)
ERP_outliers <- df_aggregated_per_subject_MFN %>% 
  dplyr::group_by(group, response_type, stimulation) %>%
  dplyr::mutate(outlier_2_sd = case_when(abs(MFN - mean(MFN, na.rm = TRUE)) <=  2 * sd(MFN, na.rm = TRUE)~ FALSE, TRUE ~ TRUE),
                outlier_3_sd = case_when(abs(MFN - mean(MFN, na.rm = TRUE)) <=  3 * sd(MFN, na.rm = TRUE)~ FALSE, TRUE ~ TRUE)) %>%
  dplyr::filter(outlier_2_sd == TRUE)  %>%
  dplyr::ungroup()


# Display ERP outliers
my_table_template(ERP_outliers, caption = "ERP outliers (> 2/3 SD below/above group mean per condition (response type x stimulation))")
```
There is one participant (P_28) whose ERN deviates > 3 SD from the group mean per condition (response type x stimulation). This participant is also quite prominent in the raincloud plot and line plot (see below). There are a few participants whose ERN deviates > 2 SD. I did not exclude participants based on this criterion. Being an outlier in ERP magnitude is also no exclusion criterion specified in the preregistration. 
<br><br>

## Data Inspection {.tabset}
***

### Distribution

```{r inspect-distribution, cache = knitr_cache_enabled}

# Plot distribution MFN
hist_MFN <- ggplot(single_trial_data_clean, aes(x = MFN_0_100_FCz)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "#8ea6b4", size = 1) +
  stat_function(fun = dnorm, args=list(mean = mean(single_trial_data_clean$MFN_0_100_FCz, na.rm = TRUE), 
                                     sd = sd(single_trial_data_clean$MFN_0_100_FCz, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(MFN_0_100_FCz, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram MFN", x = "MFN", y = "Density") + 
  my_figure_theme 

qqplot_MFN <- ggplot(single_trial_data_clean, aes(sample = MFN_0_100_FCz)) +
  stat_qq(color = "#8ea6b4") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot MFN", x = "Theoretical Quantiles", y = "Sample Quantiles") + 
  my_figure_theme


# Plot distribution ERN
hist_ERN <- ggplot(single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",], aes(x = MFN_0_100_FCz)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "#8ea6b4", size = 1) +
  stat_function(fun = dnorm, args=list(mean = mean(single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",]$MFN_0_100_FCz, na.rm = TRUE), 
                                     sd = sd(single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",]$MFN_0_100_FCz, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(MFN_0_100_FCz, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram ERN", x = "ERN", y = "Density") + 
  my_figure_theme

qqplot_ERN <- ggplot(single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",], aes(sample = MFN_0_100_FCz)) +
  stat_qq(color = "#8ea6b4") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot ERN", x = "Theoretical Quantiles", y = "Sample Quantiles") + 
  my_figure_theme


# Plot distribution CRN
hist_CRN <- ggplot(single_trial_data_clean[single_trial_data_clean$response_type == "correct",], aes(x = MFN_0_100_FCz)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "#8ea6b4", size = 1) +
  stat_function(fun = dnorm, args=list(mean = mean(single_trial_data_clean[single_trial_data_clean$response_type == "correct",]$MFN_0_100_FCz, na.rm = TRUE), 
                                     sd = sd(single_trial_data_clean[single_trial_data_clean$response_type == "correct",]$MFN_0_100_FCz, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(MFN_0_100_FCz, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram CRN", x = "CRN", y = "Density") + 
  my_figure_theme

qqplot_CRN <- ggplot(single_trial_data_clean[single_trial_data_clean$response_type == "correct",], aes(sample = MFN_0_100_FCz)) +
  stat_qq(color = "#8ea6b4") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot CRN", x = "Theoretical Quantiles", y = "Sample Quantiles") + 
  my_figure_theme

ggdraw() +
  draw_plot(hist_MFN,       x =  0,   y = .66,   width = .5, height = .33) +
  draw_plot(qqplot_MFN,     x =  .5,  y = .66,   width = .5, height = .33) +
  draw_plot(hist_ERN,       x =  0,   y = .33,   width = .5, height = .33) +
  draw_plot(qqplot_ERN,     x =  .5,  y = .33,   width = .5, height = .33) +
  draw_plot(hist_CRN,       x =  0,   y = 0,     width = .5, height = .33) +
  draw_plot(qqplot_CRN,     x =  .5,  y = 0,     width = .5, height = .33) 
```
<br><br>

### ERN/CRN per participant 

```{r plot-MFN-per-participant-and-response-type, fig.width = 12, fig.height = 20, cache = knitr_cache_enabled}

MFN_per_participant <- ggplot(single_trial_data_clean, aes(x = response_type, y = MFN_0_100_FCz, group = response_type)) + 
  geom_point(aes(fill = response_type), color = "black", shape = 21, position = "jitter") + 
  ggtitle("MFN per participant") + 
  my_figure_theme + 
  facet_wrap(~ participant_id + stimulation, ncol = 10) +
  scale_fill_manual(values = my_figure_colors) 
MFN_per_participant
```
<br><br>

### Check Normality 

For the single-trial data, Shapiro-Wilk is not suitable, as it always returns a significant result for such large samples (additionally, it can handle only samples up to 5000). Hence, we have to rely on visual inspection (see tab "Distribution") and values of skewness and kurtosis (see below). Values for skewness and kurtosis between -2 and +2 are considered acceptable in order to prove normal univariate distribution (George & Mallery, 2010).

```{r normality-single-trial-MFN}

normality_MFN <- round(data.frame(matrix(c(skewness(single_trial_data_clean$MFN_0_100_FCz),
                                           kurtosis(single_trial_data_clean$MFN_0_100_FCz),
                                           skewness(single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",]$MFN_0_100_FCz),
                                           kurtosis(single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",]$MFN_0_100_FCz),
                                           skewness(single_trial_data_clean[single_trial_data_clean$response_type == "correct",]$MFN_0_100_FCz),
                                           kurtosis(single_trial_data_clean[single_trial_data_clean$response_type == "correct",]$MFN_0_100_FCz)),
                                        nrow=2, ncol = 3)),digits = 1)
rownames(normality_MFN) <- c("Skewness","Kurtosis")
colnames(normality_MFN) <- c("MFN", "ERN", "CRN")

my_table_template(normality_MFN, row_names = TRUE)
```
<br><br>

### Determine transformation

LMM analysis of MFN/ERN/CRN will be conducted on untransformed values, as it seems that the assumption of normally distributed residuals will be met. The appropriate transformation was determined using the Box–Cox procedure (Box & Cox, 1964). 

```{r MFN-determine-transformation, fig.width = 8, fig.height = 3}

# Arrange plots
par(mfrow = c(1, 4)) 

# Determine transformation of MFN by estimating optimal lambda using Box–Cox procedure
bc_MFN <- boxcox(MFN_0_100_FCz + 100 ~ 1, data = single_trial_data_clean)
optlambda_MFN <- bc_MFN$x[which.max(bc_MFN$y)]

# Determine transformation of ERN by estimating optimal lambda using Box–Cox procedure
bc_ERN <- boxcox(MFN_0_100_FCz + 100 ~ 1, data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",])
optlambda_ERN <- bc_ERN$x[which.max(bc_ERN$y)]

# Determine transformation of CRN by estimating optimal lambda using Box–Cox procedure
bc_CRN <- boxcox(MFN_0_100_FCz + 100 ~ 1, data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",])
optlambda_CRN <- bc_CRN$x[which.max(bc_CRN$y)]

# Determine transformation of Pe by estimating optimal lambda using Box–Cox procedure
bc_Pe <- boxcox(Pe_200_400_Pz + 120 ~ 1, data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",])
optlambda_Pe <- bc_Pe$x[which.max(bc_Pe$y)]

# Reset plot layout
par(mfrow = c(1, 1)) 
```
For MFN (left plot), the optimal lambda is `r round(optlambda_MFN, digits = 2)`, suggesting that no transformation (for lambda = 1) is needed.
For ERN (middle left plot), the optimal lambda is `r round(optlambda_ERN, digits = 2)`, suggesting that no transformation (for lambda = 1) is needed.
For CRN (middle right plot), the optimal lambda is `r round(optlambda_CRN, digits = 2)`, suggesting that no transformation (for lambda = 1) is needed.
For Pe (right plot), the optimal lambda is `r round(optlambda_Pe, digits = 2)`, suggesting that no transformation (for lambda = 1) is needed.
<br><br>

### Split-half reliability

```{r permutation-split-half-reliability, cache = knitr_cache_enabled}

# Calculate permutation-based split-half internal consistency for ERN/CRN
# use ""invisible(capture.output())" to avoid having ugly console output in html
invisible(capture.output(split_half_reliability_MFN <- splithalf(data = single_trial_data_clean,
                                    outcome = "RT",
                                    score = "average",
                                    permutations = 5000,
                                    halftype = "random",
                                    var.RT = "MFN_0_100_FCz",
                                    var.trialnum = "trial",
                                    var.participant = "participant_id",
                                    var.condition = "response_type",
                                    conditionlist = c("correct","incorrect"),
                                    average = "mean",
                                    round.to = 2)))

# Display permutation-based split-half internal consistency
my_table_template(split_half_reliability_MFN$final_estimates, caption = "Permutation-based split-half reliability ERN/CRN", digits = 3)


# Calculate permutation-based split-half internal consistency for Pe
# use ""invisible(capture.output())" to avoid having ugly console output in html
invisible(capture.output(split_half_reliability_Pe <- splithalf(data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
                                    outcome = "RT",
                                    score = "average",
                                    permutations = 5000,
                                    halftype = "random",
                                    var.RT = "Pe_200_400_Pz",
                                    var.trialnum = "trial",
                                    var.participant = "participant_id",
                                    average = "mean",
                                    round.to = 2)))

# Display permutation-based split-half internal consistency
my_table_template(split_half_reliability_Pe$final_estimates, caption = "Permutation-based split-half reliability Pe", digits = 3)
```
The internal consitency of ERN and CRN was estimated using a permutation-based split-half approach (Parsons 2020) with 5000 random splits. The (Spearman-Brown corrected) split-half internal consistency of ERN, CRN, and Pe are excellent. The columns SB_low and SB_high represent 95% confidence intervall limits. The spearman-brown corrected reliability estimate for the ERN was .955, 95% CI [.932, .972], for CRN .995, 95% CI [.993, .997], for Pe .940, 95% CI [.912, .963]. The splithalf package was built to deal with RTs. However, the principle should also apply to ERP values, so the results should be correct. However, I also calculated the odd-even split-half reliabilty to compare the results (see below). In the manuscript, we will report the permutation-based split-half results.
<br><br>

```{r odd-even-split-half-reliability}

# Code odd and even trials for reliability 
single_trial_data_clean$oddeven <- single_trial_data_clean$trial %% 2


# Calculate mean CRN for odd and even trials per participant and session
internal_consistency_CRN <- single_trial_data_clean[single_trial_data_clean$response_type == "correct",] %>%
  dplyr::group_by(participant_id, session, oddeven) %>% 
  dplyr::summarise(mean_CRN = mean(MFN_0_100_FCz, na.rm=TRUE)
)  %>%
  dplyr::ungroup()


# Calculate mean ERN/Pe for odd and even trials per participant and session
internal_consistency_ERN <- single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",] %>%
  dplyr::group_by(participant_id, session, oddeven) %>% 
  dplyr::summarise(mean_ERN = mean(MFN_0_100_FCz, na.rm=TRUE),
                   mean_Pe  = mean(Pe_200_400_Pz, na.rm=TRUE)
)  %>%
  dplyr::ungroup()


# Correlating scores from even and odd items
r_CRN <- cor.test(internal_consistency_CRN$mean_CRN[internal_consistency_CRN$oddeven==1], internal_consistency_CRN$mean_CRN[internal_consistency_CRN$oddeven==0])
r_ERN <- cor.test(internal_consistency_ERN$mean_ERN[internal_consistency_ERN$oddeven==1], internal_consistency_ERN$mean_ERN[internal_consistency_ERN$oddeven==0])
r_Pe  <- cor.test(internal_consistency_ERN$mean_Pe[internal_consistency_ERN$oddeven==1],  internal_consistency_ERN$mean_Pe[internal_consistency_ERN$oddeven==0])


# Adjusting with the Spearman-Brown prophecy formula
r_CRN_SB <- (2 * r_CRN$estimate) / (1 + r_CRN$estimate)
r_ERN_SB <- (2 * r_ERN$estimate) / (1 + r_ERN$estimate)
r_Pe_SB  <- (2 * r_Pe$estimate)  / (1 + r_Pe$estimate)


# Display odd-even split-half reliability
oddeven_split_half_reliability <- matrix(c(r_CRN$estimate, r_ERN$estimate, r_Pe$estimate, r_CRN$conf.int[1], r_ERN$conf.int[1], r_Pe$conf.int[1], 
                                           r_CRN$conf.int[2], r_ERN$conf.int[2], r_Pe$conf.int[2], r_CRN_SB, r_ERN_SB, r_Pe_SB), ncol=4)
colnames(oddeven_split_half_reliability) <- c("splithalf_raw", "splithalf_CI_lower_limit", "splithalf_CI_upper_limit", "splithalf_spearmanbrown")
rownames(oddeven_split_half_reliability) <- c("CRN", "ERN", "Pe")
my_table_template(oddeven_split_half_reliability, row_names = TRUE, caption = "Odd-even split-half reliability", digits = 2)
```
Odd-even reliability for the ERN/CRN/Pe is also good to excellent. 
<br><br>

## Descriptive Statistics {.tabset}
***

### Means and CIs

```{r descriptive-statistics-table}

##### ERN/CRN
descriptive_statistics_MFN <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "MFN_0_100_FCz",
  withinvars    = c("response_type", "stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95) 


##### ERN/CRN without session
descriptive_statistics_MFN_no_session <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "MFN_0_100_FCz",
  withinvars    = c("response_type", "stimulation"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Format confidence interval column
  dplyr::mutate(
    ci_MFN = paste0("[", round(MFN_0_100_FCz - ci, digits = 2), 
                   ", ", round(MFN_0_100_FCz + ci, digits = 2), "]")) %>%
  # Round MFN means to two decimals
  dplyr::mutate_at("MFN_0_100_FCz", round, digits = 2) %>%    
  # Select columns to be displayed
  dplyr::select(c("group", "response_type", "stimulation", "MFN_0_100_FCz", "ci_MFN", "ci"))



# Split and re-merge MFN table to display both groups next to each other
descriptive_statistics_MFN_display <-  split(descriptive_statistics_MFN_no_session, descriptive_statistics_MFN_no_session$group)
descriptive_statistics_MFN_display <-  left_join(descriptive_statistics_MFN_display$HC, descriptive_statistics_MFN_display$OCD, 
                                                 by = c("response_type", "stimulation"))


# Display descriptive statistics for MFN (and select columns)
my_table_template(descriptive_statistics_MFN_display[,c(2:5,8:9)],
  caption = "MFN (in μV)",
  col_names = c("Response type", "Stimulation", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 2, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)
```
<br><br>

### Raincloud plot {.active}

```{r raincloud-plot, fig.width = 10, fig.height = 6, fig.cap = "Note. Response-locked ERP (ERN, CRN; 0-100 ms at FCz) in the flanker task is shown as a function of response type, stimulation condition, and group. Means and 95% confidence intervals (= lines at violin plot bottom) were calculated based on single-trial data. Boxplots and violin plots are based on data aggregated by participant. CIs are adjusted for within-participant designs as described by Morey (2008)."}


# Define facet labels
response_type.labs <- c("CRN", "ERN")
names(response_type.labs) <- c("correct", "incorrect")


# From JB: "Raincloud plots (Allen et al., 2019) show means and 95% CIs calculated with the summarySEwithin function (Morey, 2008) on single trial data, and points, and distributions for data aggregated by subject"

# Create raincloud plot MFN 
plot_MFN_raincloud <- ggplot() +
  # Add aggregated distribution, boxplot and data points
  geom_flat_violin(data = df_aggregated_per_subject_MFN, aes(x = stimulation, y = MFN, fill = group), position = position_nudge(x = .1, y = 0), 
                   adjust = 1.5, trim = FALSE, alpha = .5, colour = NA)+
  geom_point(data = df_aggregated_per_subject_MFN, aes(x = as.numeric(stimulation)-.15, y = MFN, colour = group),
             position = position_jitter(width = .05), size = 1, shape = 20)+
  geom_boxplot(data = df_aggregated_per_subject_MFN, aes(x = stimulation, y = MFN, fill = group),
               outlier.shape = NA, alpha = .5, width = .1, colour = "black")+
  # Add single-trial mean + CI
  geom_point(data = descriptive_statistics_MFN_no_session, aes(x = as.numeric(stimulation)+0.1, y = MFN_0_100_FCz, colour = group), 
             shape = 95, size = 6) +
  geom_errorbar(data = descriptive_statistics_MFN_no_session, aes(x = as.numeric(stimulation)+0.1, ymax = MFN_0_100_FCz + ci, ymin = MFN_0_100_FCz - ci), 
                colour = "black", width = 0) +
  # Add style
  scale_colour_manual(values = my_figure_colors, name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~response_type,labeller = labeller(response_type = response_type.labs)) + 
  labs(x = "\nStimulation condition", y = "MFN (mean amplitude 0-100 ms at FCz [μV])") +
  my_figure_theme 


# Save plot
 ggsave("./figures/figure_MFN_raincloud.tiff", width = 20, height = 25, units = "cm", dpi=600, compression = "lzw")


# Display plot
plot_MFN_raincloud
```
<br><br>

### Line plot participant-wise

```{r line-plot, fig.height = 9, fig.width = 6}

# Define facet labels
response_type.labs <- c("CRN", "ERN")
names(response_type.labs) <- c("correct", "incorrect")


# Create line plot MFN 
plot_MFN_lines <- ggplot(df_aggregated_per_subject_MFN, aes(x=stimulation, y=MFN, group=participant_id)) +
  geom_point(aes(colour=group), size=4.5, position=position_dodge(width=0.1)) +
  geom_line(size=0.5, alpha=0.5, position=position_dodge(width=0.1)) +
  # Add style
  scale_colour_manual(values = my_figure_colors, guide=FALSE) +
  facet_grid(cols = vars(response_type), rows = vars(group), scales = "free",labeller = labeller(response_type = response_type.labs))+
  labs(x = "\nStimulation condition", y = "MFN (mean amplitude 0-100 ms at FCz [μV])") +
  my_figure_theme 


# Save plot
ggsave("./figures/figure_MFN_lines.tiff", width = 13, height = 18, units = "cm", dpi=600, compression = "lzw")


# Display plot
plot_MFN_lines
```
<br><br><br>

##### **How many participants show the verum < sham stimulation effect on ERN/CRN?**

```{r table-count-stimulation-effect}

# Split and re-merge aggregated table to display both stimulation conditions next to each other
count_stimulation_effect <-  split(df_aggregated_per_subject_MFN, df_aggregated_per_subject_MFN$stimulation)
count_stimulation_effect <-  left_join(count_stimulation_effect$sham, count_stimulation_effect$verum, by = c("participant_id", "response_type", "group")) 


# Make table how many participants show the verum < sham stimulation effect on ERN/CRN
count_stimulation_effect <- count_stimulation_effect %>% 
  dplyr::group_by(response_type, group) %>%
  subset(MFN.x < MFN.y) %>% # MFN.x is sham, MFN.y is verum (but ERN is neg, so we need < sign)
  dplyr::summarise(n = n(), .groups = 'drop')


# Display table
my_table_template(count_stimulation_effect)
```
<br><br>

### Plot without session

```{r descriptive-statistics-plot-MFN, fig.width = 8, fig.height = 7, fig.cap = "Note. Response-locked ERP (ERN, CRN; 0-100 ms at FCz) in the flanker task is shown as a function of response type, stimulation condition, and group. Means and 95% confidence intervals (shown in orange/red) were calculated based on single-trial data. Boxplots are based on data aggregated by participant. CIs are adjusted for within-participant designs as described by Morey (2008)."}

# Create plot MFN 
plot_MFN <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_MFN, aes(x = stimulation, y = MFN, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_MFN_no_session, aes(x = stimulation, y = MFN_0_100_FCz, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_MFN_no_session, aes(x = stimulation, ymax = MFN_0_100_FCz + ci, ymin = MFN_0_100_FCz - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_MFN_no_session, aes(x = stimulation, y = MFN_0_100_FCz, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("#b23f00", "#ff9b64"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~response_type, nrow = 1) +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "MFN (mean amplitude 0-100 ms at FCz [μV])")


# Save plot
ggsave("./figures/figure_MFN.tiff", width = 12, height = 12, units = "cm", dpi=600, compression = "lzw")


# Display plot
plot_MFN
``` 
<br><br>

### Plot with session

```{r descriptive-statistics-plot-including-session, fig.width = 10, fig.height = 7, fig.cap = "Note. Response-locked ERP (ERN, CRN; 0-100 ms at FCz) in the flanker task is shown as a function of response type, stimulation condition, group, and session. Means and 95% confidence intervals (shown in orange/red) were calculated based on single-trial data. Boxplots are based on data aggregated by participant. CIs are adjusted for within-participant designs as described by Morey (2008)."}

# Create plot MFN 
plot_MFN_session <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_MFN, aes(x = stimulation, y = MFN, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_MFN, aes(x = stimulation, y = MFN_0_100_FCz, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_MFN, aes(x = stimulation, ymax = MFN_0_100_FCz + ci, ymin = MFN_0_100_FCz - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_MFN, aes(x = stimulation, y = MFN_0_100_FCz, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("#b23f00", "#ff9b64"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~response_type + session, nrow = 1) +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "MFN (mean amplitude 0-100 ms at FCz [μV])") 


# Save plot
ggsave("./figures/figure_MFN_session.tiff", width = 20, height = 25, units = "cm", dpi=600, compression = "lzw")


# Display plot
plot_MFN_session
```  
<br><br>

## LMM Analyses
***

MFN/ERN/CRN were modeled using a linear mixed-effects models (LMMs). <br><br>

**Fixed effects**

*Group (HC, OCD), stimulation (verum, sham), and response type (correct, incorrect)* were specified as fixed factors. Fixed effects were coded using effect coding (this equals sliding difference contrasts for two levels for factors with two levels or sum coding/2), such that the intercept reflects the grand mean across all conditions and differences in means between factor levels are tested. Fixed effects were not eliminated using model comparison techniques because they correspond to the original experimental design and a priori hypotheses. <br><br>


**Random effects**

Participants were specified as random factors. The random-effects structure for each model was determined based on the procedure proposed by Bates, Kliegl, et al. (2015). We started with the maximal random-effects structure  that was justified by the design, including random intercepts for participants, as well as random slopes for all main effects and interactions specified as fixed effects. If the model with the maximal random-effects structure would not converge, correlations of the random terms were set to zero. We performed a principal components analysis on the random-effects variance–covariance estimates to determine the number of components supported by the data and removed random effects explaining zero variance to prevent overparametrization (Matuschek et al., 2017).

```{r MFN-LMM-contrast-coding}

# Define contrasts (sliding difference contrasts)
contrasts(single_trial_data_clean$stimulation)   <- contr.sdif(2)
contrasts(single_trial_data_clean$group)         <- contr.sdif(2)
contrasts(single_trial_data_clean$response_type) <- contr.sdif(2)
contrasts(single_trial_data_clean$session)       <- contr.sdif(2)
contrasts(single_trial_data_clean$stimulus_type) <- contr.sdif(2)
contrasts(single_trial_data_clean$medication)    <- contr.sdif(2)


# Add contrasts as numerical covariates via model matrix* (specify all possible contasts for now)
model_matrix <- model.matrix(~ stimulation * group * response_type, single_trial_data_clean)


# Attach the model matrix (8 columns) to the dataframe
single_trial_data_clean[, (ncol(single_trial_data_clean) + 1):(ncol(single_trial_data_clean) + 8)] <- model_matrix


# Assign descriptive names to the contrasts
names(single_trial_data_clean)[(ncol(single_trial_data_clean) - 7):ncol(single_trial_data_clean)] <- c("Grand Mean", "verum_sham", "OCD_HC", "incorrect_correct", "verum_sham:OCD_HC", "verum_sham:incorrect_correct", "OCD_HC:incorrect_correct", "verum_sham:OCD_HC:incorrect_correct")


# *Note: For the random effects, we needed to enter the separate random effect terms in the models to enable
# double-bar notation (||). This allows fitting a model that sets correlations of the random terms to zero.
```
<br><br>

### LMM for MFN {.tabset}

#### Model

This is the overall model, including error and correct trials. This model will be reported before reporting the separate models for ERN and CRN, because it also shows the overall group effect and stimulation effect. But if reporting this model somehow does not fit well in manuscript, we will not report it. 

```{r LMM-MFN, cache = knitr_cache_enabled}

# Run model with maximal random-effects structure
LMM_MFN <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * incorrect_correct +
  (1 + verum_sham + incorrect_correct + verum_sham:incorrect_correct | participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_MFN) # Model does converge
# isSingular(LMM_MFN) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_MFN)) # All terms explain variance (> 0.5%)


# Display results (fixed effects)
tab_model(LMM_MFN,
  dv.labels = "MFN [μV]", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>
There is a main effect of stimulation and response type and a trend for group. 
<br><br>

#### Assumption checks 

```{r LMM-MFN-assumptions, fig.width = 20, fig.height = 15, cache = knitr_cache_enabled}

# Check model assumptions
performance::check_model(LMM_MFN, panel = TRUE)

# In addition to plots, print verbal output for some assumption tests to facilitate conclusion
print("# Check for heteroscedasticity")
performance::check_heteroscedasticity(LMM_MFN)

print("# Check for influential observations (Cook's distance)")
performance::check_outliers(LMM_MFN, effects = "random")

print("# Check for normal distributed random effects")
performance::check_normality(LMM_MFN, effects = "random")
```

* **Assumption 1: Independence of Data Points / Absence of collinearity -> Is OK**
    + Are predictors not highly correlated?
    + Multicollinearity plot shows only low correlations 

* **Assumption 2: Normality of Residuals -> Is Not OK???** 
    + Are residuals approximately normally distributed?
    + Q-Q plot and density plot look not so great; Q-Q plot quite a bit off at the extremes 
    + It is debated whether this is problematic at all; and violation does not seem so bad, so maybe not worry about it? 

* **Assumption 3: Linearity -> Is OK** 
    + Is the dependent variable linearly related to the fixed factors, random factors, and covariates?
    + Plot of the residuals against the fitted values shows a random scatter pattern, no nonlinear or curvy pattern 

* **Assumption 4: Homogeneity of Residual Variance (Heteroscedasticity) -> Is OK???**
    + Have residuals constant variance across the range of the predicted values?
    + Plot of the residuals against the fitted values shows an even spread around the centered line; but written output says this is not ok

* **Assumption 5: Absence of Influential Data Points -> Is OK** 
    + Are there are no influential values? 
    + Cook's distance plot looks fine (for large N, Cook's distances should be below 1) and written output says there are no outliers 

* **Assumption 6: Normality of Random Effects -> Is OK**
    + Are random effects approximately normally distributed?
    + Written output says this is (mostly) ok 
<br><br>

### LMM for ERN / CRN {.tabset}

#### Model

```{r LMM-ERN-CRN, cache = knitr_cache_enabled}

# ERN
LMM_ERN <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_ERN) # Model does converge
# isSingular(LMM_ERN) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_ERN)) # All terms explain variance (> 0.5%)


# CRN
LMM_CRN <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC  +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_CRN) # Model does converge
# isSingular(LMM_CRN) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_CRN)) # All terms explain variance


# Display models in one table
tab_model(LMM_ERN, LMM_CRN,
  dv.labels = c("ERN [μV]", "CRN [μV]"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>
There is a trend for a higher ERN in OCD compared to HC. There also is a trend for a higher CRN in OCD compared to HC. For CRN, there is an effect of stimulation, with smaller (i.e., less negative) CRN in the verum stimulation compared to the sham condition. There is a trend for a stimulation effect on ERN (which turns significant when using maximum likelihood instead of REML estimation - we will still stick with the REML estimation since the "just significant" effect from the ML estimation appears not really trustworthy to the readers as well). <br><br>

Reinhart and Woodman (2014) found no stimulation effect on CRN. But from their Figure 2, I actually had the impression that the cathodal stimulation seems to rather increase the CRN (more neg), and a little bit reduces ERN. Maybe that their finding of a stimulation effect on the ERN (which was quantified as difference ERN-CRN) might be caused to a great deal by the effect on CRN? But possibly this stimulation effect was not significant when analyzing the CRN or ERN alone. In our data, the stimulation also reduces the CRN, which is probably why I see no effect in the difference ERN-CRN quantification as Reinhart & Woodman did (see ANOVA section). <br><br>

We will report analyses on CRN as exploratory in the manuscript.
<br><br>

#### Assumption checks ERN

```{r LMM-ERN-assumptions, fig.width = 20, fig.height = 15, cache = knitr_cache_enabled}

# ERN check model assumptions
performance::check_model(LMM_ERN, panel = TRUE)

# In addition to plots, print verbal output for some assumption tests to facilitate conclusion
print("# Check for heteroscedasticity")
performance::check_heteroscedasticity(LMM_ERN)

print("# Check for influential observations (Cook's distance)")
performance::check_outliers(LMM_ERN, effects = "random")

print("# Check for normal distributed random effects")
performance::check_normality(LMM_ERN, effects = "random")
```

Notes for assumption check would be the same as written for the MFN model.
<br><br>

#### Assumption checks CRN

```{r LMM-CRN-assumptions, fig.width = 20, fig.height = 15, cache = knitr_cache_enabled}

# CRN check model assumptions
performance::check_model(LMM_CRN, panel = TRUE)

# In addition to plots, print verbal output for some assumption tests to facilitate conclusion
print("# Check for heteroscedasticity")
performance::check_heteroscedasticity(LMM_CRN)

print("# Check for influential observations (Cook's distance)")
performance::check_outliers(LMM_CRN, effects = "random")

print("# Check for normal distributed random effects")
performance::check_normality(LMM_CRN, effects = "random")
```

Notes for assumption check would be the same as written for the MFN model.
<br><br>

#### Equivalence Test: ERN

```{r ERN-equivalence-test}

# Equivalence bounds (refers to difference in raw values of 1 microvolt, not standardized differences in means)
bound_l <- -1 # lower equivalence bound
bound_u <-  1 # upper equivalence bound (the more positive this number, the less negative verum ERN compared to sham ERN) - this is the boundary of interest


# Use the contest1D functions of the lmerTest package to perform tests centered on the lower and upper equivalence bound, using the rhs option
lower <- contest1D(LMM_ERN, c(0, 1, 0, 0), confint = TRUE, rhs = bound_l) # get t value for test against lower bound
upper <- contest1D(LMM_ERN, c(0, 1, 0, 0), confint = TRUE, rhs = bound_u) # get t value for test against upper bound
# Note: c(0,1,0,0) means test for stimulation effect (= second fixed effect), as can be seen with fixef(LMM_ERN)


# Recalculate the required one-sided tests from the t-values (test provided by contest1D is two-sided)
p_lower <- pt(lower$`t value`, lower$df, lower.tail = FALSE) # test against lower bound
p_upper <- pt(upper$`t value`, upper$df, lower.tail = TRUE) # test against upper bound


# ALTERNATIVE: Divide the p-values provided by contest1D by 2, because one-sided tests are required but contest1D provides two-sided test
# p_lower <- lower$`Pr(>|t|)`/2 
# p_upper <- upper$`Pr(>|t|)`/2
```

Equivalence testing: two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI) 

Statistically equivalent: difference between groups is smaller than what is considered meaningful and statistically falls within the interval indicated by the equivalence bounds <br><br>

Test against lower bound: t(`r round(lower$df, 2)`) = `r round(lower$"t value", 2)`, *p* = `r round(p_lower, 5)`  <br>
Test against upper bound: t(`r round(upper$df, 2)`) = `r round(upper$"t value", 2)`, *p* = `r round(p_upper, 3)`  <br>

*Since the test against the upper bound is not significant, we cannot reject that the true value is not equal to or more extreme than the upper equivalence bound (i.e., we cannot reject that the verum ERN is at least 1 microvolt less negative than the sham ERN), which means that we did not obtain an equivalent result.* <br><br>

**The ERN amplitude in the sham vs. verum condition is not statistically equivalent and not statistically different (or trend only)** <br>
"Based on equivalence bounds (= smalles effect size of interest, SESOI) of -1.0 microvolt and 1.0 microvolt for the stimulation effect on the ERN quantified as mean amplitude from 0 to 100 ms at FCz, **we cannot reject effect sizes that we still consider meaningful**, t(44.18) = −0.33, p = .371. Because the effect was also not statistically different from 0 in a traditional null-hypothesis test, the result is inconclusive. **We can neither conclude that the effect is different from zero, nor that the effect is too small to matter**. We need to collect more data to draw conclusions about the presence of an effect, or the absence of a meaningful effect (or both)." <br> *citation: Improving Inferences About Null Effects With Bayes Factors and Equivalence Tests (Lakens et al., 2020)* <br><br>

*Is this SESOI justified? Wy do we consider changes of at least 1 (or 1.5°) microvolt as meaningful and below as not meaningful? E.g. can it be based on the fact that 1 microvolt ERN reduction in OCD would lead to comparable ERN magnitude in OCD and HC? Does an ERN change of 1 microvolt predict something meaningful? The effect is considerably smalller than the one reported by Reinhart & Woodman (2014) and much smaller than reported ERN modification effects by Klawohn et al. (2015°°; 2020). But we should rather refer only to the Woodman paper here, not to Julia's.*

*°would still lead to the same conclusion*
*°° also used mean amplitude* <br><br>

**Possible justification of SESOI of 1 microvolt: effect size 25% smaller than effect reported by Reinhart & Woodman**

* Problem 1: this is no established method for defining a SESOI (according to Lakens, 2017)

* Problem 2: Woodman does not report unstandadized effect size in microvolt, only standadized effect size Cohen's d

* Solution Problem 2: I can use the web digitizer to get delta ERN scores for each participant from Figure 5 (get red dots) and calculate mean of all values -> should give me the difference in the raw means, correct? (Unstandardized effect size is about 1.36 microvolt -> SESOI of 1 microvolt is more than 25% (26,5%) smaller than this effect)

<br><br><br><br>

##### **Equivalence test based on standardized effect size boundaries and t test**

```{r ERN-equivalence-test-TOSTER}
 
# Create aggrated dataframe for equivalence test
 equivalence_data <- single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",] %>%
  dplyr::group_by(participant_id, stimulation) %>%
  dplyr::summarize(
    MFN = mean(MFN_0_100_FCz, na.rm = TRUE)
   )  %>%
  dplyr::ungroup()%>%
  dplyr::select(participant_id,stimulation, MFN)
 equivalence_data <-  split(equivalence_data, equivalence_data$stimulation)
 equivalence_data <-  left_join(equivalence_data$verum, equivalence_data$sham, by = "participant_id")

 
 # Run equivalence test
TOSTpaired(n = 56, 
           m1 = mean(equivalence_data$MFN.x), 
           m2 = mean(equivalence_data$MFN.y), 
           sd1 = sd(equivalence_data$MFN.x), 
           sd2 = sd(equivalence_data$MFN.y), 
           r12 = cor.test(equivalence_data$MFN.x,equivalence_data$MFN.y)$estimate, 
           low_eqbound_dz = -0.38, 
           high_eqbound_dz = 0.38, 
           alpha = 0.05, 
           plot = TRUE, verbose = TRUE)
```
<br><br> 

**Possible justification of SESOI: Telescope approach: SESOI = effect size that original study had 33% power to detect (would be Cohens dz = 0.38, see below)**

* Problem 1: equivalence bounds would then be based on standardized effect size, which I can only implement in t test, not in LMM

* for eqivalence test in the LMM, I need unstandardized effect size and I find no convicing method to calculate Cohen's d from a LMM in order to be able to translate the standardized effect size to an unstandadized effect size for performing the equivalence test

* I found (dz = beta fixed effect / sqrt(n) * SE fixed effect OR ALSO dz = t/sqrt(n)) -> about 1.2 microvolt would correspond to d = 0.38?

* but in Brysbaert paper, there is other description of how to calculate effect size for LMM that gives very different result

* also package effectsize gives very different effect size (should also be Cohen’s d) for stimulation effect in LMM

* Problem 2: interpretation of results is a bit more "abstract"? (when using -1 microvolt as bound, this would be very clear "we cannot reject that the verum ERN is at least 1 microvolt less negative than the sham ERN")

* Solution Problem 1: Run equivalence test on t test (same as in LMM, has a trend for the sitmulation effect); t test also shows trend for stimulation effect (p = .07); interestingly mine mean difference is quite similar to Woodman's

* Solution Problem 2: TOSTER output also shows unstandardized effect size that I can report to facilitate interpretation (unstandadized effect size is even larger than the one from the LMM: 1,3 vs. 1 microvolt)

This approach is favored and will be reported in the manuscript.

<br><br>

#### Check covariates

The purpose of including the covariates was to see how the effects change when controlling for the overall effect of the covariate. Thus, covariates were included only as fixed factor, not as random term. I first included the covariates as main effect only, not allowing any interactions with stimulation or group. However, inspecting the interactions as well might lead to new, important insights. These models including the interactions are presented below. Note: The covariate number of errors refers to the actual number of errors committed by each participant, not the number included in the analyses. Continuous predictors were grand mean standardized (number of errors, number of feedback faster) or within-participant standardized (baseline EEG). <br><br>
Inclusion of baseline activity as covariate was inspired by Frömer et al. (Nature Comm 2021; "For each ERP, we regressed out the baseline activity at the same electrode sites; baseline was included as a nuisance regressor (Alday et al., 2019)). Alday (2019) argues that including baseline activity as predictor is superior to baseline correction. Since I did baseline correction during preprocessing, adding it also as covariate is probably not advantageous. But for the response-locked ERPs in the flanker task it may be worth checking at least, since an early interval (-500 to -300 ms) was used for baseline correction and a later baseline interval (-200 to 0 ms) as covariate. So I keep the analysis with this covariate here. <br><br> 
We also included trial-by-trial P3 and N2 amplitude (within-participant standardized) as a covariate. Thereby, we aimed to control for amplitude differences resulting from group or stimulation effects on stimulus-locked ERPs. Hence, we checked whether inclusion of P3/N2 as covariate changes the results of the stimulation or group effect on ERN/CRN (to control for the shift in the stimulus-locked ERP caused by group or stimulation that affects the ERN/CRN results). The model with the P3 will be reported as post hoc analysis (i.e., not stated in the preregistration), the N2 analysis is of no real interest and will not be reported. This additional analysis including P3 as predictor is justified by two papers (Klawohn et al., Psychophys., 2020; Meyer et al., Psychophys., 2017), reporting that the overlap of P300 with ERN might be problematic when trying to make inferences on effects of one of these components. Note that the P3 was quantified at CPz (since this is where this ERP typically emerges most; is also in line with the preregistration). However, since we actually aim to control for the P3 signal contained in the ERN, one could also use the P3 quantified at FCz as covariate (this value would rather reflect how much of the P3 is left in the FCz and thus affects the ERN). We decided to go with the first option. <br><br>

Of these covariates, we will report P3 and medication in the manuscript. For P3, there are some interactions with P3 that we find difficult to explain. But in the model adding P3 only as main effect, the stimulation effect is not significant. For medication status, we will report the covariate analysis, not the (preregistered) subgroup analysis since this would result in non-significant results due to power problems. For the covariate model, do we use the whole group (but than medication status is confounded with group?) or only the OCD group (but then again we have not enoug power to find the stimulation effect)? For medication, we will only report that the effects hold true when controlling for medication and put the respective model only in the supplement. Will we report the analyses with covariates only for the ERN model or also for CRN/Pe?

```{r LMM-ERN-CRN-covariates, cache = knitr_cache_enabled}

# ERN check covariate P3 (added only as main effect)
LMM_ERN_P3_main_effect <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC + P3_300_500_CPz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate P3 (added only as main effect)
LMM_CRN_P3_main_effect <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC + P3_300_500_CPz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_P3_main_effect,LMM_CRN_P3_main_effect,
  dv.labels = c("ERN [μV], covariate P3 (added only as main effect)","CRN [μV], covariate P3 (added only as main effect)"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# ERN check covariate P3 (added only in interaction with stimulation)
LMM_ERN_P3_int_stim <- lmer(MFN_0_100_FCz ~ verum_sham * (OCD_HC + P3_300_500_CPz_standardized)  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate P3 (added only in interaction with stimulation)
LMM_CRN_P3_int_stim <- lmer(MFN_0_100_FCz ~ verum_sham * (OCD_HC + P3_300_500_CPz_standardized)  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_P3_int_stim,LMM_CRN_P3_int_stim,
  dv.labels = c("ERN [μV], covariate P3 (added only in interaction with stimulation)","CRN [μV], covariate P3 (added only in interaction with stimulation)"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# ERN check covariate P3
LMM_ERN_P3 <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * P3_300_500_CPz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate P3
LMM_CRN_P3 <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * P3_300_500_CPz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_P3, LMM_CRN_P3,
  dv.labels = c("ERN [μV], covariate P3", "CRN [μV], covariate P3"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# ERN check covariate N2
LMM_ERN_N2 <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * N2_200_300_FCz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate N2
LMM_CRN_N2 <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * N2_200_300_FCz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_N2, LMM_CRN_N2,
  dv.labels = c("ERN [μV], covariate N2", "CRN [μV], covariate N2"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# ERN check covariate baseline EEG
LMM_ERN_baseline_EEG <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * MFN_baseline_pre_200_0_FCz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate baseline EEG
LMM_CRN_baseline_EEG <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * MFN_baseline_pre_200_0_FCz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_baseline_EEG, LMM_CRN_baseline_EEG,
  dv.labels = c("ERN [μV], covariate baseline EEG", "CRN [μV], covariate baseline EEG"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# ERN check covariate session
LMM_ERN_session <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * session  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate session
LMM_CRN_session <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * session  +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_session, LMM_CRN_session,
  dv.labels = c("ERN [μV], covariate session","CRN [μV], covariate session"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# ERN check covariate number of errors (predictor was z standardized)
LMM_ERN_number_errors <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * number_errors_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok

 
# CRN check covariate number of errors (predictor was z standardized)
LMM_CRN_number_errors <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * number_errors_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_number_errors, LMM_CRN_number_errors,
  dv.labels = c("ERN [μV], covariate number of errors","CRN [μV], covariate number of errors"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# ERN check covariate number of feedback faster (predictor was z standardized)
LMM_ERN_number_feedback_faster <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * number_feedback_faster_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate number of feedback faster (predictor was z standardized)
LMM_CRN_number_feedback_faster <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * number_feedback_faster_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_number_feedback_faster, LMM_CRN_number_feedback_faster,
  dv.labels = c("ERN [μV], covariate number of feedback faster","CRN [μV], covariate number of feedback faster"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# ERN check covariate stimulus type
LMM_ERN_stimulus_type <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * stimulus_type  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate stimulus_type
LMM_CRN_stimulus_type <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * stimulus_type  +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_stimulus_type, LMM_CRN_stimulus_type,
  dv.labels = c("ERN [μV], covariate stimulus type", "CRN [μV], covariate stimulus type"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# ERN check covariate medication
LMM_ERN_medication <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * medication  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate medication
LMM_CRN_medication <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC * medication  +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_medication, LMM_CRN_medication,
  dv.labels = c("ERN [μV], covariate medication","CRN [μV], covariate medication"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# ERN check covariate medication (only OCD group)
LMM_ERN_medication_OCD <- lmer(MFN_0_100_FCz ~ verum_sham * medication  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect" & single_trial_data_clean$group == "OCD",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate medication (only OCD group)
LMM_CRN_medication_OCD <- lmer(MFN_0_100_FCz ~ verum_sham * medication  +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct" & single_trial_data_clean$group == "OCD",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_medication_OCD, LMM_CRN_medication_OCD,
  dv.labels = c("ERN [μV], covariate medication (only OCD group)","CRN [μV], covariate medication (only OCD group)"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# ERN include only unmedicated participants
LMM_ERN_unmedicated <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC +  
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect" & single_trial_data_clean$medication == "no",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN  include only unmedicated participants
LMM_CRN_unmedicated <- lmer(MFN_0_100_FCz ~ verum_sham * OCD_HC + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct" & single_trial_data_clean$medication == "no",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_unmedicated, LMM_CRN_unmedicated,
  dv.labels = c("ERN [μV], only unmedicated participants","CRN [μV], only unmedicated participants"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# ERN include only unmedicated patients
LMM_ERN_unmedicated_OCD <- lmer(MFN_0_100_FCz ~ verum_sham  +  
  (1 | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect" & single_trial_data_clean$medication == "no" & single_trial_data_clean$group == "OCD",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN  include only unmedicated patients
LMM_CRN_unmedicated_OCD <- lmer(MFN_0_100_FCz ~ verum_sham  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct" & single_trial_data_clean$medication == "no" & single_trial_data_clean$group == "OCD",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_unmedicated_OCD, LMM_CRN_unmedicated_OCD,
  dv.labels = c("ERN [μV], only unmedicated patients","CRN [μV], only unmedicated patients"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# # Plot predicted effects for interaction P3 and stimulation
# plot_P3_stimulation <- plot_model(LMM_ERN_P3,
#                                 type   = "pred",
#                                 terms  = c("P3_300_500_CPz_standardized","verum_sham"),
#                                 ci.lvl = .95) +
#   labs(title = "ERN LMM: Plot interaction between P3 and stimulation", 
#        x     = "P3 (standardized mean amplitude 300-500 ms at CPz [μV])", 
#        y     = "ERN (mean amplitude 0-100 ms at FCz [μV])") +
#   my_figure_theme +
#   aes(color = group) + 
#   scale_color_manual(name = "Stimulation condition", labels = c("sham","verum"), values = c("#ffad48", "#43b7c2")) +
#   theme(axis.ticks.x = element_line()) + 
#   scale_y_continuous(breaks=seq(-40, 40, 10), limits = c(-45,40), expand = c(0,0)) +
#   scale_x_continuous(breaks=seq(-6, 4, 2),   limits = c(-6, 4),  expand = c(0,0)) 
# plot_P3_stimulation
# 
# 
# # Plot predicted effects for interaction P3 and group
# plot_P3_group <- plot_model(LMM_ERN_P3,
#                                 type   = "pred",
#                                 terms  = c("P3_300_500_CPz_standardized","OCD_HC"),
#                                 ci.lvl = .95) +
#   labs(title = "ERN LMM: Plot interaction between P3 and group", 
#        x     = "P3 (standardized mean amplitude 300-500 ms at CPz [μV])", 
#        y     = "ERN (mean amplitude 0-100 ms at FCz [μV])") +
#   my_figure_theme +
#   aes(color = group) + 
#   scale_color_manual(name = "Group", labels = c("HC","OCD"), values = c("#ffad48", "#43b7c2")) +
#   theme(axis.ticks.x = element_line()) +
#   scale_y_continuous(breaks=seq(-40, 40, 10), limits = c(-45,40), expand = c(0,0)) +
#   scale_x_continuous(breaks=seq(-6, 4, 2),   limits = c(-6, 4),  expand = c(0,0))
# plot_P3_group
```
<br><br>

**Covariates**
<br><br>

* **ERN group difference**: Turns significant when including the covariate P3. Remains a trend when including the covariate session, number of feedback faster, or stimulus type in the model. Turns non-significant when including the variable N2, baseline EEG, number of errors or medication in the model. 

* **CRN group difference**: Remains a trend when including the covariate P3, N2, baseline EEG, session, number of feedback faster, or stimulus type in the model. Turns non-significant when including the variable number of errors or medication in the model.

* **ERN stimulation effect**: Turns significant when including the covariate P3 or N2. Remains a trend when including the covariate baseline EEG, session, or medication in the model. Turns non-significant when including the variable number of errors, number of feedback faster, or stimulus type in the model. 

* **CRN stimulation effect**: Remains significant when including the covariate P3, N2, baseline EEG, session, number of errors (almost sign.), stimulus type, or medication in the model. Turns to a trend when including the variable number of feedback faster in the model (but only when allowing interactions with the covariate, not when only allowing its main effect). 
<br><br>

**Subgroup analyses**

When including only unmedicated participants, there is no ERN group difference anymore, no CRN group trend anymore and no CRN stimulation effect anymore (possibly due to reduced power). When including only unmedicated participants, there is no stimulation effect on ERN or CRN anymore.
<br><br>

#### Model with avg ref

To check whether we find a group difference or stimulation effect when using average reference insted of linked mastoid reference, we re-referenced the data before epoching (i.e., after ICA) to average reference. JK and I think that it is not necessary to reverse the mastoid referencing that was done before. <br>
We will not report this model in the manuscript for now. 

```{r LMM-ERN-CRN-avg-ref, cache = knitr_cache_enabled}

# ERN
LMM_ERN_avg_ref <- lmer(MFN_0_100_FCz_avg_ref ~ verum_sham * OCD_HC  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect" ,],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_ERN_avg_ref) # Model does converge
# isSingular(LMM_ERN_avg_ref) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_ERN_avg_ref)) # All terms explain variance (> 0.5%)



# CRN
LMM_CRN_avg_ref <- lmer(MFN_0_100_FCz_avg_ref ~ verum_sham * OCD_HC  +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct" ,],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_CRN_avg_ref) # Model does converge
# isSingular(LMM_CRN_avg_ref) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_CRN_avg_ref)) # All terms explain variance



# Display models in one table
tab_model(LMM_ERN_avg_ref, LMM_CRN_avg_ref,
  dv.labels = c("ERN [μV]","CRN [μV]"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>
With average refrence, there is a significant stimulation effect for ERN and CRN. There is a trend for a group difference for CRN but no group difference for ERN. Note: The results remain qualitatively the same, when we keep the few trials that have an artifact for the MFN_0_100_FCz but not for MFN_0_100_FCz_avg_ref. <br><br>
I also checked the nested models: For ERN, there is no group differences within the stimulation conditions, trend for stimulation effect within both groups (HC: p = .095, OCD: p = .053). For CRN, there is a group difference in the sham condition and a stimulation effect within the OCD group. 
<br><br>

### LMM for ERN / CRN (nested) {.tabset}

#### Model

Since the interaction group * stimulation is not significant in the full model (and there are no a priori hypotheses regarding the stimulation effect within the separate groups), we will not report the nested model in the manuscript for now. For the CRN, there is almost a trend, but we will still not report the nested model for now, since analyzing the CRN is exploratory itself and the trend is not really present (p = .114) and explaining why there is stimulation effect more in OCD might also be hard to explain.

```{r LMM-ERN-CRN-per-group, cache = knitr_cache_enabled}

# ERN
LMM_ERN_group <- lmer(MFN_0_100_FCz ~ group/stimulation  +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# CRN
LMM_CRN_group <- lmer(MFN_0_100_FCz ~ group/stimulation  +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Display models in one table
tab_model(LMM_ERN_group, LMM_CRN_group,
  dv.labels = c("ERN [μV]","CRN [μV]"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)

# # Alternatives to nested model for following-up interactions (with false discovery rate–adjusted p-values, Benjamini & Hochberg, 1995!)
# emm_options(lmer.df = "Satterthwaite", lmerTest.limit = Inf)
# 
# # Alternative 1 (from JB, 2021)
# follow <- emmeans(LMM_ERN, ~verum_sham:OCD_HC)
# follow
# contr <- list(verum_sham_HC = c(-1,1,0,0), verum_sham_OCD =c(0,0,-1,1), sham_HC_OCD =c(-1,0,1,0), verum_HC_OCD =c(0,-1,0,1))
# pairwise <- contrast(follow, contr, adjust= "fdr")
# my_table_template(pairwise)
# 
# # Alternative 2 
# # Calculate pairwise comparisons (results completely identical with nested model)
# LMM_ERN_pairwise_stimulation_by_group <- pairs(emmeans(LMM_ERN, "verum_sham", by = "OCD_HC"), adjust = "fdr")
# LMM_ERN_pairwise_group_by_stimulation <- pairs(emmeans(LMM_ERN, "OCD_HC", by = "verum_sham"), adjust = "fdr")
# # Get CIs for estimates (CIs not completely identical with nested model, also not with adjust = "none")
# LMM_ERN_pairwise_stimulation_by_group_CI <- confint(LMM_ERN_pairwise_stimulation_by_group, adjust = "fdr", level = 0.95)[c(6,7)]
# LMM_ERN_pairwise_group_by_stimulation_CI <- confint(LMM_ERN_pairwise_group_by_stimulation, adjust = "fdr", level = 0.95)[c(6,7)]
# # Display table
# my_table_template(cbind(LMM_ERN_pairwise_stimulation_by_group,LMM_ERN_pairwise_stimulation_by_group_CI))
# my_table_template(cbind(LMM_ERN_pairwise_group_by_stimulation,LMM_ERN_pairwise_group_by_stimulation_CI))
```
<br><br>
For the separate groups, there is no significant effect of stimulation on ERN. According to the plots above it seemed that at least in the HC group, there is a smaller (i.e., less negative) ERN in the verum than in the sham condition. But this is not significant in the LMM.
For the CRN, there was a main effect of stimulation (see above). When testing this effect separately in the groups, a stimulation effect on CRN is evident for the OCD but not for the HC group. The higher order interaction group * stimulation was not significant (p = .114).
<br><br>

#### Assumption checks 

Is same as in non-nested models (tab LMM separately for ERN / CRN)
<br><br>

#### Check covariates

```{r LMM-ERN-CRN-per-group-covariates, cache = knitr_cache_enabled}


# ERN check covariate P3 (added only as main effect)
LMM_ERN_group_P3_main_effect <- lmer(MFN_0_100_FCz ~ group/stimulation + P3_300_500_CPz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate P3 (added only as main effect)
LMM_CRN_group_P3_main_effect <- lmer(MFN_0_100_FCz ~ group/stimulation + P3_300_500_CPz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_group_P3_main_effect,LMM_CRN_group_P3_main_effect,
  dv.labels = c("ERN [μV], covariate P3 (added only as main effect)","CRN [μV], covariate P3 (added only as main effect)"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# ERN check covariate P3
LMM_ERN_group_P3 <- lmer(MFN_0_100_FCz ~ group/stimulation * P3_300_500_CPz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate P3
LMM_CRN_group_P3 <- lmer(MFN_0_100_FCz ~ group/stimulation * P3_300_500_CPz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_group_P3, LMM_CRN_group_P3,
  dv.labels = c("ERN [μV], covariate P3", "CRN [μV], covariate P3"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# ERN check covariate N2
LMM_ERN_group_N2 <- lmer(MFN_0_100_FCz ~ group/stimulation * N2_200_300_FCz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate N2
LMM_CRN_group_N2 <- lmer(MFN_0_100_FCz ~ group/stimulation * N2_200_300_FCz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_group_N2, LMM_CRN_group_N2,
  dv.labels = c("ERN [μV], covariate N2", "CRN [μV], covariate N2"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# ERN check covariate baseline EEG
LMM_ERN_group_baseline_EEG <- lmer(MFN_0_100_FCz ~ group/stimulation * MFN_baseline_pre_200_0_FCz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate baseline EEG
LMM_CRN_group_baseline_EEG <- lmer(MFN_0_100_FCz ~ group/stimulation * MFN_baseline_pre_200_0_FCz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_group_baseline_EEG,LMM_CRN_group_baseline_EEG,
  dv.labels = c("ERN [μV], covariate baseline EEG","CRN [μV], covariate baseline EEG"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 25, digits.re = 3
)


# ERN check covariate session
LMM_ERN_group_session <- lmer(MFN_0_100_FCz ~ group/stimulation * session  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate session
LMM_CRN_group_session <- lmer(MFN_0_100_FCz ~ group/stimulation * session  +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_group_session,LMM_CRN_group_session,
  dv.labels = c("ERN [μV], covariate session","CRN [μV], covariate session"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 25, digits.re = 3
)


# ERN check covariate number of errors (predictor was z standardized)
LMM_ERN_group_number_errors <- lmer(MFN_0_100_FCz ~ group/stimulation * number_errors_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate number of errors (predictor was z standardized)
LMM_CRN_group_number_errors <- lmer(MFN_0_100_FCz ~ group/stimulation * number_errors_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_group_number_errors,LMM_CRN_group_number_errors,
  dv.labels = c("ERN [μV], covariate number of errors","CRN [μV], covariate number of errors"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 25, digits.re = 3
)


# ERN check covariate number of feedback faster (predictor was z standardized)
LMM_ERN_group_number_feedback_faster <- lmer(MFN_0_100_FCz ~ group/stimulation * number_feedback_faster_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate number of feedback faster (predictor was z standardized)
LMM_CRN_group_number_feedback_faster <- lmer(MFN_0_100_FCz ~ group/stimulation * number_feedback_faster_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_group_number_feedback_faster,LMM_CRN_group_number_feedback_faster,
  dv.labels = c("ERN [μV], covariate number of feedback faster","CRN [μV], covariate number of feedback faster"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 25, digits.re = 3
)


# ERN check covariate stimulus type
LMM_ERN_group_stimulus_type <- lmer(MFN_0_100_FCz ~ group/stimulation * stimulus_type  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate stimulus_type
LMM_CRN_group_stimulus_type <- lmer(MFN_0_100_FCz ~ group/stimulation * stimulus_type  +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_group_stimulus_type,LMM_CRN_group_stimulus_type,
  dv.labels = c("ERN [μV], covariate stimulus type","CRN [μV], covariate stimulus type"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 25, digits.re = 3
)


# ERN check covariate medication
LMM_ERN_group_medication <- lmer(MFN_0_100_FCz ~ group/stimulation * medication  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN check covariate medication
LMM_CRN_group_medication <- lmer(MFN_0_100_FCz ~ group/stimulation * medication  +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_group_medication,LMM_CRN_group_medication,
  dv.labels = c("ERN [μV], covariate medication","CRN [μV], covariate medication"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 25, digits.re = 3
)


# ERN include only unmedicated participants
LMM_ERN_group_unmedicated <- lmer(MFN_0_100_FCz ~ group/stimulation +  
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect" & single_trial_data_clean$medication == "no",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# CRN  include only unmedicated participants
LMM_CRN_group_unmedicated <- lmer(MFN_0_100_FCz ~ group/stimulation + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "correct" & single_trial_data_clean$medication == "no",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok


# Display models in one table
tab_model(LMM_ERN_group_unmedicated,LMM_CRN_group_unmedicated,
  dv.labels = c("ERN [μV], only unmedicated participants","CRN [μV], only unmedicated participants"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 25, digits.re = 3
)
```
<br><br>
**Covariates**

For further infos, see notes in section non-nested ERN-CRN models.
<br><br>

* **ERN group difference**: See notes in section non-nested ERN-CRN models.

* **CRN group difference**: See notes in section non-nested ERN-CRN models.

* **ERN stimulation effect in HC/OCD**: For HC and OCD remains non-significant in all models (turns to trend in both groups when including P3 as covariate).

* **CRN stimulation effect in HC/OCD**: For HC remains non-significant in all models. For OCD remains significant in all models. 
<br><br>

**Subgroup analyses**

When including only unmedicated patients, there is no ERN group difference anymore, no CRN group trend anymore and no CRN stimulation effect in OCD anymore (possibly due to reduced power). 
<br><br>

### LMM for Pe {.tabset}

#### Model

```{r LMM-Pe, cache = knitr_cache_enabled}

# Pe
LMM_Pe <- lmer(Pe_200_400_Pz ~ verum_sham * OCD_HC  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_Pe) # Model does converge
# isSingular(LMM_Pe) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_Pe)) # All terms explain variance (> 0.5%)


tab_model(LMM_Pe,
  dv.labels = "Pe [μV]", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>
There is no group difference but a significant stimulation effect on Pe, with a higher Pe for verum stimulation (this is in line with Reinhart & Woodman, 2014). There is no interaction group * stimulation. 
<br><br>

#### Assumption checks

```{r LMM-Pe-assumptions, fig.width = 20, fig.height = 15, cache = knitr_cache_enabled}

# Pe check model assumptions
performance::check_model(LMM_Pe, panel = TRUE)

# In addition to plots, print verbal output for some assumption tests to facilitate conclusion
print("# Check for heteroscedasticity")
performance::check_heteroscedasticity(LMM_Pe)

print("# Check for influential observations (Cook's distance)")
performance::check_outliers(LMM_Pe, effects = "random")

print("# Check for normal distributed random effects")
performance::check_normality(LMM_Pe, effects = "random")
```

Notes for assumption check would be the same as written for the MFN model.
<br><br>

#### Check covariates

```{r LMM-Pe-covariates, cache = knitr_cache_enabled}

# Pe check covariate P3 (added only as main effect)
LMM_Pe_P3_main_effect <- lmer(Pe_200_400_Pz ~ verum_sham * OCD_HC + P3_300_500_CPz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_P3_main_effect,
  dv.labels = "Pe [μV], covariate P3 (added only as main effect)", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# Pe check covariate P3 (added only in interaction with stimulation)
LMM_Pe_P3_int_stim <- lmer(Pe_200_400_Pz ~ verum_sham * (OCD_HC + P3_300_500_CPz_standardized)  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_P3_int_stim,
  dv.labels = "Pe [μV], covariate P3 (added only in interaction with stimulation)", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# Pe check covariate P3
LMM_Pe_P3 <- lmer(Pe_200_400_Pz ~ verum_sham * OCD_HC * P3_300_500_CPz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) 
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_P3,
  dv.labels = "Pe [μV], covariate P3", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# Pe check covariate N2
LMM_Pe_N2 <- lmer(Pe_200_400_Pz ~ verum_sham * OCD_HC * N2_200_300_FCz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) 
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_N2, 
  dv.labels = "Pe [μV], covariate N2", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# Pe check covariate baseline EEG
LMM_Pe_baseline_EEG <- lmer(Pe_200_400_Pz ~ verum_sham * OCD_HC * Pe_baseline_pre_200_0_Pz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_baseline_EEG,
  dv.labels = "Pe [μV], covariate baseline EEG", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# Pe check covariate session
LMM_Pe_session <- lmer(Pe_200_400_Pz ~ verum_sham * OCD_HC * session  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_session,
  dv.labels = "Pe [μV], covariate session", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# Pe check covariate number of errors (predictor was z standardized)
LMM_Pe_number_errors <- lmer(Pe_200_400_Pz ~ verum_sham * OCD_HC * number_errors_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_number_errors,
  dv.labels = "Pe [μV], covariate number of errors", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)

 
# Pe check covariate number of feedback faster (predictor was z standardized)
LMM_Pe_number_feedback_faster <- lmer(Pe_200_400_Pz ~ verum_sham * OCD_HC * number_feedback_faster_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_number_feedback_faster,
  dv.labels = "Pe [μV], covariate number of feedback faster", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# Pe check covariate stimulus type
LMM_Pe_stimulus_type <- lmer(Pe_200_400_Pz ~ verum_sham * OCD_HC * stimulus_type  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_stimulus_type,
  dv.labels = "Pe [μV], covariate stimulus type", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# Pe check covariate medication
LMM_Pe_medication <- lmer(Pe_200_400_Pz ~ verum_sham * OCD_HC * medication  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_medication,
  dv.labels = "Pe [μV], covariate medication", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# Pe check covariate medication (only OCD group)
LMM_Pe_medication_OCD <- lmer(Pe_200_400_Pz ~ verum_sham * medication  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect" & single_trial_data_clean$group == "OCD",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_medication_OCD,
  dv.labels = "Pe [μV], covariate medication (only OCD group)", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# Pe include only unmedicated participants
LMM_Pe_unmedicated <- lmer(Pe_200_400_Pz ~ verum_sham  +  
  (1 +  verum_sham| participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect" & single_trial_data_clean$medication == "no",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_unmedicated,
  dv.labels = "Pe [μV], only unmedicated participants", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# Pe include only unmedicated patients
LMM_Pe_unmedicated_OCD <- lmer(Pe_200_400_Pz ~ verum_sham  +  
  (1 | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect" & single_trial_data_clean$medication == "no" & single_trial_data_clean$group == "OCD",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_unmedicated_OCD,
  dv.labels = "Pe [μV], only unmedicated patients", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>
**Covariates**

For further infos, see notes in section non-nested ERN-CRN models.
<br><br>

* **Pe group difference**: Remains non-significant in all models. 

* **Pe stimulation effect**: Remains significant when including the covariate P3, N2, session, number of errors, number of feedback faster. Turns trend when including the covariate baseline EEG. Turns non-significant when including the variable stimulus type or medication in the model. 

* **Subgroup analysis**: When including only unmedicated participants or unmedicated patients, the stimulation effect on Pe remains significant. 
<br><br>

### LMM for Pe (nested) {.tabset}

#### Model

Since the interaction group * stimulation is not significant in the full model (and there are no a priori hypotheses regarding the stimulation effect within the separate groups), we will not report the nested model in the manuscript for now.

```{r LMM-Pe-per-group, cache = knitr_cache_enabled}

# Pe
LMM_Pe_group <- lmer(Pe_200_400_Pz ~ group/stimulation + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_Pe_group) # Model does converge
# isSingular(LMM_Pe_group) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_Pe_group)) # All terms explain variance (> 0.5%)


tab_model(LMM_Pe_group,
  dv.labels = "Pe [μV]", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>
For the Pe, there was a main effect of stimulation (see above). When testing this effect separately in the groups, a stimulation effect on Pe is evident for the OCD but not for the HC group. The higher order interaction group * stimulation was not significant.
<br><br>


#### Assumption checks 

Is same as in non-nested model (tab LMM Pe)
<br><br>

#### Check covariates

```{r LMM-Pe-per-group-covariates, cache = knitr_cache_enabled}

# Pe check covariate P3 (added only as main effect)
LMM_Pe_group_P3_main_effect <- lmer(Pe_200_400_Pz ~ group/stimulation + P3_300_500_CPz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) # Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_group_P3_main_effect,
  dv.labels = "Pe [μV], covariate P3 (added only as main effect)", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# Pe check covariate P3
LMM_Pe_group_P3 <- lmer(Pe_200_400_Pz ~ group/stimulation * P3_300_500_CPz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) 
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_group_P3,
  dv.labels = "Pe [μV], covariate P3", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# Pe check covariate N2
LMM_Pe_group_N2 <- lmer(Pe_200_400_Pz ~ group/stimulation * N2_200_300_FCz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
) 
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_group_N2, 
  dv.labels = "Pe [μV], covariate N2", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)


# Pe check covariate baseline EEG
LMM_Pe_group_baseline_EEG <- lmer(Pe_200_400_Pz ~ group/stimulation * Pe_baseline_pre_200_0_Pz_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_group_baseline_EEG,
  dv.labels = "Pe [μV], covariate baseline EEG", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# Pe check covariate session
LMM_Pe_group_session <- lmer(Pe_200_400_Pz ~ group/stimulation * session  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_group_session,
  dv.labels = "Pe [μV], covariate session", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# Pe check covariate number of errors (predictor was z standardized)
LMM_Pe_group_number_errors <- lmer(Pe_200_400_Pz ~ group/stimulation * number_errors_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_group_number_errors,
  dv.labels = "Pe [μV], covariate number of errors", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)

 
# Pe check covariate number of feedback faster (predictor was z standardized)
LMM_Pe_group_number_feedback_faster <- lmer(Pe_200_400_Pz ~ group/stimulation * number_feedback_faster_standardized  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_group_number_feedback_faster,
  dv.labels = "Pe [μV], covariate number of feedback faster", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# Pe check covariate stimulus type
LMM_Pe_group_stimulus_type <- lmer(Pe_200_400_Pz ~ group/stimulation * stimulus_type  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_group_stimulus_type,
  dv.labels = "Pe [μV], covariate stimulus type", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# Pe check covariate medication
LMM_Pe_group_medication <- lmer(Pe_200_400_Pz ~ group/stimulation * medication  + 
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_group_medication,
  dv.labels = "Pe [μV], covariate medication", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# Pe include only unmedicated participants
LMM_Pe_group_unmedicated <- lmer(Pe_200_400_Pz ~ group/stimulation  +  
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[single_trial_data_clean$response_type == "incorrect" & single_trial_data_clean$medication == "no",],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_Pe_group_unmedicated,
  dv.labels = "Pe [μV], only unmedicated participants", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = FALSE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>
**Covariates**

For further infos, see notes in section non-nested ERN-CRN models.
<br><br>

* **Pe group difference**: See notes in section non-nested model.

* **Pe stimulation effect in HC/OCD**: For HC remains non-significant in all models. For OCD remains significant in all models, except for the model including the variable baseline EEG or stimulus type. 
<br><br>

### LMM for N2/P3 {.tabset}

We additionally checked whether there is a group difference of stimulation effect on stimulus-locked ERPs (P3, N2). The idea behind this was that the response-locked ERPs are superimposed on these stimulus-locked ERPs. hus, effects of group or stimulation on these stimulus-locked ERPs are of interest to us. We also included trial-by-trial P3 and N2 as a covariate (for results, see section "LMM for ERN/CRN", tab "Check covariates"). <br><br> 

#### Model

We will report this model for the P3 in the manuscript (will be put in the supplement, only report briefly in main manuscript that there was a stimulation effect on P3 which is why P3 was added as covariate to the other models). For now, model with the factor stimulus type (see next tab) will not be reported and the model for N2 will not be reported for now as well. 

```{r LMM-N2-P3, cache = knitr_cache_enabled}

# Add contrasts as numerical covariates via model matrix* (specify all possible contasts for now)
model_matrix_N2_P3 <- model.matrix(~ stimulation * group * response_type * stimulus_type, single_trial_data_clean)


# Attach the model matrix (4 columns) to the dataframe - only the new relevant columns
single_trial_data_clean[, (ncol(single_trial_data_clean) + 1):(ncol(single_trial_data_clean) + 4)] <- model_matrix_N2_P3[,c(5,9,11,14)]


# Assign descriptive names to the contrasts - only the new relevant columns
names(single_trial_data_clean)[(ncol(single_trial_data_clean) - 3):ncol(single_trial_data_clean)] <- c("incongruent_congruent", "verum_sham:incongruent_congruent", "incorrect_correct:incongruent_congruent", "verum_sham:incorrect_correct:incongruent_congruent")



# N2 run final model (without correlations between random terms to achieve convergence) 
LMM_N2 <- lmer(N2_200_300_FCz ~ verum_sham * OCD_HC * incorrect_correct  + 
  (1 + verum_sham * incorrect_correct || participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_N2) # Model does converge
# isSingular(LMM_N2) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_N2)) # All terms explain variance (> 0.5%)



# P3 run final model
LMM_P3 <- lmer(P3_300_500_CPz ~ verum_sham * OCD_HC * incorrect_correct * 
  (1 + verum_sham * incorrect_correct | participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_P3) # Model does converge
# isSingular(LMM_P3) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_P3)) # All terms explain variance (> 0.5%)



# Display models in one table
tab_model(LMM_N2,LMM_P3,
  dv.labels = c("N2 [μV]","P3 [μV]"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)
```
<br><br>
There is a stimulation effect on P3 (P3 more positive for verum than sham) and an effect of response type (see tab "ERP Visualization") on P3. There are no group differences or further interactions.
<br><br>

#### Model with stimulus type

```{r LMM-N2-P3-stim-type, cache = knitr_cache_enabled}

# N2 run final model (without correlations between random terms and random slope verum_sham:incongruent_congruent to achieve convergence) 
LMM_N2_stim_type <- lmer(N2_200_300_FCz ~ verum_sham * OCD_HC * incorrect_correct * incongruent_congruent + 
  (1 + verum_sham + incongruent_congruent + incorrect_correct + verum_sham:incorrect_correct + 
     incorrect_correct:incongruent_congruent + verum_sham:incorrect_correct:incongruent_congruent || participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_N2_stim_type) # Model does converge
# isSingular(LMM_N2_stim_type) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_N2_stim_type)) # All terms explain variance (> 0.5%)



# P3 run final model (without correlations between random terms and random slope verum_sham:incongruent_congruent to achieve convergence)  
LMM_P3_stim_type <- lmer(P3_300_500_CPz ~ verum_sham * OCD_HC * incorrect_correct * incongruent_congruent + 
  (1 + verum_sham + incongruent_congruent + incorrect_correct  + verum_sham:incorrect_correct + 
     incorrect_correct:incongruent_congruent + verum_sham:incorrect_correct:incongruent_congruent || participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_P3_stim_type) # Model does converge
# isSingular(LMM_P3_stim_type) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_P3_stim_type)) # All terms explain variance (> 0.5%)



# Display models in one table
tab_model(LMM_N2_stim_type,LMM_P3_stim_type,
  dv.labels = c("N2 [μV]","P3 [μV]"), show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 30, digits.re = 3
)
```
<br><br>
There is a stimulation effect on P3 (P3 more positive for verum than sham) and an effect of response type (see tab "ERP Visualization") on P3. There is an effect of stimulus type (N2 more neg for incongruent than congruent) for N2. There are no group differences or further interactions.
<br><br>

#### Assumption checks

```{r LMM-N2-P3-assumptions, fig.width = 20, fig.height = 15, cache = knitr_cache_enabled}

# N2 check model assumptions
performance::check_model(LMM_N2, panel = TRUE)

# In addition to plots, print verbal output for some assumption tests to facilitate conclusion
print("# Check for heteroscedasticity")
performance::check_heteroscedasticity(LMM_N2)

print("# Check for influential observations (Cook's distance)")
performance::check_outliers(LMM_N2, effects = "random")

print("# Check for normal distributed random effects")
performance::check_normality(LMM_N2, effects = "random")



# P3 check model assumptions
performance::check_model(LMM_P3, panel = TRUE)

# In addition to plots, print verbal output for some assumption tests to facilitate conclusion
print("# Check for heteroscedasticity")
performance::check_heteroscedasticity(LMM_P3)

print("# Check for influential observations (Cook's distance)")
performance::check_outliers(LMM_P3, effects = "random")

print("# Check for normal distributed random effects")
performance::check_normality(LMM_P3, effects = "random")
```

* **Assumption 1: Independence of Data Points / Absence of collinearity -> Is OK?**
    + Are predictors not highly correlated?
    + Multicollinearity plot shows low and moderate correlations 

* **Assumption 2: Normality of Residuals -> Is Not OK???** 
    + Are residuals approximately normally distributed?
    + Q-Q plot and density plot look not so great; Q-Q plot quite a bit off at the extremes 
    + It is debated whether this is problematic at all; and violation does not seem so bad, so maybe not worry about it? 

* **Assumption 3: Linearity -> Is OK** 
    + Is the dependent variable linearly related to the fixed factors, random factors, and covariates?
    + Plot of the residuals against the fitted values shows a random scatter pattern, no nonlinear or curvy pattern 

* **Assumption 4: Homogeneity of Residual Variance (Heteroscedasticity) -> Is OK???**
    + Have residuals constant variance across the range of the predicted values?
    + Plot of the residuals against the fitted values shows an even spread around the centered line; but written output says this is not ok

* **Assumption 5: Absence of Influential Data Points -> Is OK** 
    + Are there are no influential values? 
    + Cook's distance plot looks fine (for large N, Cook's distances should be below 1) and written output says there are no outliers 

* **Assumption 6: Normality of Random Effects -> Is OK**
    + Are random effects approximately normally distributed?
    + Written output says this is (mostly) ok 
<br><br>

## ANOVAs {.tabset}
***

### ANOVAs

To facilitate comparison with previously reported results obtained using a similar task and aggregation-based analyses (Reinhart & Woodman, 2014), MFN/ERN/CRN/Diff ERN-CRN were additionally analyzed with repeated-measures analyses of variance (ANOVAs) including the within-participant factors stimulation (verum, sham), group (OCD, HC), and (only for MRN) response type (correct, incorrect). No  Greenhouse–Geisser correction was applied, as no factor had more than two levels. As can be seen below, the MFN, ERN, and CRN ANOVAs yielded the same results as obtained with mixed-effects modeling with respect to all effects (trend for ERN and CRN group difference and stimulation effect on ERN, significant stimulation effect on CRN). <br><br>

For comparison with Reinhart & Woodman (2014), we also used area amplitude quantifications. Since it is not clear from the paper which exact quantification method was used°, we exported ERN and difference ERN-CRN from -50 to 150 ms both as integral area amplitude (area for negatives subtracted from area for positives; is equivalent to mean amplitude, except that the mean is divided by the length of the interval) and negative area amplitude (positives are zeroed, only neg area is considered). When using the ERN quantification as integral area amplitude from -50 to 150 ms, there is a significant stimulation effect (is a trend for ERN negative area amplitude quantification). For the difference area amplitude quantifications, there is no stimulation effect. <br><br>

Note: <br>
The area amplitude quantification in the flanker task contains 18 RT outlier trials (out of 55680 trials; 5 of those 18 errors). Those trials were not excluded from the bins in the flanker analysis, but this should be ok (also, RT in these trials is only slightly below 100 or above 800 ms). <br>
° I guess that Reinhart & Woodman used the quantification integral area amplitude difference ERN-CRN (instead of negative area amplitude), since one figure shows that ERN and CRN were both positive in their study. But still, it is not clear in their paper when/whether they used ERN and when difference ERN-CRN as measure (see Evernote 2020_12_21). <br><br>

The ANOVAs will not be reported in the manuscript for now. ANOVA for mean amplitude brings no additional value and ANOVAs based on Woodman's difference measure does not fit to our data (since we also have CRN stimulation effect and OCD group). ANOVA on ERN integral area quantification brings most interesting insight, but does not fit to Woodman's quantification (only time window and aggregation-base approach are identical). However, reporting this ANOVA would likely cause some problems for argumentation, e.g. why effect is suddently present for other time window and aggregation-based analysis. 

```{r ANOVAs}

# Due to the afex package, contrasts are automatically set to effect-coding (contr.sum). Afex package 
# also checks sphericity assumptions and automatically corrects for any violations if necessary.


# ANOVA MFN mean amplitude (0 to 100 ms at FCz)
anova_MFN_mean_amp <- aov_ez(
  id     = "participant_id", 
  dv     = "MFN", 
  data   = df_aggregated_per_subject_MFN,
  within = c("stimulation", "response_type"),
  between= c("group"),
  observed = c("group")
)


# ANOVA ERN mean amplitude (0 to 100 ms at FCz)
anova_ERN_mean_amp <- aov_ez(
  id     = "participant_id", 
  dv     = "MFN", 
  data   = df_aggregated_per_subject_MFN[df_aggregated_per_subject_MFN$response_type == "incorrect",],
  within = c("stimulation"),
  between= c("group"),
  observed = c("group")
)


# ANOVA CRN mean amplitude (0 to 100 ms at FCz)
anova_CRN_mean_amp <- aov_ez(
  id     = "participant_id", 
  dv     = "MFN", 
  data   = df_aggregated_per_subject_MFN[df_aggregated_per_subject_MFN$response_type == "correct",],
  within = c("stimulation"),
  between= c("group"),
  observed = c("group")
)


# ANOVA difference ERN - CRN mean amplitude (0 to 100 ms at FCz)
anova_difference_ERN_CRN_mean_amp <- aov_ez(
  id     = "participant_id", 
  dv     = "difference_ERN_CRN_mean_amplitude_0_100_FCz", 
  data   = df_aggregated_per_subject_ANOVA,
  within = c("stimulation"),
  between= c("group"),
  observed = c("group")
)


# ANOVA ERN integral area amplitude (-50 to 150 ms at FCz)
anova_ERN_int_area <- aov_ez(
  id     = "participant_id", 
  dv     = "ERN_integral_area_amplitude_pre_50_150_FCz", 
  data   = df_aggregated_per_subject_ANOVA,
  within = c("stimulation"),
  between= c("group"),
  observed = c("group")
)


# ANOVA difference ERN - CRN integral area amplitude (-50 to 150 ms at FCz)
anova_difference_ERN_CRN_int_area <- aov_ez(
  id     = "participant_id", 
  dv     = "difference_ERN_CRN_integral_area_amplitude_pre_50_150_FCz", 
  data   = df_aggregated_per_subject_ANOVA,
  within = c("stimulation"),
  between= c("group"),
  observed = c("group")
)


# ANOVA ERN negative area amplitude (-50 to 150 ms at FCz)
anova_ERN_neg_area <- aov_ez(
  id     = "participant_id", 
  dv     = "ERN_negative_area_amplitude_pre_50_150_FCz", 
  data   = df_aggregated_per_subject_ANOVA,
  within = c("stimulation"),
  between= c("group"),
  observed = c("group")
)


# ANOVA difference ERN - CRN negative area amplitude (-50 to 150 ms at FCz)
anova_difference_ERN_CRN_neg_area <- aov_ez(
  id     = "participant_id", 
  dv     = "difference_ERN_CRN_negative_area_amplitude_pre_50_150_FCz", 
  data   = df_aggregated_per_subject_ANOVA,
  within = c("stimulation"),
  between= c("group"),
  observed = c("group")
)


# Display ANOVA results
my_table_template(nice(anova_MFN_mean_amp, MSE = FALSE),                caption = "MFN mean amplitude (0 to 100 ms at FCz)")
my_table_template(nice(anova_ERN_mean_amp, MSE = FALSE),                caption = "ERN mean amplitude (0 to 100 ms at FCz)")
my_table_template(nice(anova_CRN_mean_amp, MSE = FALSE),                caption = "CRN mean amplitude (0 to 100 ms at FCz)")
my_table_template(nice(anova_difference_ERN_CRN_mean_amp, MSE = FALSE), caption = "Difference ERN - CRN mean amplitude (0 to 100 ms at FCz)")
my_table_template(nice(anova_ERN_int_area, MSE = FALSE),                caption = "ERN integral area amplitude (-50 to 150 ms at FCz)")
my_table_template(nice(anova_difference_ERN_CRN_int_area, MSE = FALSE), caption = "Difference ERN - CRN integral area amplitude (-50 to 150 ms at FCz)")
my_table_template(nice(anova_ERN_neg_area, MSE = FALSE),                caption = "ERN negative area amplitude (-50 to 150 ms at FCz)")
my_table_template(nice(anova_difference_ERN_CRN_neg_area, MSE = FALSE), caption = "Difference ERN - CRN negative area amplitude (-50 to 150 ms at FCz)")
```
<br><br>

### Pairwise comparisons

As no significant main effect with > 2 levels or interaction effect was present, no post hoc pairwise comparisons would need to be conducted. However, the stimulation effect on ERN in the separate groups may be seen as pre-planned contrast that we want to test. For these effects, I calculated pairwise comparisons using the emmeans package with Holm–Bonferroni *p* value adjustments. There is still no stimulation effect on ERN in the two groups (but not too far away either). The effect on CRN is present in the OCD group. In the sham condition, the CRN is higher (more negative) in the OCD group; in the verum condition there is no significant group difference anymore. <br><br>
Quantifying the ERN as the integral area amplitude (-50 to 150 ms at FCz), there is a stimulation effect in the HC group and a trend for a stimulation effect in the OCD group (this trend is also present in the negative area amplitude ERN quantification). Quantifying the ERN as the integral area amplitude difference ERN-CRN (-50 to 150 ms at FCz), there is a trend for a stimulation effect in the HC group. 

```{r ANOVAs-pairwise-tests}


# Use multivariate model for all follow-up tests to adequately control for violations of sphericity
afex_options(emmeans_model = "multivariate")


# Pairwise t tests
pairwise_ERN_mean_amp                <- summary(pairs(emmeans(anova_ERN_mean_amp,                "stimulation", by = "group"), adjust = "holm"))
pairwise_CRN_mean_amp                <- summary(pairs(emmeans(anova_CRN_mean_amp,                "stimulation", by = "group"), adjust = "holm"))
pairwise_CRN_mean_amp_by_stimulation <- summary(pairs(emmeans(anova_CRN_mean_amp,                "group", by = "stimulation"), adjust = "holm"))
pairwise_difference_ERN_CRN_mean_amp <- summary(pairs(emmeans(anova_difference_ERN_CRN_mean_amp, "stimulation", by = "group"), adjust = "holm"))
pairwise_ERN_int_area                <- summary(pairs(emmeans(anova_ERN_int_area,                "stimulation", by = "group"), adjust = "holm"))
pairwise_difference_ERN_CRN_int_area <- summary(pairs(emmeans(anova_difference_ERN_CRN_int_area, "stimulation", by = "group"), adjust = "holm"))
pairwise_ERN_neg_area                <- summary(pairs(emmeans(anova_ERN_neg_area,                "stimulation", by = "group"), adjust = "holm"))
pairwise_difference_ERN_CRN_neg_area <- summary(pairs(emmeans(anova_difference_ERN_CRN_neg_area, "stimulation", by = "group"), adjust = "holm"))


# Add Cohen's dz (CIs for d could be added if needed, as it can be returned by the "t_to_d" function)
pairwise_ERN_mean_amp$cohens_dz                 <- round(t_to_d(pairwise_ERN_mean_amp$t.ratio, pairwise_ERN_mean_amp$df, paired = TRUE)[1], digits = 2)
pairwise_CRN_mean_amp$cohens_dz                 <- round(t_to_d(pairwise_CRN_mean_amp$t.ratio, pairwise_CRN_mean_amp$df, paired = TRUE)[1], digits = 2)
pairwise_CRN_mean_amp_by_stimulation$cohens_dz  <- round(t_to_d(pairwise_CRN_mean_amp_by_stimulation$t.ratio, pairwise_CRN_mean_amp_by_stimulation$df, paired = TRUE)[1], digits = 2)
pairwise_difference_ERN_CRN_mean_amp$cohens_dz  <- round(t_to_d(pairwise_difference_ERN_CRN_mean_amp$t.ratio, pairwise_difference_ERN_CRN_mean_amp$df, paired = TRUE)[1], digits = 2)
pairwise_ERN_int_area$cohens_dz                 <- round(t_to_d(pairwise_ERN_int_area$t.ratio, pairwise_ERN_int_area$df, paired = TRUE)[1], digits = 2)
pairwise_difference_ERN_CRN_int_area$cohens_dz  <- round(t_to_d(pairwise_difference_ERN_CRN_int_area$t.ratio, pairwise_difference_ERN_CRN_int_area$df, paired = TRUE)[1], digits = 2)
pairwise_ERN_neg_area$cohens_dz                 <- round(t_to_d(pairwise_ERN_neg_area$t.ratio, pairwise_ERN_neg_area$df, paired = TRUE)[1], digits = 2)
pairwise_difference_ERN_CRN_neg_area$cohens_dz  <- round(t_to_d(pairwise_difference_ERN_CRN_neg_area$t.ratio, pairwise_difference_ERN_CRN_neg_area$df, paired = TRUE)[1], digits = 2)


# Display results 
my_table_template(pairwise_ERN_mean_amp, 
                  digits = c(0, 0, 2, 2, 0, 2, 3, 2),
                  caption = "ERN Mean Amplitude (0 to 100 ms at FCz): Stimulation Effect Within Groups",
                  footnote = "P values are adjusted with Holm–Bonferroni method.") 


my_table_template(pairwise_CRN_mean_amp, 
                  digits = c(0, 0, 2, 2, 0, 2, 3, 2),
                  caption = "CRN Mean Amplitude (0 to 100 ms at FCz): Stimulation Effect Within Groups",
                  footnote = "P values are adjusted with Holm–Bonferroni method.") 

my_table_template(pairwise_CRN_mean_amp_by_stimulation, 
                  digits = c(0, 0, 2, 2, 0, 2, 3, 2),
                  caption = "CRN Mean Amplitude (0 to 100 ms at FCz): Group Effect Within Stimulation",
                  footnote = "P values are adjusted with Holm–Bonferroni method.") 


my_table_template(pairwise_difference_ERN_CRN_mean_amp, 
                  digits = c(0, 0, 2, 2, 0, 2, 3, 2),
                  caption = "Difference ERN-CRN Mean Amplitude (0 to 100 ms at FCz): Stimulation Effect Within Groups",
                  footnote = "P values are adjusted with Holm–Bonferroni method.") 


my_table_template(pairwise_ERN_int_area, 
                  digits = c(0, 0, 2, 2, 0, 2, 3, 2),
                  caption = "ERN Integral Area Amplitude (-50 to 150 ms at FCz): Stimulation Effect Within Groups",
                  footnote = "P values are adjusted with Holm–Bonferroni method.") 


my_table_template(pairwise_difference_ERN_CRN_int_area, 
                  digits = c(0, 0, 2, 2, 0, 2, 3, 2),
                  caption = "Difference ERN-CRN Integral Area Amplitude (-50 to 150 ms at FCz): Stimulation Effect Within Groups",
                  footnote = "P values are adjusted with Holm–Bonferroni method.")


my_table_template(pairwise_ERN_neg_area, 
                  digits = c(0, 0, 2, 2, 0, 2, 3, 2),
                  caption = "ERN Negative Area Amplitude (-50 to 150 ms at FCz): Stimulation Effect Within Groups",
                  footnote = "P values are adjusted with Holm–Bonferroni method.") 


my_table_template(pairwise_difference_ERN_CRN_neg_area, 
                  digits = c(0, 0, 2, 2, 0, 2, 3, 2),
                  caption = "Difference ERN-CRN Negative Area Amplitude (-50 to 150 ms at FCz): Stimulation Effect Within Groups",
                  footnote = "P values are adjusted with Holm–Bonferroni method.") 
```
<br><br>

### Assumption checks (only ANOVAs on mean amplitude)

```{r ANOVAs-assumptions, fig.height = 6}

# Get residuals
residuals_anova_MFN      <- as.data.frame(anova_MFN_mean_amp$lm$residuals)
residuals_anova_ERN      <- as.data.frame(anova_ERN_mean_amp$lm$residuals)
residuals_anova_CRN      <- as.data.frame(anova_CRN_mean_amp$lm$residuals)


# Plot residuals
hist_residuals_anova_MFN <- ggplot(gather(residuals_anova_MFN, cols, value), aes(x = value)) + 
  geom_histogram(color = my_figure_colors[2], fill = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "Histogram Residuals MFN") + my_figure_theme

qqplot_residuals_anova_MFN <- ggplot(gather(residuals_anova_MFN, cols, value), aes(sample = value)) + 
  stat_qq(color = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "QQ-Plot Residuals MFN") + my_figure_theme

hist_residuals_anova_ERN <- ggplot(gather(residuals_anova_ERN, cols, value), aes(x = value)) + 
  geom_histogram(color = my_figure_colors[2], fill = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "Histogram Residuals ERN") + my_figure_theme

qqplot_residuals_anova_ERN <- ggplot(gather(residuals_anova_ERN, cols, value), aes(sample = value)) + 
  stat_qq(color = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "QQ-Plot Residuals ERN") + my_figure_theme

hist_residuals_anova_CRN <- ggplot(gather(residuals_anova_CRN, cols, value), aes(x = value)) + 
  geom_histogram(color = my_figure_colors[2], fill = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "Histogram Residuals CRN") + my_figure_theme

qqplot_residuals_anova_CRN <- ggplot(gather(residuals_anova_CRN, cols, value), aes(sample = value)) + 
  stat_qq(color = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "QQ-Plot Residuals CRN") + my_figure_theme

ggdraw() +
  draw_plot(hist_residuals_anova_MFN,       x =  0,   y = .6, width = .5, height = .4) +
  draw_plot(qqplot_residuals_anova_MFN,     x =  .5,  y = .6, width = .5, height = .4) +
  draw_plot(hist_residuals_anova_ERN,       x =  0,   y = .3, width = .5, height = .3) +
  draw_plot(qqplot_residuals_anova_ERN,     x =  .5,  y = .3, width = .5, height = .3) +
  draw_plot(hist_residuals_anova_CRN,       x =  0,   y = 0,  width = .5, height = .3) +
  draw_plot(qqplot_residuals_anova_CRN,     x =  .5,  y = 0,  width = .5, height = .3) 


# Test normality of residuals
print("# ANOVA MFN: Check Normality of Residuals with Shapiro Wilk Test")
do.call(rbind, lapply(residuals_anova_MFN[,], function(x) shapiro.test(x)["p.value"]))

print("# ANOVA ERN: Check Normality of Residuals with Shapiro Wilk Test")
do.call(rbind, lapply(residuals_anova_ERN[,], function(x) shapiro.test(x)["p.value"]))

print("# ANOVA CRN: Check Normality of Residuals with Shapiro Wilk Test")
do.call(rbind, lapply(residuals_anova_CRN[,], function(x) shapiro.test(x)["p.value"]))
```

* **Assumption #1: Dependent variable interval or ratio variable -> Is OK**

* **Assumption #2: Balanced design** (each participant has to have a value in each condition) **-> Is OK**

* **Assumption #3: No dependency in the scores between participants** (dependency can exist only across scores for individuals) **-> Is OK**

* **Assumption #4: Residuals** of the dependent variable in each level of the within-participants factor are approximately **normally distributed -> Is OK** (see above; for incorrect verum condition only small deviation)

* **Assumption #5: Sphericity** (only relevant for within-participant factors with > 2 levels) (afex package automatically corrects (Greenhouse Geisser) for any violations if necessary) **-> Is OK**

<br><br>

