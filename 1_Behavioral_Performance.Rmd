---
title: "Behavioral Performance"
output: 
  html_document

---

<!-- Set general settings -->

```{r setup, include = FALSE}

# Set general settings for markdown file
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  comment = "",
  results = "hold"
)


# Clear environment
rm(list = ls())


# Enable/disable caching of time-consuming code chunks
knitr_cache_enabled = TRUE


# Load packages
library(dplyr)      # for data manipulation
library(knitr)      # for integrating computing and reporting in markdown
library(kableExtra) # for customizing appearance of tables
library(ggplot2)    # for plotting
library(cowplot)    # for arranging plots
library(e1071)      # for functions skewness and kurtosis
library(MASS)       # for boxcox function and contrast definition
library(lme4)       # for (G)LMMs
library(lmerTest)   # for LMM p values (Satterthwaite's method for approximating dfs for the t and F tests)
library(sjPlot)     # for tab_model function to display (G)LMM results



# Load functions
source("./functions/summarySEwithinO.R")  # Function provided by R-cookbook: http://www.cookbook-r.com/Graphs/Plotting_means_and_error_bars_(ggplot2)/
source("./functions/my_table_template.R") # Function to create table template


# Turn off scientific notation
options(scipen = 999)


# Set figure theme and colors
my_figure_theme <- theme_classic(base_size = 11) +
  theme(legend.position = "bottom", strip.background = element_rect(fill="grey95", linetype = "blank"), axis.ticks.x = element_blank(), axis.title.y = element_text(vjust = -0.5)) 
# instad of theme_classic: + theme_apa(base_size = 11)

my_figure_colors <- c("#8ea6b4", "#465369")



# Prepare labels for (G)LMM tables

```
<br><br>

## Data Cleaning
***
```{r load-and-clean-data, cache = TRUE}

# Load data
load(file = "./data/Single_Trial_Data.rda")



# Create some variables: numeric word accuracy, correctness of previous response, post-error accuracy (pea), and post-correct accuracy (pca)
# Important to do this before trial exclusion! Otherwise the previous trial in the table might not have been the previous trial in the task!
single_trial_data <- single_trial_data %>%
  dplyr::mutate(
    accuracy_numeric = ifelse(response_type == "correct", 1, 0),
    accuracy_prev_trial = ifelse(lag(response_type == "correct", default = TRUE) == TRUE, "correct", "incorrect"),
    pea = ifelse(lag(response_type == "correct") == FALSE & response_type == "correct", 1,
                 ifelse(lag(response_type == "correct") == FALSE & response_type == "incorrect", 0, NA)),
    pca = ifelse(lag(response_type == "correct") == TRUE & response_type == "correct", 1,
                 ifelse(lag(response_type == "correct") == TRUE & response_type == "incorrect", 0, NA))
    ) 


# Exclude missing responses, RT outliers and trials with ERP artifacts
single_trial_data_clean <- single_trial_data %>%
  dplyr::filter(
      response_type != "miss" &
      rt_invalid  == FALSE &
      !is.nan(MFN_0_100_FCz)
  ) # (54998 of 55680 trials left)


# Exclude congruent errors
single_trial_data_clean$congruent_error <- NA
single_trial_data_clean[(single_trial_data_clean$response_type == "incorrect" &
                           single_trial_data_clean$stimulus_type == "congruent"),]$congruent_error <- TRUE
# single_trial_data_clean <- single_trial_data_clean %>%
#   dplyr::filter(is.na(congruent_error))
  



# Create column with single-trial PES (RTpost-error − RTpre-error for all CCEC sequences)
single_trial_data_clean$pes <- NA
# also make sure to exclude sequences where a trial was excluded in between (or when error occurred in first or last trial)

for (i in 3:(nrow(single_trial_data_clean)-1)) {
  if (single_trial_data_clean[i,]$response_type == "incorrect" &
      single_trial_data_clean[(i+1),]$response_type == "correct" &
      single_trial_data_clean[(i-1),]$response_type == "correct" &
      single_trial_data_clean[(i-2),]$response_type == "correct" &
      single_trial_data_clean[(i+1),]$trial - single_trial_data_clean[(i-1),]$trial == 2) {
    single_trial_data_clean[i,]$pes <- (single_trial_data_clean[(i+1),]$rt) - (single_trial_data_clean[(i-1),]$rt)
  }
}


# Make categorical variables factors
single_trial_data_clean$participant_id     <- as.factor(single_trial_data_clean$participant_id) 
single_trial_data_clean$group              <- as.factor(single_trial_data_clean$group)
single_trial_data_clean$session            <- as.factor(single_trial_data_clean$session)
single_trial_data_clean$stimulation        <- as.factor(single_trial_data_clean$stimulation)
single_trial_data_clean$stimulus_type      <- as.factor(single_trial_data_clean$stimulus_type)
single_trial_data_clean$response_type      <- as.factor(single_trial_data_clean$response_type)
single_trial_data_clean$response_type_2nd  <- as.factor(single_trial_data_clean$response_type_2nd)
single_trial_data_clean$accuracy_prev_trial  <- as.factor(single_trial_data_clean$accuracy_prev_trial)
```

Trials were excluded from all analyses if RT was shorter than 100 ms or longer than 800 ms or if the response in a trial was missing. We further discarded trials containing artifacts in the EEG, i.e., a voltage difference exceeding 50 μV between two consecutive sampling points or 200 μV within an epoch. We discarded incorrect responses to congruent stimuli from the RT analysis as these were very rare (0.67% of all trials; N = 372 trials). - Does this make sense?? 

```{r excluded-trials}

# Calculate percentage of excluded trials
excluded_trials <- single_trial_data %>%
  group_by(group, participant_id, session) %>%
  dplyr::summarize(
    invalid_rt   = sum(!is.na(rt_invalid) & rt_invalid != FALSE) / length(participant_id) * 100,
    misses       = sum(response_type == "miss") / length(participant_id) * 100,
    EEG_artifact = sum(is.nan(MFN_0_100_FCz)) / length(participant_id) * 100,
   ) %>%
  group_by(group) %>%
  # Calculate M and SD of the variables
  dplyr::summarise_each(list(mean,sd,min,max), -c(participant_id, session))


# Create dataframe and change order of rows for display
table_excluded_trials <- as.data.frame(excluded_trials[,c(1,2,5,8,11,3,6,9,12,4,7,10,13)])


# Display percentage of excluded trials
my_table_template(table_excluded_trials,
  caption = "Excluded Trials (in %)", 
  col_names = c("Group", "M", "SD", "min", "max", "M", "SD", "min", "max", "M", "SD", "min", "max"),
  header_above_config = c(" " = 1, "RT < 100 / > 800 ms" = 4, "Misses" = 4, "EEG artifact" = 4)
)
```
<br><br>

## Data Inspection
***
### RT per participant 

```{r plot-RT-per-subject-and-response-type, fig.width = 12, fig.height = 20}

rt_per_participant <- ggplot(single_trial_data_clean, aes(x = response_type, y = rt)) + 
  geom_point(position = "jitter", aes(color = stimulus_type)) + 
  ggtitle("RT per participant") + 
  my_figure_theme + 
  facet_wrap(~ participant_id + session, ncol = 10) +
  scale_color_manual(values = my_figure_colors) 
rt_per_participant
```
<br>

### Distribution

```{r inspect-distribution, fig.width = 8, fig.height = 16}

# Plot distribution RT
hist_rt <- ggplot(single_trial_data_clean, aes(x = rt)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "#8ea6b4", size = 1) +
  stat_function(fun = dnorm, args=list(mean = mean(single_trial_data_clean$rt, na.rm = TRUE), 
                                     sd = sd(single_trial_data_clean$rt, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(rt, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram RT", x = "RT", y = "Density") + 
  theme(plot.title = element_text(hjust = 0.5)) 

qqplot_rt <- ggplot(single_trial_data_clean, aes(sample = rt)) +
  stat_qq(color = "#8ea6b4") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot RT", x = "Theoretical Quantiles", y = "Sample Quantiles") + 
  theme(plot.title = element_text(hjust = 0.5))


# Plot distribution log RT
hist_rt_log <- ggplot(single_trial_data_clean, aes(x = rt_log)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "#8ea6b4", size = 1) +
  stat_function(fun = dnorm, args=list(mean = mean(single_trial_data_clean$rt_log, na.rm = TRUE), 
                                     sd = sd(single_trial_data_clean$rt_log, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(rt_log, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram log(RT)", x = "log(RT)", y = "Density") + 
  theme(plot.title = element_text(hjust = 0.5)) 

qqplot_rt_log <- ggplot(single_trial_data_clean, aes(sample = rt_log)) +
  stat_qq(color = "#8ea6b4") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot log(RT)", x = "Theoretical Quantiles", y = "Sample Quantiles") + 
  theme(plot.title = element_text(hjust = 0.5))


# Plot distribution PES
hist_pes <- ggplot(single_trial_data_clean, aes(x = pes)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "#8ea6b4", size = 1) +
  stat_function(fun = dnorm, args=list(mean = mean(single_trial_data_clean$pes, na.rm = TRUE), 
                                     sd = sd(single_trial_data_clean$pes, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(pes, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram PES", x = "PES", y = "Density") + 
  theme(plot.title = element_text(hjust = 0.5))  

qqplot_pes <- ggplot(single_trial_data_clean, aes(sample = pes)) +
  stat_qq(color = "#8ea6b4") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot PES", x = "Theoretical Quantiles", y = "Sample Quantiles") + 
  theme(plot.title = element_text(hjust = 0.5))


## For accuracy, post-error and post-correct accuracy we need the aggregated values as only for those we can make a histogram 7 q-q plot (to check normality for ANOVA)
accuracy_aggregated <- single_trial_data_clean %>%
  group_by(participant_id, session) %>%
  dplyr::summarize(
    mean_accuracy = sum(accuracy_numeric) / length(participant_id) * 100,
    mean_pea      = sum(pea, na.rm = TRUE) / length(participant_id[!is.na(pea)]) * 100,
    mean_pca      = sum(pca, na.rm = TRUE) / length(participant_id[!is.na(pca)]) * 100)


# Plot distribution accuracy 
hist_accuracy <- ggplot(accuracy_aggregated, aes(x = mean_accuracy)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "#8ea6b4", size = 1) +
  stat_function(fun = dnorm, args = list(mean = mean(accuracy_aggregated$mean_accuracy, na.rm = TRUE), 
                                     sd = sd(accuracy_aggregated$mean_accuracy, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(mean_accuracy, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram Mean Accuracy", x = "Mean Accuracy", y = "Density") + 
  theme(plot.title = element_text(hjust = 0.5))

qqplot_accuracy <- ggplot(accuracy_aggregated, aes(sample = mean_accuracy)) +
  stat_qq(color = "#8ea6b4") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot Accuracy", x = "Theoretical Quantiles", y = "Sample Quantiles") + 
  theme(plot.title = element_text(hjust = 0.5))


# Plot distribution PEA 
hist_pea <- ggplot(accuracy_aggregated, aes(x = mean_pea)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "#8ea6b4", size = 1) +
  stat_function(fun = dnorm, args = list(mean = mean(accuracy_aggregated$mean_pea, na.rm = TRUE), 
                                     sd = sd(accuracy_aggregated$mean_pea, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(mean_pea, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram Mean Post-Error Accuracy", x = "Post-Error Accuracy", y = "Density") + 
  theme(plot.title = element_text(hjust = 0.5))

qqplot_pea <- ggplot(accuracy_aggregated, aes(sample = mean_pea)) +
  stat_qq(color = "#8ea6b4") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot Mean Post-Error Accuracy", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme(plot.title = element_text(hjust = 0.5))


# Plot distribution PCA 
hist_pca <- ggplot(accuracy_aggregated, aes(x = mean_pca)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "#8ea6b4", size = 1) +
  stat_function(fun = dnorm, args = list(mean = mean(accuracy_aggregated$mean_pca, na.rm = TRUE), 
                                     sd = sd(accuracy_aggregated$mean_pca, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(mean_pca, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram Mean Post-Correct Accuracy", x = "Post-Correct Accuracy", y = "Density") + 
  theme(plot.title = element_text(hjust = 0.5))

qqplot_pca <- ggplot(accuracy_aggregated, aes(sample = mean_pca)) +
  stat_qq(color = "#8ea6b4") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot Mean Post-Correct Accuracy", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme(plot.title = element_text(hjust = 0.5))


ggdraw() +
  draw_plot(hist_rt,        x =  0,   y = .80, width = .5,  height = .16) +
  draw_plot(qqplot_rt,      x =  .5,  y = .80, width = .5,  height = .16) +
  draw_plot(hist_rt_log,    x =  0,   y = .64, width = .5,  height = .16) +
  draw_plot(qqplot_rt_log,  x =  .5,  y = .64, width = .5,  height = .16) +
  draw_plot(hist_pes,       x =  0,   y = .48, width = .5,  height = .16) +
  draw_plot(qqplot_pes,     x =  .5,  y = .48, width = .5,  height = .16) +
  draw_plot(hist_accuracy,  x =  0,   y = .32, width = .5,  height = .16) +
  draw_plot(qqplot_accuracy,x =  .5,  y = .32, width = .5,  height = .16) +
  draw_plot(hist_pea,       x =  0,   y = .16, width = .5,  height = .16) +
  draw_plot(qqplot_pea,     x =  .5,  y = .16, width = .5,  height = .16) +
  draw_plot(hist_pca,       x =  0,   y = 0,   width = .5,  height = .16) +
  draw_plot(qqplot_pca,     x =  .5,  y = 0,   width = .5,  height = .16) 
```
<br><br>

### Check RT Normality 

For the single-trial data, Shapiro-Wilk is not suitable, as it always returns a significant result for such large samples (additionally, it can handle only samples up to 5000). Hence, we have to rely on visual inspection (see above) and values of skewness and kurtosis. Values for skewness and kurtosis between -2 and +2 are considered acceptable in order to prove normal univariate distribution (George & Mallery, 2010).

```{r normality-single-trial-RTs}

normality_rt <- round(data.frame(matrix(c(skewness(single_trial_data_clean$rt),
                                          kurtosis(single_trial_data_clean$rt),
                                          skewness(single_trial_data_clean$rt_log),
                                          kurtosis(single_trial_data_clean$rt_log),
                                          skewness(single_trial_data_clean[!is.na(single_trial_data_clean$pes),]$pes),
                                          kurtosis(single_trial_data_clean[!is.na(single_trial_data_clean$pes),]$pes)),
                                        nrow=2,ncol=3)),digits = 1)
rownames(normality_rt) <- c("Skewness","Kurtosis")
colnames(normality_rt) <- c("RT","log(RT)", "PES")

my_table_template(normality_rt)
```
<br>

### Determine RT transformation

LMM analysis of RT will be conducted on log-transformed RT values to meet the assumption of normally distributed residuals. The appropriate transformation was validated (?) using the Box–Cox procedure (Box & Cox, 1964). 

```{r RT-determine-transformation, fig.width = 8, fig.height = 3}

# Arrange plots
par(mfrow = c(1, 2)) 

# Determine transformation of RT by estimating optimal lambda using Box–Cox procedure
bc_rt <- boxcox(rt ~ 1, data = single_trial_data_clean)
optlambda_rt <- bc_rt$x[which.max(bc_rt$y)]

# Determine transformation of PES by estimating optimal lambda using Box–Cox procedure
bc_pes <- boxcox(pes+1000 ~ 1, data = single_trial_data_clean[!is.na(single_trial_data_clean$pes),])
optlambda_pes <- bc_pes$x[which.max(bc_pes$y)]

# Reset plot layout
par(mfrow = c(1, 1)) 
```
For RT (left plot), the optimal lambda is `r round(optlambda_rt, digits = 2)`, suggesting that log transformation (for lambda = 0) is appropriate. Actually, for lambda = -0.5, the most appropriate transformation would be Y^-0.5 = 1/(√(Y)), but this does not seem very common to me. As our lambda is not far from 0, I chose log transformation, which is more commonly used. 
For PES (right plot), the optimal lambda is `r round(optlambda_pes, digits = 2)`, suggesting that no transformation (for lambda = 1) is needed.
<br><br>

## Descriptive Statistics
***
### Means and CIs

```{r descriptive-statistics-table}

##### RT
# Calculate descriptive statistics for RT per condition
descriptive_statistics_rt <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "rt",
  withinvars    = c("response_type", "stimulus_type", "stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Round numeric values to two decimals
  dplyr::mutate_if(is.numeric, round, digits = 2
  ) %>%
  # Format confidence interval column
  dplyr::mutate(
    ci_rt       = paste0("[", rt - ci, ", ", rt + ci, "]")
  ) %>%
  # Select columns to be displayed
  dplyr::select(c("group", "response_type", "stimulus_type", "stimulation", "session", "rt", "ci_rt", "ci"))


# Split and re-merge RT table to display both groups next to each other
descriptive_statistics_rt_display <-  split(descriptive_statistics_rt, descriptive_statistics_rt$group)
descriptive_statistics_rt_display <-  left_join(descriptive_statistics_rt_display$HC, descriptive_statistics_rt_display$OCD, by = c("stimulus_type", "response_type", "stimulation", "session"))


# Display descriptive statistics for RT (and select columns)
my_table_template(descriptive_statistics_rt_display[,c(2:7,10:11)],
  caption = "Behavioral Performance: RT (in ms)",
  col_names = c("Response type", "Stimulus type", "Stimulation", "Session", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 4, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)



##### Accuracy
# Calculate descriptive statistics for accuracy per condition
descriptive_statistics_accuracy <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "accuracy_numeric",
  withinvars    = c("stimulus_type", "stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100)
  )  %>%  
  # Round numeric values to two decimals
  dplyr::mutate_if(is.numeric, round, digits = 2
  ) %>%
  # Format confidence interval column
  dplyr::mutate(
    ci_accuracy = paste0("[", accuracy_numeric - ci, ", ", accuracy_numeric + ci, "]")
  ) %>%
  # Select columns to be displayed
  dplyr::select(c("group", "stimulus_type", "stimulation", "session", "accuracy_numeric", "ci_accuracy", "ci"))


# Split and re-merge accuracy table to display both groups next to each other
descriptive_statistics_accuracy_display <-  split(descriptive_statistics_accuracy, descriptive_statistics_accuracy$group)
descriptive_statistics_accuracy_display <-  left_join(descriptive_statistics_accuracy_display$HC, descriptive_statistics_accuracy_display$OCD, by = c("stimulus_type", "stimulation", "session"))


# Display descriptive statistics for Accuracy (and select columns)
my_table_template(descriptive_statistics_accuracy_display[,c(2:6,9:10)],
  caption = "Behavioral Performance: Accuracy (in %)",
  col_names = c("Stimulus type", "Stimulation", "Session", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 3, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)



##### PES
# Calculate descriptive statistics for PES per condition
descriptive_statistics_pes <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
  measurevar    = "pes",
  withinvars    = c("stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Round numeric values to two decimals
  dplyr::mutate_if(is.numeric, round, digits = 2
  ) %>%
  # Format confidence interval column
  dplyr::mutate(
    ci_pca = paste0("[", pes - ci, ", ", pes + ci, "]")
  )


# Split and re-merge PEA table to display both groups next to each other
descriptive_statistics_pes_display <-  split(descriptive_statistics_pes, descriptive_statistics_pes$group)
descriptive_statistics_pes_display <-  left_join(descriptive_statistics_pes_display$HC, descriptive_statistics_pes_display$OCD, by = c("stimulation", "session"))


# Display descriptive statistics for PES (and select columns)
my_table_template(descriptive_statistics_pes_display[,c(2,3,5,10,13,18)],
  caption = "Behavioral Performance: Post-Error Slowing (in %)",
  col_names = c("Stimulation", "Session", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 2, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)



##### PEA
# Calculate descriptive statistics for post-error accuracy per condition
descriptive_statistics_pea <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pea),],
  measurevar    = "pea",
  withinvars    = c("stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100)
  )  %>%  
  # Round numeric values to two decimals
  dplyr::mutate_if(is.numeric, round, digits = 2
  ) %>%
  # Format confidence interval column
  dplyr::mutate(
    ci_pea = paste0("[", pea - ci, ", ", pea + ci, "]")
  )

# Split and re-merge PEA table to display both groups next to each other
descriptive_statistics_pea_display <-  split(descriptive_statistics_pea, descriptive_statistics_pea$group)
descriptive_statistics_pea_display <-  left_join(descriptive_statistics_pea_display$HC, descriptive_statistics_pea_display$OCD, by = c("stimulation", "session"))


# Display descriptive statistics for PEA (and select columns)
my_table_template(descriptive_statistics_pea_display[,c(2,3,5,10,13,18)],
  caption = "Behavioral Performance: Post-Error Accuracy (in %)",
  col_names = c("Stimulation", "Session", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 2, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)



##### PCA
# Calculate descriptive statistics for post-correct accuracy per condition
descriptive_statistics_pca <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pca),],
  measurevar    = "pca",
  withinvars    = c("stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100)
  )  %>%  
  # Round numeric values to two decimals
  dplyr::mutate_if(is.numeric, round, digits = 2
  ) %>%
  # Format confidence interval column
  dplyr::mutate(
    ci_pca = paste0("[", pca - ci, ", ", pca + ci, "]")
  )


# Split and re-merge PCA table to display both groups next to each other
descriptive_statistics_pca_display <-  split(descriptive_statistics_pca, descriptive_statistics_pca$group)
descriptive_statistics_pca_display <-  left_join(descriptive_statistics_pca_display$HC, descriptive_statistics_pca_display$OCD, by = c("stimulation", "session"))


# Display descriptive statistics for PCA (and select columns)
my_table_template(descriptive_statistics_pca_display[,c(2,3,5,10,13,18)],
  caption = "Behavioral Performance: Post-Correct Accuracy (in %)",
  col_names = c("Stimulation", "Session", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 2, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)
```
<br>
To quantify PES robust, we used the method proposed by Dutilh et al. (2012). "A single-trial value of PES was computed by performing a pairwise comparison of correct trials around each error (RTpost-error − RTpre-error). The mean PES was computed by averaging these differences. This method ensures that post-error and post-correct trials originate from the same time periods in the data set and thus controls for global fluctuations in motivation and attention (Dutilh et al., 2012)." To avoid the effects of consecutive errors on RTs, we considered only error trials that were preceded by at least two correct responses and followed by at least one correct response (i.e., sequences of CCEC trials, where ‘C’ represents correct trials and ‘E’ represents error trials). We further quantified accuracy following incorrect and correct responses (post-error accuracy and post-correct accuracy).
<br><br>

### Plots

```{r descriptive-statistics-plot-rt-acc, fig.width = 8, fig.height = 7, fig.cap = "Note. (A) RT, (B) accuracy, (c) post-error slowing and (D) post-error accuracy, and (E) post-correct accuracy in the flanker task are shown as a function of stimulus type, response type, stimulation condition, and group. Error bars represent 95% confidence intervals adjusted for within-participant designs as described by Morey (2008)."}

# Calcuate means and CIs adjusted for within-participant factors (without session)
descriptive_statistics_rt_no_session <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "rt",
  withinvars    = c("response_type", "stimulus_type", "stimulation"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
)

descriptive_statistics_accuracy_no_session <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "accuracy_numeric",
  withinvars    = c("stimulus_type", "stimulation"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100))


descriptive_statistics_pes_no_session <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
  measurevar    = "pes",
  withinvars    = "stimulation",
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
)

descriptive_statistics_pea_no_session <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pea),],
  measurevar    = "pea",
  withinvars    = "stimulation",
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100))

descriptive_statistics_pca_no_session <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pca),],
  measurevar    = "pca",
  withinvars    = "stimulation",
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100))


# Create plot RT  
plot_rt <- ggplot(descriptive_statistics_rt_no_session, 
                  aes(x = stimulation, y = rt, fill = group)) +
  geom_bar(stat = "identity", position = position_dodge(), colour = "black", size = 0.25) +
  geom_errorbar(aes(ymax = rt + ci, ymin = rt - ci), 
                position = position_dodge(width = 0.9), width = 0.2, size = 0.3) +
  my_figure_theme +
  labs(x = "\nStimulation condition", y = "RT (ms)") +
  coord_cartesian(ylim = c(200, 500)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_manual(values = my_figure_colors, name = "Group:")  +
  facet_wrap(.~ response_type + stimulus_type, nrow = 1) +
  guides(fill = guide_legend(nrow = 1, title.position = "left")) 

  
# Create plot accuracy
plot_accuracy <- ggplot(descriptive_statistics_accuracy_no_session, 
                        aes(x = stimulation, y = accuracy_numeric, fill = group)) +
  geom_bar(stat = "identity", position = position_dodge(), colour = "black", size = 0.25) +
  geom_errorbar(aes(ymax = accuracy_numeric + ci, ymin = accuracy_numeric - ci),
                position = position_dodge(width = 0.9), width = 0.2, size = 0.3) +
  my_figure_theme +
  labs(x = "\nStimulation condition", y = "Accuracy (%)") +
  coord_cartesian(ylim = c(80, 100)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_manual(values = my_figure_colors, name = "Group") +
  facet_wrap(.~ stimulus_type, nrow = 1)


# Create plot PES  
plot_pes <- ggplot(descriptive_statistics_pes_no_session, 
                  aes(x = stimulation, y = pes, fill = group)) +
  geom_bar(stat = "identity", position = position_dodge(), colour = "black", size = 0.25) +
  geom_errorbar(aes(ymax = pes + ci, ymin = pes - ci), 
                position = position_dodge(width = 0.9), width = 0.2, size = 0.3) +
  my_figure_theme +
  labs(x = "\nStimulation condition", y = "Post-error slowing (ms)") +
  coord_cartesian(ylim = c(0, 70)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_manual(values = my_figure_colors, name = "Group:") 


# Create plot PEA
plot_pea <- ggplot(descriptive_statistics_pea_no_session, 
                  aes(x = stimulation, y = pea, fill = group)) +
  geom_bar(stat = "identity", position = position_dodge(), colour = "black", size = 0.25) +
  geom_errorbar(aes(ymax = pea + ci, ymin = pea - ci), 
                position = position_dodge(width = 0.9), width = 0.2, size = 0.3) +
  my_figure_theme +
  labs(x = "\nStimulation condition", y = "Post-error accuracy (%)") +
  coord_cartesian(ylim = c(80, 100)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_manual(values = my_figure_colors, name = "Group:")  


# Create plot PCA
plot_pca <- ggplot(descriptive_statistics_pca_no_session, 
                  aes(x = stimulation, y = pca, fill = group)) +
  geom_bar(stat = "identity", position = position_dodge(), colour = "black", size = 0.25) +
  geom_errorbar(aes(ymax = pca + ci, ymin = pca - ci), 
                position = position_dodge(width = 0.9), width = 0.2, size = 0.3) +
  my_figure_theme +
  labs(x = "\nStimulation condition", y = "Post-correct accuracy (%)") +
  coord_cartesian(ylim = c(80, 100)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_manual(values = my_figure_colors, name = "Group:")  


# Create common legend for plots (function from http://www.sthda.com/english/wiki/wiki.php?id_contents=7930#add-a-common-legend-for-multiple-ggplot2-graphs)
get_legend <- function(myggplot) {
  tmp      <- ggplot_gtable(ggplot_build(myggplot))
  leg      <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend   <- tmp$grobs[[leg]]
  return(legend)
}
legend    <- get_legend(plot_rt)


# Remove previous legends from plots
plot_rt       <- plot_rt       + theme(legend.position = "none")
plot_accuracy <- plot_accuracy + theme(legend.position = "none")
plot_pes      <- plot_pes      + theme(legend.position = "none")
plot_pea      <- plot_pea      + theme(legend.position = "none")
plot_pca      <- plot_pca      + theme(legend.position = "none")


# Arrange plots
figure_behav <- ggdraw() +
  draw_plot(plot_rt,       x =  0,   y = .5,  width = .65, height = .5) +
  draw_plot(plot_accuracy, x = .65,  y = .5,  width = .35, height = .5) +
  draw_plot(plot_pes,      x =  0,   y = .15, width = .33, height = .4) +
  draw_plot(plot_pea,      x = .33,  y = .15, width = .33, height = .4) +
  draw_plot(plot_pca,      x = .66,  y = .15, width = .33, height = .4) +
  draw_plot(legend,        x = .27,  y = .05, width = .5,  height = .1)   +
  draw_plot_label(c("A", "B", "C", "D", "E"), c(0, .65, 0, .33, .66), c(1, 1, .585, .585, .585), size = 15)


# Save plot
ggsave("figure_behav.tiff", width = 16, height = 15, units = "cm", dpi=600, compression = "lzw")


# Display plot
figure_behav
``` 
<br>

### Plots including session

```{r descriptive-statistics-plot-including-session, fig.width = 10, fig.height = 15, fig.cap = "Note. (A) RT, (B) accuracy, (c) post-error slowing and (D) post-error accuracy, and (E) post-correct accuracy in the flanker task are shown as a function of stimulation condition, group, and session. Error bars represent 95% confidence intervals adjusted for within-participant designs as described by Morey (2008)."}

# Create plot RT
plot_rt_session <- ggplot(descriptive_statistics_rt, 
                  aes(x = stimulation, y = rt, fill = group)) +
  geom_bar(stat = "identity", position = position_dodge(), colour = "black", size = 0.25) +
  geom_errorbar(aes(ymax = rt + ci, ymin = rt - ci), 
                position = position_dodge(width = 0.9), width = 0.2, size = 0.3) +
  my_figure_theme +
  labs(x = "\nStimulation condition", y = "RT (ms)") +
  coord_cartesian(ylim = c(200, 500)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_manual(values = my_figure_colors, name = "Group:")  +
  facet_wrap(.~ response_type + stimulus_type + session, nrow = 1) +
  guides(fill = guide_legend(nrow = 1, title.position = "left")) 

  
# Create plot accuracy
plot_accuracy_session <- ggplot(descriptive_statistics_accuracy, 
                        aes(x = stimulation, y = accuracy_numeric, fill = group)) +
  geom_bar(stat = "identity", position = position_dodge(), colour = "black", size = 0.25) +
  geom_errorbar(aes(ymax = accuracy_numeric + ci, ymin = accuracy_numeric - ci),
                position = position_dodge(width = 0.9), width = 0.2, size = 0.3) +
  my_figure_theme +
  labs(x = "\nStimulation condition", y = "Accuracy (%)") +
  coord_cartesian(ylim = c(80, 100)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_manual(values = my_figure_colors, name = "Group") +
  facet_wrap(.~ stimulus_type + session, nrow = 1)


# Create plot PES  
plot_pes_session <- ggplot(descriptive_statistics_pes, 
                  aes(x = stimulation, y = pes, fill = group)) +
  geom_bar(stat = "identity", position = position_dodge(), colour = "black", size = 0.25) +
  geom_errorbar(aes(ymax = pes + ci, ymin = pes - ci), 
                position = position_dodge(width = 0.9), width = 0.2, size = 0.3) +
  my_figure_theme +
  labs(x = "\nStimulation condition", y = "Post-error slowing (ms)") +
  coord_cartesian(ylim = c(0, 70)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +  
  facet_wrap(.~ session, nrow = 1)


# Create plot PEA
plot_pea_session <- ggplot(descriptive_statistics_pea, 
                  aes(x = stimulation, y = pea, fill = group)) +
  geom_bar(stat = "identity", position = position_dodge(), colour = "black", size = 0.25) +
  geom_errorbar(aes(ymax = pea + ci, ymin = pea - ci), 
                position = position_dodge(width = 0.9), width = 0.2, size = 0.3) +
  my_figure_theme +
  labs(x = "\nStimulation condition", y = "Post-error accuracy (%)") +
  coord_cartesian(ylim = c(80, 100)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_manual(values = my_figure_colors, name = "Group:")  +  
  facet_wrap(.~ session, nrow = 1)


# Create plot PCA
plot_pca_session <- ggplot(descriptive_statistics_pca, 
                  aes(x = stimulation, y = pca, fill = group)) +
  geom_bar(stat = "identity", position = position_dodge(), colour = "black", size = 0.25) +
  geom_errorbar(aes(ymax = pca + ci, ymin = pca - ci), 
                position = position_dodge(width = 0.9), width = 0.2, size = 0.3) +
  my_figure_theme +
  labs(x = "\nStimulation condition", y = "Post-correct accuracy (%)") +
  coord_cartesian(ylim = c(80, 100)) +
  scale_y_continuous(expand = c(0, 0)) +
  scale_fill_manual(values = my_figure_colors, name = "Group:")  +  
  facet_wrap(.~ session, nrow = 1) 


# Create common legend for plots (function from http://www.sthda.com/english/wiki/wiki.php?id_contents=7930#add-a-common-legend-for-multiple-ggplot2-graphs)
get_legend <- function(myggplot) {
  tmp      <- ggplot_gtable(ggplot_build(myggplot))
  leg      <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend   <- tmp$grobs[[leg]]
  return(legend)
}
legend    <- get_legend(plot_rt_session)


# Remove previous legends from plots
plot_rt_session       <- plot_rt_session       + theme(legend.position = "none")
plot_accuracy_session <- plot_accuracy_session + theme(legend.position = "none")
plot_pes_session      <- plot_pes_session      + theme(legend.position = "none")
plot_pea_session      <- plot_pea_session      + theme(legend.position = "none")
plot_pca_session      <- plot_pca_session      + theme(legend.position = "none")


# Arrange plots
figure_behav_session <- ggdraw() +
  draw_plot(plot_rt_session,       x =  0,   y = .66,  width = 1,   height = .33) +
  draw_plot(plot_accuracy_session, x =  0,   y = .33,  width = .5,  height = .33) +
  draw_plot(legend,                x = .6,   y = .36,  width = .5,  height = .33) +
  draw_plot(plot_pes_session,      x =  0,   y = .0,   width = .33, height = .33) +
  draw_plot(plot_pea_session,      x = .33,  y = .0,   width = .33, height = .33) +
  draw_plot(plot_pca_session,      x = .66,  y = .0,   width = .33, height = .33) +
  draw_plot_label(c("A", "B", "C","D","E"), c(0, 0, 0, 0.33, 0.66), c(1, .66, .33, .33, .33), size = 15)

#plot_grid(plot_rt_session, plot_accuracy_session, plot_grid(plot_pes_session, plot_pea_session, plot_pca_session), labels = "AUTO", rows = 2)

# Save plot
 ggsave("figure_behav_session.tiff", width = 20, height = 25, units = "cm", dpi=600, compression = "lzw")


# Display plot
figure_behav_session
```  
<br><br>



## (G)LMM Analyses
***

RT and accuracy were modeled using a linear mixed-effects model (LMM) and a binomial generalized linear mixed-effects model (GLMM), respectively. In both models, **stimulus type (congruent, incongruent), group (HC, OCD), stimulation (verum, sham), and, where applicable (= for RT), response type (correct, incorrect)** were specified as fixed factors and participants as random factors. <br><br>
Fixed effects were coded using effect coding (this equals sliding difference contrasts for two levels for factors with two levels), such that the intercept reflects the grand mean across all conditions and differences in means between factor levels are tested. The random-effects structure for each model was determined based on the procedure proposed by Bates, Kliegl, et al. (2015). We started with the maximal random-effects structure, including random intercepts for participants, as well as random slopes for all main effects and interactions specified as fixed effects. If the model with the maximal random-effects structure would not converge, correlations of the random terms were set to zero. We performed a principal components analysis on the random-effects variance–covariance estimates to determine the number of components supported by the data and removed random effects explaining zero variance to prevent overparametrization (Matuschek et al., 2017).

```{r RT-accuracy-(G)LMM-contrast-coding}

# Define contrasts (sliding difference contrasts)
contrasts(single_trial_data_clean$stimulus_type)          <- contr.sdif(2)
contrasts(single_trial_data_clean$response_type)          <- contr.sdif(2)
contrasts(single_trial_data_clean$stimulation)            <- contr.sdif(2)
contrasts(single_trial_data_clean$group)                  <- contr.sdif(2)
contrasts(single_trial_data_clean$session)                <- contr.sdif(2)
contrasts(single_trial_data_clean$accuracy_prev_trial)    <- contr.sdif(2)


# Add contrasts as numerical covariates via model matrix* (specify all possible contasts for now)
model_matrix <- model.matrix(~ stimulus_type * response_type * stimulation * group * session * accuracy_prev_trial, single_trial_data_clean)


# Attach the model matrix (32 columns) to the dataframe
single_trial_data_clean[, (ncol(single_trial_data_clean) + 1):(ncol(single_trial_data_clean) + 64)] <- model_matrix


# Assign descriptive names to the contrasts
names(single_trial_data_clean)[(ncol(single_trial_data_clean) - 63):ncol(single_trial_data_clean)] <- c("Grand Mean", "incongruent_congruent", "incorrect_correct", "verum_sham", "OCD_HC", "T2_T1", "prev_trial_incorrect_correct", "incongruent_congruent:incorrect_correct", "incongruent_congruent:verum_sham", "incorrect_correct:verum_sham", "incongruent_congruent:OCD_HC", "incorrect_correct:OCD_HC", "verum_sham:OCD_HC",  "incongruent_congruent:T2_T1", "incorrect_correct:T2_T1", "verum_sham:T2_T1", "OCD_HC:T2_T1", "incongruent_congruent:prev_trial_incorrect_correct", 
  "incorrect_correct:prev_trial_incorrect_correct", "verum_sham:prev_trial_incorrect_correct", "OCD_HC:prev_trial_incorrect_correct", "T2_T1:prev_trial_incorrect_correct", "incongruent_congruent:incorrect_correct:verum_sham", "incongruent_congruent:incorrect_correct:OCD_HC", "incongruent_congruent:verum_sham:OCD_HC", "incorrect_correct:verum_sham:OCD_HC", "incongruent_congruent:incorrect_correct:T2_T1", "incongruent_congruent:verum_sham:T2_T1", "incorrect_correct:verum_sham:T2_T1", "incongruent_congruent:OCD_HC:T2_T1", "incorrect_correct:OCD_HC:T2_T1", "verum_sham:OCD_HC:T2_T1", "incongruent_congruent:incorrect_correct:prev_trial_incorrect_correct",  "incongruent_congruent:verum_sham:prev_trial_incorrect_correct", "incorrect_correct:verum_sham:prev_trial_incorrect_correct", "incongruent_congruent:OCD_HC:prev_trial_incorrect_correct",   "incorrect_correct:OCD_HC:prev_trial_incorrect_correct", "verum_sham:OCD_HC:prev_trial_incorrect_correct", "incongruent_congruent:T2_T1:prev_trial_incorrect_correct", "incorrect_correct:T2_T1:prev_trial_incorrect_correct", "verum_sham:T2_T1:prev_trial_incorrect_correct", "OCD_HC:T2_T1:prev_trial_incorrect_correct", "incongruent_congruent:incorrect_correct:verum_sham:OCD_HC", "incongruent_congruent:incorrect_correct:verum_sham:T2_T1", "incongruent_congruent:incorrect_correct:OCD_HC:T2_T1", 
  "incongruent_congruent:verum_sham:OCD_HC:T2_T1", "incorrect_correct:verum_sham:OCD_HC:T2_T1", "incongruent_congruent:incorrect_correct:verum_sham:prev_trial_incorrect_correct", 
  "incongruent_congruent:incorrect_correct:OCD_HC:prev_trial_incorrect_correct", "incongruent_congruent:verum_sham:OCD_HC:prev_trial_incorrect_correct", 
  "incorrect_correct:verum_sham:OCD_HC:prev_trial_incorrect_correct", "incongruent_congruent:incorrect_correct:T2_T1:prev_trial_incorrect_correct", 
  "incongruent_congruent:verum_sham:T2_T1:prev_trial_incorrect_correct", "incorrect_correct:verum_sham:T2_T1:prev_trial_incorrect_correct", "incongruent_congruent:OCD_HC:T2_T1:prev_trial_incorrect_correct", "incorrect_correct:OCD_HC:T2_T1:prev_trial_incorrect_correct", "verum_sham:OCD_HC:T2_T1:prev_trial_incorrect_correct", 
  "incongruent_congruent:incorrect_correct:verum_sham:OCD_HC:T2_T1", "incongruent_congruent:incorrect_correct:verum_sham:OCD_HC:prev_trial_incorrect_correct", 
  "incongruent_congruent:incorrect_correct:verum_sham:T2_T1:prev_trial_incorrect_correct", "incongruent_congruent:incorrect_correct:OCD_HC:T2_T1:prev_trial_incorrect_correct", 
  "incongruent_congruent:verum_sham:OCD_HC:T2_T1:prev_trial_incorrect_correct", "incorrect_correct:verum_sham:OCD_HC:T2_T1:prev_trial_incorrect_correct", "incongruent_congruent:incorrect_correct:verum_sham:OCD_HC:T2_T1:prev_trial_incorrect_correct")


# *Note: For the random effects, we needed to enter the separate random effect terms in the models to enable
# double-bar notation (||). This allows fitting a model that sets correlations of the random terms to zero.
```
<br>

### LMM RT

I decided to favor a model that only predicts all main effects and interactions only with the factor stimulation (rt ~ stimulation * (group + response type + stimulus type). This is the most sparse model for testing the effect of stimulation on RT (the random effects are specified according to the fixed effects). Other models could also be specified, additionally testing interactions with the factor group (rt ~ stimulation * group * (response type + stimulus type) or testing  all interactions (rt ~ stimulation * group * response type * stimulus type). Specifically, these models additionally test the following effects:  group:response_type, group:stimulus_type, group:response_type:stimulation, and group:stimulus_type:stimulation (this list refers to when congruent errors are excluded). Of these, group:stimulus_type was significant with p = .05 in the first preliminary analysis. But to me, these effects are of no prior interest, and/or difficult to interpret. Thus, I chose the simple model that only tests interactions with the factor stimulation.

I thought about including session as fixed effect. But first, this was not stated in preregistration, and second, there is only a main effect, no interaction with stimulation (there is a trend though, but I do not find this effect to be meaningfully interpretable).

Furthermore, I decided to test the stimulation effect on PES and PEA/PCA in sperate models. Including the factor correctness of previous trial would have led to overly complex models. Additionally, all trials would have been included then, preventing to use the robust (and preregistered) PES quantification by Dutilh et al., (2012).

```{r LMM-RT}

# Run model with maximal random-effects structure (I skipped model allowing correlations between radom effects here; check its convergence later)
LMM_rt <- lmer(rt_log ~ stimulation * (group + response_type + stimulus_type) +
  (1 + incongruent_congruent + incorrect_correct + verum_sham + verum_sham:incongruent_congruent + verum_sham:incorrect_correct || participant_id),
data = single_trial_data_clean[is.na(single_trial_data_clean$congruent_error),],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_rt) # Model does converge


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_rt)) # All terms explain variance


# Display results (fixed effects)
tab_model(LMM_rt,
  dv.labels = "log(RT)", show.stat = TRUE, show.icc = FALSE, show.r2 = FALSE, 
  show.re.var = FALSE, show.ngroups = FALSE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite"
)
```
<br>
Responses are faster in the congruent than in the incongruent condition. Incorrect responses are faster than correct responses. There is no evidence for an effect of group or stimulation. 
<br><br>

### LMM PES

```{r LMM-PES}

# Run model with maximal random-effects structure (I skipped model allowing correlations between radom effects here; check its convergence later)
LMM_pes <- lmer(pes ~ stimulation * group + stimulus_type +
  (1 + verum_sham + stimulus_type || participant_id),
data = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_pes) # Model does converge


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_pes)) # All terms explain variance


# Display results (fixed effects)
tab_model(LMM_pes,
  dv.labels = "PES", show.stat = TRUE, show.icc = FALSE, show.r2 = FALSE, 
  show.re.var = FALSE, show.ngroups = FALSE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite"
)
```
<br>
There is PES. "One potential interfering factor in PES can be the flanker congruency of the respective trial." There was no effect of congruency on trial n on PES. There is no evidence for an effect of group or stimulation. When excluding congruent errors, there is a trend for more PES in OCD compared to HC. 
<br><br>

### GLMM Accuracy

I included the factor accuracy of previous trial in the model, to directly test post-error/post-correct accuracy (post-error increase of accuracy) and the effect of stimulation on it in the same model. This was not preregistered.

OR 

Accuracy was first analyzed with only stimulation, group and stimulus type as fixed factors. Second, we included the factor previous accuracy to directly test the difference between post-error vs. post-correct accuracy (post-error increase of accuracy) and the effect of stimulation on it. This was not preregistered.

```{r GLMM-accuracy}

# Run model with maximal random-effects structure (I skipped model allowing correlations between radom effects here; check its convergence later)
GLMM_accuracy <- glmer(accuracy_numeric ~ stimulation * (group + stimulus_type + accuracy_prev_trial) +
  (1 + incongruent_congruent + verum_sham + prev_trial_incorrect_correct + verum_sham:incongruent_congruent + verum_sham:prev_trial_incorrect_correct  || participant_id),
data = single_trial_data_clean,
family = binomial,
control = glmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(GLMM_accuracy) # Model does converge


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(GLMM_accuracy)) # All terms explain variance


# Display results (fixed effects)
tab_model(GLMM_accuracy,
  dv.labels = "Accuracy", show.stat = TRUE, show.icc = FALSE, show.r2 = FALSE, 
  show.re.var = FALSE, show.ngroups = FALSE, string.est = "b", string.stat = "z value", 
  string.ci = "95 % CI", string.p = "p value"
)
```
<br>
Accuracy is lower in the incongruent condition than in the congruent condition. Furthermore, accuracy is higher on trials following incorrect responses than on trials following correct responses (post-error increase of accuracy). There was no interaction involving congruency on trial n and response type on trial n − 1. There is no evidence for an effect of group or stimulation. (When removing congruent errors from this analysis, there is also an effect of group, with patients being more accurate than control participants.)
<br><br>


# Further thoughts 

Include factor congruence for PES to avoid confound with congruency?

See Riesel:  Post-error slowing was analyzed by comparing the difference between reaction times following correct incongruent trials with reaction times following errors in incongruent trials. Only incongruent trials were included in this analysis in order to avoid a confound with the congruency effect, and because errors occurred mainly in incongruent trials

also adjust in plots when excluding congruent errors!