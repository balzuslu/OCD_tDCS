---
title: "Behavioral Performance"
output: 
  html_document

---

<!-- Set general settings -->

```{r setup, include = FALSE}

# Set general settings for markdown file
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  comment = "",
  results = "hold"
)


# Clear environment
rm(list = ls())


# Enable/disable caching of time-consuming code chunks
knitr_cache_enabled = TRUE


# Load packages
library(dplyr)      # for data manipulation
library(knitr)      # for integrating computing and reporting in markdown
library(kableExtra) # for customizing appearance of tables
library(ggplot2)    # for plotting
library(cowplot)    # for arranging plots
library(e1071)      # for functions skewness and kurtosis
library(MASS)       # for boxcox function and contrast definition
library(lme4)       # for (G)LMMs
library(lmerTest)   # for LMM p values (Satterthwaite's method for approximating dfs for the t and F tests)
library(sjPlot)     # for tab_model function to display (G)LMM results
library(performance)# for check of model various assumptions
library(influence.ME) # to calculate Cook's distance
library(emmeans)    # for pairwise comparisons
library(afex)       # for ANOVAs (convenience functions, e.g. for nice display)
library(effectsize) # for effect sizes (t_to_d function)
library(tidyr)      # for reshape function


# Load functions
source("./functions/summarySEwithinO.R")  # Function provided by R-cookbook: http://www.cookbook-r.com/Graphs/Plotting_means_and_error_bars_(ggplot2)/
source("./functions/my_table_template.R") # Function to create table template


# Turn off scientific notation
options(scipen = 999)


# Set figure theme and colors
my_figure_theme <- theme_classic(base_size = 11) +
  theme(legend.position = "bottom", 
        strip.background = element_rect(fill="grey95", linetype = "blank"),
        axis.ticks.x = element_blank(), 
        plot.title = element_text(hjust = 0.5)) 
# instad of theme_classic: + theme_apa(base_size = 11)

my_figure_colors <- c("#8ea6b4", "#465369")
```

## Data Cleaning
***

```{r load-and-clean-data, cache = knitr_cache_enabled}

# Load data
load(file = "./data/Single_Trial_Data.rda")
load(file = "./data/Feedback_Infos.rda")


# Exclude P_02 (due to retainer) and C_02 (as preregistered: patients are excluded with their match)
single_trial_data <- single_trial_data[single_trial_data$participant_id != "P_02" & single_trial_data$participant_id != "C_02",]
feedback_infos    <- feedback_infos[feedback_infos$participant_id != "P_02" & feedback_infos$participant_id != "C_02",]


# Create some variables: numeric word accuracy, correctness of previous response, post-error accuracy (pea), and post-correct accuracy (pca)
# Important to do this before trial exclusion! Otherwise the previous trial in the table might not have been the previous trial in the task!
single_trial_data <- single_trial_data %>%
  dplyr::mutate(
    accuracy_numeric = ifelse(response_type == "correct", 1, 0),
    accuracy_prev_trial = ifelse(lag(response_type == "correct", default = TRUE) == TRUE, "correct", "incorrect"),
    pea = ifelse(lag(response_type == "correct") == FALSE & response_type == "correct", 1,
                 ifelse(lag(response_type == "correct") == FALSE & response_type == "incorrect", 0, NA)),
    pca = ifelse(lag(response_type == "correct") == TRUE & response_type == "correct", 1,
                 ifelse(lag(response_type == "correct") == TRUE & response_type == "incorrect", 0, NA))
    ) 

# For each first trial in a block, accuracy_prev_trial cannot be determined; set these values to NA
single_trial_data[single_trial_data$trial == 1   | single_trial_data$trial == 81 |
                  single_trial_data$trial == 161 | single_trial_data$trial == 241| 
                  single_trial_data$trial == 321 | single_trial_data$trial == 401, "accuracy_prev_trial"] <- NA


# For each last trial in a block, pea and pca cannot be determined; set these values to NA
single_trial_data[single_trial_data$trial == 80  | single_trial_data$trial == 160|
                  single_trial_data$trial == 240 | single_trial_data$trial == 320| 
                  single_trial_data$trial == 400 | single_trial_data$trial == 480, c("pea", "pca")] <- NA


# Exclude missing responses, RT outliers and trials with ERP artifacts
single_trial_data_clean <- single_trial_data %>%
  dplyr::filter(
      response_type != "miss" &
      rt_invalid  == FALSE &
      !is.nan(MFN_0_100_FCz)
  ) # (54998 of 55680 trials left)


# Create specific condition column (to exclude congruent errors if necessary)
single_trial_data_clean <- single_trial_data_clean %>%
  dplyr::mutate(
    condition = as.factor(paste0(stimulus_type, "_", response_type))
  ) 


# Create column with single-trial PES (RTpost-error − RTpre-error for all CCEC sequences)
single_trial_data_clean$pes <- NA
# also make sure to exclude sequences where a trial was excluded in between (or when error occurred in first or last trial)
for (i in 3:(nrow(single_trial_data_clean)-1)) {
  if (single_trial_data_clean[i,]$response_type == "incorrect" &
      single_trial_data_clean[(i+1),]$response_type == "correct" &
      single_trial_data_clean[(i-1),]$response_type == "correct" &
      single_trial_data_clean[(i-2),]$response_type == "correct" &
      single_trial_data_clean[(i+1),]$trial - single_trial_data_clean[(i-1),]$trial == 2) {
    single_trial_data_clean[i,]$pes <- (single_trial_data_clean[(i+1),]$rt) - (single_trial_data_clean[(i-1),]$rt)
  }
}


# For each last first and trial in a block, PES cannot be determined; set these values to NA
single_trial_data_clean[single_trial_data_clean$trial == 80  | single_trial_data_clean$trial ==81  |
                        single_trial_data_clean$trial == 160 | single_trial_data_clean$trial == 161| 
                        single_trial_data_clean$trial == 240 | single_trial_data_clean$trial == 241|
                        single_trial_data_clean$trial == 320 | single_trial_data_clean$trial == 321|
                        single_trial_data_clean$trial == 400 | single_trial_data_clean$trial == 401| 
                        single_trial_data_clean$trial == 480, "pes"] <- NA


# Add column for number of errors (needed as covariate later) - this variable contains total number of errors, not only those entering analysis (preferred according to JK)
single_trial_data_clean <- single_trial_data %>% 
  dplyr::group_by(participant_id, session) %>% 
  dplyr::summarize(number_errors = sum(response_type == "incorrect")) %>%
  dplyr::ungroup(.) %>%
  dplyr::left_join(single_trial_data_clean, ., by = c("participant_id", "session"))
# Standardize this variable
single_trial_data_clean$number_errors_standardized <- scale(single_trial_data_clean$number_errors, center = TRUE, scale = TRUE)


# Add column for number of speeding (needed as covariate later) 
single_trial_data_clean <- feedback_infos[feedback_infos$block != 6,] %>% 
  dplyr::group_by(participant_id, session, feedback) %>%
  dplyr::count(feedback, .drop = FALSE) %>%
  dplyr::filter(feedback == " schneller") %>%
  dplyr::rename(number_feedback_faster = n) %>%
  dplyr::ungroup(.) %>%  
  dplyr::select(-feedback) %>%
  dplyr::left_join(single_trial_data_clean, ., by = c("participant_id", "session"))
# Standardize this variable
single_trial_data_clean$number_feedback_faster_standardized <- scale(single_trial_data_clean$number_feedback_faster, center = TRUE, scale = TRUE)


# Make categorical variables factors
single_trial_data_clean$participant_id      <- as.factor(single_trial_data_clean$participant_id) 
single_trial_data_clean$group               <- as.factor(single_trial_data_clean$group)
single_trial_data_clean$session             <- as.factor(single_trial_data_clean$session)
single_trial_data_clean$stimulation         <- as.factor(single_trial_data_clean$stimulation)
single_trial_data_clean$stimulus_type       <- as.factor(single_trial_data_clean$stimulus_type)
single_trial_data_clean$response_type       <- as.factor(single_trial_data_clean$response_type)
single_trial_data_clean$response_type_2nd   <- as.factor(single_trial_data_clean$response_type_2nd)
single_trial_data_clean$accuracy_prev_trial <- as.factor(single_trial_data_clean$accuracy_prev_trial)


# Calculate aggregated data per subject for boxplots,  outlier detection, and ANOVAs
df_aggregated_per_subject_rt <- single_trial_data_clean %>%
  dplyr::group_by(participant_id, group, response_type, stimulus_type, stimulation, session) %>%
  dplyr::summarize(
    rt = mean(rt, na.rm = TRUE)
    ) 

df_aggregated_per_subject_accuracy <- single_trial_data_clean %>%
  dplyr::group_by(participant_id, group, stimulus_type, stimulation, session) %>%
  dplyr::summarize(
    accuracy_numeric = mean(accuracy_numeric, na.rm = TRUE)*100
    ) 

df_aggregated_per_subject_pes_pea_pca <- single_trial_data_clean %>%
  dplyr::group_by(participant_id, group, stimulation, session) %>%
  dplyr::summarize(
    pes = mean(pes, na.rm = TRUE),
    pea = mean(pea, na.rm = TRUE)*100,
    pca = mean(pca, na.rm = TRUE)*100
  ) 
```

Trials were excluded from all analyses if RT was shorter than 100 ms or longer than 800 ms or if the response in a trial was missing. We further discarded trials containing artifacts in the EEG, i.e., a voltage difference exceeding 50 μV between two consecutive sampling points or 200 μV within an epoch. Please see section "ERP Analysis" for percentage of excluded trials. 

```{r performance-outliers}

# Detect performance outliers (accuracy deviates more than 2/3 SD below/above group mean per condition)
performance_outliers <- df_aggregated_per_subject_accuracy %>% 
  dplyr::group_by(group, stimulus_type, stimulation) %>%
  dplyr::mutate(outlier_2_sd = case_when(abs(accuracy_numeric - mean(accuracy_numeric, na.rm = TRUE)) <=  2 * sd(accuracy_numeric, na.rm = TRUE)~ FALSE, TRUE ~ TRUE),
                outlier_3_sd = case_when(abs(accuracy_numeric - mean(accuracy_numeric, na.rm = TRUE)) <=  3 * sd(accuracy_numeric, na.rm = TRUE)~ FALSE, TRUE ~ TRUE)
                ) %>%
  dplyr::filter(outlier_2_sd == TRUE) 


# Display performance outliers
my_table_template(performance_outliers, caption = "Performance outliers (accuracy > 2/3 SD below/above group mean per condition (stimulus type x stimulation))")


# Calculate number of errors
number_of_errors <- single_trial_data %>% 
  dplyr::group_by(participant_id, group, session) %>% 
  dplyr::summarize(errors_total    = sum(response_type == "incorrect"), 
                   errors_analyzed = sum(response_type == "incorrect" & !is.nan(MFN_0_100_FCz) & rt_invalid == FALSE)
  ) %>%
  dplyr::group_by(group) %>%
  # Calculate M and SD of the variables
  dplyr::summarize(across(-c(participant_id, session), list(mean,sd,min,max)))
  
                                                                                                 
# Display number of errors
my_table_template(number_of_errors, 
                  caption = "Number of errors", 
                  col_names = c("Group", "M", "SD", "min", "max", "M", "SD", "min", "max"),
                  header_above_config = c(" " = 1, "In total" = 4, "In analysis" = 4)
)
```
For some participants, in the task is more than 2 SD (N sessions = `r nrow(performance_outliers)`) or even 3 SD (N sessions = `r nrow(performance_outliers[performance_outliers$outlier_3_sd == TRUE,])`) below the group mean per condition (stimulus type x stimulation). But their accuracy is still quite good (> 70% in incongruent category), so these will not be excluded. Being an outlier in task performance is also no exclusion criterion specified in the preregistration. No participant committed < 6 errors. Thus, no participant will be excluded based on this criterion.
<br><br>

```{r feedback}

# Create contingency table
feedback_contingency <- table(feedback_infos$feedback, feedback_infos$group)


# Display contingency table
my_table_template(feedback_contingency, caption = "Feedback in the task", row_names = TRUE)


# Calculate Chi-squared test
chisq.test(feedback_infos$feedback, feedback_infos$group)
```
The groups differ in the feedback they received. The OCD group received more feedback emphasizing speed. This is ok and could be expected. 
<br><br>


## Data Inspection {.tabset}
***

### Distribution

```{r inspect-distribution, fig.width = 8, fig.height = 16}

# Plot distribution RT
hist_rt <- ggplot(single_trial_data_clean, aes(x = rt)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "#8ea6b4", size = 1) +
  stat_function(fun = dnorm, args=list(mean = mean(single_trial_data_clean$rt, na.rm = TRUE), 
                                     sd = sd(single_trial_data_clean$rt, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(rt, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram RT", x = "RT", y = "Density") + 
  my_figure_theme

qqplot_rt <- ggplot(single_trial_data_clean, aes(sample = rt)) +
  stat_qq(color = "#8ea6b4") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot RT", x = "Theoretical Quantiles", y = "Sample Quantiles") + 
  my_figure_theme


# Plot distribution log RT
hist_rt_log <- ggplot(single_trial_data_clean, aes(x = rt_log)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "#8ea6b4", size = 1) +
  stat_function(fun = dnorm, args=list(mean = mean(single_trial_data_clean$rt_log, na.rm = TRUE), 
                                     sd = sd(single_trial_data_clean$rt_log, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(rt_log, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram log(RT)", x = "log(RT)", y = "Density") + 
  my_figure_theme

qqplot_rt_log <- ggplot(single_trial_data_clean, aes(sample = rt_log)) +
  stat_qq(color = "#8ea6b4") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot log(RT)", x = "Theoretical Quantiles", y = "Sample Quantiles") + 
  my_figure_theme


# Plot distribution PES
hist_pes <- ggplot(single_trial_data_clean[!is.na(single_trial_data_clean$pes),], aes(x = pes)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "#8ea6b4", size = 1) +
  stat_function(fun = dnorm, args=list(mean = mean(single_trial_data_clean$pes, na.rm = TRUE), 
                                     sd = sd(single_trial_data_clean$pes, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(pes, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram PES", x = "PES", y = "Density") + 
  my_figure_theme

qqplot_pes <- ggplot(single_trial_data_clean[!is.na(single_trial_data_clean$pes),], aes(sample = pes)) +
  stat_qq(color = "#8ea6b4") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot PES", x = "Theoretical Quantiles", y = "Sample Quantiles") + 
  my_figure_theme


# Plot distribution accuracy (we first need the values aggregated per session and participant)
accuracy_histogram <- single_trial_data_clean %>%
  dplyr::group_by(participant_id, session) %>%
  dplyr::summarize(mean_accuracy = sum(accuracy_numeric) / length(participant_id) * 100)

hist_accuracy <- ggplot(accuracy_histogram, aes(x = mean_accuracy)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "#8ea6b4", size = 1) +
  stat_function(fun = dnorm, args = list(mean = mean(accuracy_histogram$mean_accuracy, na.rm = TRUE), 
                                     sd = sd(accuracy_histogram$mean_accuracy, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(mean_accuracy, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram Mean Accuracy", x = "Mean Accuracy", y = "Density") + 
  my_figure_theme

qqplot_accuracy <- ggplot(accuracy_histogram, aes(sample = mean_accuracy)) +
  stat_qq(color = "#8ea6b4") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot Accuracy", x = "Theoretical Quantiles", y = "Sample Quantiles") + 
  my_figure_theme


# Plot distribution PEA 
hist_pea <- ggplot(df_aggregated_per_subject_pes_pea_pca, aes(x = pea)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "#8ea6b4", size = 1) +
  stat_function(fun = dnorm, args = list(mean = mean(df_aggregated_per_subject_pes_pea_pca$pea, na.rm = TRUE), 
                                     sd = sd(df_aggregated_per_subject_pes_pea_pca$pea, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(pea, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram Mean Post-Error Accuracy", x = "Post-Error Accuracy", y = "Density") + 
  my_figure_theme

qqplot_pea <- ggplot(df_aggregated_per_subject_pes_pea_pca, aes(sample = pea)) +
  stat_qq(color = "#8ea6b4") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot Mean Post-Error Accuracy", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  my_figure_theme


# Plot distribution PCA 
hist_pca <- ggplot(df_aggregated_per_subject_pes_pea_pca, aes(x = pca)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "#8ea6b4", size = 1) +
  stat_function(fun = dnorm, args = list(mean = mean(df_aggregated_per_subject_pes_pea_pca$pca, na.rm = TRUE), 
                                     sd = sd(df_aggregated_per_subject_pes_pea_pca$pca, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(pca, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram Mean Post-Correct Accuracy", x = "Post-Correct Accuracy", y = "Density") + 
  my_figure_theme

qqplot_pca <- ggplot(df_aggregated_per_subject_pes_pea_pca, aes(sample = pca)) +
  stat_qq(color = "#8ea6b4") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot Mean Post-Correct Accuracy", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  my_figure_theme


ggdraw() +
  draw_plot(hist_rt,        x =  0,   y = .80, width = .5,  height = .16) +
  draw_plot(qqplot_rt,      x =  .5,  y = .80, width = .5,  height = .16) +
  draw_plot(hist_rt_log,    x =  0,   y = .64, width = .5,  height = .16) +
  draw_plot(qqplot_rt_log,  x =  .5,  y = .64, width = .5,  height = .16) +
  draw_plot(hist_pes,       x =  0,   y = .48, width = .5,  height = .16) +
  draw_plot(qqplot_pes,     x =  .5,  y = .48, width = .5,  height = .16) +
  draw_plot(hist_accuracy,  x =  0,   y = .32, width = .5,  height = .16) +
  draw_plot(qqplot_accuracy,x =  .5,  y = .32, width = .5,  height = .16) +
  draw_plot(hist_pea,       x =  0,   y = .16, width = .5,  height = .16) +
  draw_plot(qqplot_pea,     x =  .5,  y = .16, width = .5,  height = .16) +
  draw_plot(hist_pca,       x =  0,   y = 0,   width = .5,  height = .16) +
  draw_plot(qqplot_pca,     x =  .5,  y = 0,   width = .5,  height = .16) 
```
<br><br>

### RT per participant 

```{r plot-RT-per-subject-and-response-type, fig.width = 12, fig.height = 20, cache = knitr_cache_enabled}

rt_per_participant <- ggplot(single_trial_data_clean, aes(x = response_type, y = rt)) + 
  geom_point(position = "jitter", aes(color = stimulus_type)) + 
  ggtitle("RT per participant") + 
  my_figure_theme + 
  facet_wrap(~ participant_id + session, ncol = 10) +
  scale_color_manual(values = my_figure_colors) 
rt_per_participant
```
<br><br>

### Check RT Normality 

For the single-trial data, Shapiro-Wilk is not suitable, as it always returns a significant result for such large samples (additionally, it can handle only samples up to 5000). Hence, we have to rely on visual inspection (see tab "Distribution") and values of skewness and kurtosis (see below). Values for skewness and kurtosis between -2 and +2 are considered acceptable in order to prove normal univariate distribution (George & Mallery, 2010).

```{r normality-single-trial-RTs}

normality_rt <- round(data.frame(matrix(c(skewness(single_trial_data_clean$rt),
                                          kurtosis(single_trial_data_clean$rt),
                                          skewness(single_trial_data_clean$rt_log),
                                          kurtosis(single_trial_data_clean$rt_log),
                                          skewness(single_trial_data_clean[!is.na(single_trial_data_clean$pes),]$pes),
                                          kurtosis(single_trial_data_clean[!is.na(single_trial_data_clean$pes),]$pes)),
                                        nrow=2,ncol=3)),digits = 1)
rownames(normality_rt) <- c("Skewness","Kurtosis")
colnames(normality_rt) <- c("RT","log(RT)", "PES")

my_table_template(normality_rt, row_names = TRUE)
```
<br><br>

### Determine RT transformation

LMM analysis of RT will be conducted on log-transformed RT values to meet the assumption of normally distributed residuals. The appropriate transformation was determined (or rather "validaated", see notes below) using the Box–Cox procedure (Box & Cox, 1964). 

```{r RT-determine-transformation, fig.width = 8, fig.height = 3}

# Arrange plots
par(mfrow = c(1, 2)) 

# Determine transformation of RT by estimating optimal lambda using Box–Cox procedure
bc_rt <- boxcox(rt ~ 1, data = single_trial_data_clean)
optlambda_rt <- bc_rt$x[which.max(bc_rt$y)]

# Determine transformation of PES by estimating optimal lambda using Box–Cox procedure
bc_pes <- boxcox(pes+1000 ~ 1, data = single_trial_data_clean[!is.na(single_trial_data_clean$pes),])
optlambda_pes <- bc_pes$x[which.max(bc_pes$y)]

# Reset plot layout
par(mfrow = c(1, 1)) 
```
For RT (left plot), the optimal lambda is `r round(optlambda_rt, digits = 2)`, suggesting that log transformation (for lambda = 0) is appropriate. Actually, for lambda = -0.5, the most appropriate transformation would be Y^-0.5 = 1/(√(Y)), but this transformation does not seem very common to me. As our lambda is not far from 0, I chose log transformation, which is more commonly used. 
For PES (right plot), the optimal lambda is `r round(optlambda_pes, digits = 2)`, suggesting that no transformation (for lambda = 1) is needed.
<br><br>

## Descriptive Statistics {.tabset}
***

### Means and CIs

```{r descriptive-statistics-table}

##### RT
# Calculate descriptive statistics for RT per condition
descriptive_statistics_rt <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "rt",
  withinvars    = c("response_type", "stimulus_type", "stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Format confidence interval column
  dplyr::mutate(
    ci_rt = paste0("[", round(rt - ci, digits = 0), 
                  ", ", round(rt + ci, digits = 0), "]")) %>%
  # Round RT means to zero decimals
  dplyr::mutate_at("rt", round, digits = 0) %>%    
  # Select columns to be displayed
  dplyr::select(c("group", "response_type", "stimulus_type", "stimulation", "session", "rt", "ci_rt", "ci"))


# Split and re-merge RT table to display both groups next to each other
descriptive_statistics_rt_display <-  split(descriptive_statistics_rt, descriptive_statistics_rt$group)
descriptive_statistics_rt_display <-  left_join(descriptive_statistics_rt_display$HC, descriptive_statistics_rt_display$OCD, by = c("stimulus_type", "response_type", "stimulation", "session"))


# Display descriptive statistics for RT (and select columns)
my_table_template(descriptive_statistics_rt_display[,c(2:7,10:11)],
  caption = "Behavioral Performance: RT (in ms)",
  col_names = c("Response type", "Stimulus type", "Stimulation", "Session", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 4, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)



##### Accuracy
# Calculate descriptive statistics for accuracy per condition
descriptive_statistics_accuracy <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "accuracy_numeric",
  withinvars    = c("stimulus_type", "stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100)
  )  %>%  
  # Format confidence interval column
  dplyr::mutate(
    ci_accuracy = paste0("[", round(accuracy_numeric - ci, digits = 2), 
                        ", ", round(accuracy_numeric + ci, digits = 2), "]")) %>%
  # Round accuracy means to two decimals
  dplyr::mutate_at("accuracy_numeric", round, digits = 2) %>% 
  # Select columns to be displayed
  dplyr::select(c("group", "stimulus_type", "stimulation", "session", "accuracy_numeric", "ci_accuracy", "ci"))


# Split and re-merge accuracy table to display both groups next to each other
descriptive_statistics_accuracy_display <-  split(descriptive_statistics_accuracy, descriptive_statistics_accuracy$group)
descriptive_statistics_accuracy_display <-  left_join(descriptive_statistics_accuracy_display$HC, descriptive_statistics_accuracy_display$OCD, by = c("stimulus_type", "stimulation", "session"))


# Display descriptive statistics for Accuracy (and select columns)
my_table_template(descriptive_statistics_accuracy_display[,c(2:6,9:10)],
  caption = "Behavioral Performance: Accuracy (in %)",
  col_names = c("Stimulus type", "Stimulation", "Session", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 3, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)



##### PES
# Calculate descriptive statistics for PES per condition
descriptive_statistics_pes <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
  measurevar    = "pes",
  withinvars    = c("stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Format confidence interval column
  dplyr::mutate(
    ci_pes = paste0("[", round(pes - ci, digits = 0), 
                   ", ", round(pes + ci, digits = 0), "]")) %>%
  # Round PES means to zero decimals
  dplyr::mutate_at("pes", round, digits = 0) 


# Split and re-merge PEA table to display both groups next to each other
descriptive_statistics_pes_display <-  split(descriptive_statistics_pes, descriptive_statistics_pes$group)
descriptive_statistics_pes_display <-  left_join(descriptive_statistics_pes_display$HC, descriptive_statistics_pes_display$OCD, by = c("stimulation", "session"))


# Display descriptive statistics for PES (and select columns)
my_table_template(descriptive_statistics_pes_display[,c(2,3,5,10,13,18)],
  caption = "Behavioral Performance: Post-Error Slowing (in %)",
  col_names = c("Stimulation", "Session", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 2, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)



##### PEA
# Calculate descriptive statistics for post-error accuracy per condition
descriptive_statistics_pea <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pea),],
  measurevar    = "pea",
  withinvars    = c("stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100)
  )  %>%  
  # Format confidence interval column
  dplyr::mutate(
    ci_pea = paste0("[", round(pea - ci, digits = 2), 
                   ", ", round(pea + ci, digits = 2), "]")) %>%
  # Round PEA means to two decimals
  dplyr::mutate_at("pea", round, digits = 2) 

# Split and re-merge PEA table to display both groups next to each other
descriptive_statistics_pea_display <-  split(descriptive_statistics_pea, descriptive_statistics_pea$group)
descriptive_statistics_pea_display <-  left_join(descriptive_statistics_pea_display$HC, descriptive_statistics_pea_display$OCD, by = c("stimulation", "session"))


# Display descriptive statistics for PEA (and select columns)
my_table_template(descriptive_statistics_pea_display[,c(2,3,5,10,13,18)],
  caption = "Behavioral Performance: Post-Error Accuracy (in %)",
  col_names = c("Stimulation", "Session", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 2, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)



##### PCA
# Calculate descriptive statistics for post-correct accuracy per condition
descriptive_statistics_pca <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pca),],
  measurevar    = "pca",
  withinvars    = c("stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100)
  )  %>%  
  # Format confidence interval column
  dplyr::mutate(
    ci_pca = paste0("[", round(pca - ci, digits = 2), 
                   ", ", round(pca + ci, digits = 2), "]")) %>%
  # Round PCA means to two decimals
  dplyr::mutate_at("pca", round, digits = 2) 


# Split and re-merge PCA table to display both groups next to each other
descriptive_statistics_pca_display <-  split(descriptive_statistics_pca, descriptive_statistics_pca$group)
descriptive_statistics_pca_display <-  left_join(descriptive_statistics_pca_display$HC, descriptive_statistics_pca_display$OCD, by = c("stimulation", "session"))


# Display descriptive statistics for PCA (and select columns)
my_table_template(descriptive_statistics_pca_display[,c(2,3,5,10,13,18)],
  caption = "Behavioral Performance: Post-Correct Accuracy (in %)",
  col_names = c("Stimulation", "Session", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 2, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)


# Calcuate means and CIs adjusted for within-participant factors (without session) - for plots
descriptive_statistics_rt_no_session <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "rt",
  withinvars    = c("response_type", "stimulus_type", "stimulation"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
)

descriptive_statistics_accuracy_no_session <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "accuracy_numeric",
  withinvars    = c("stimulus_type", "stimulation"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100))

descriptive_statistics_pes_no_session <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
  measurevar    = "pes",
  withinvars    = "stimulation",
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
)

descriptive_statistics_pea_no_session <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pea),],
  measurevar    = "pea",
  withinvars    = "stimulation",
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100))

descriptive_statistics_pca_no_session <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pca),],
  measurevar    = "pca",
  withinvars    = "stimulation",
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100))
```
<br><br>

### Plot without session {.active}

```{r descriptive-statistics-plot-rt-acc, fig.width = 8, fig.height = 7, fig.cap = "Note. (A) RT, (B) accuracy, (c) post-error slowing and (D) post-error accuracy, and (E) post-correct accuracy in the flanker task are shown as a function of stimulus type, response type, stimulation condition, and group. Means and 95% confidence intervals (shown in orange/red) were calculated based on single-trial data. Boxplots are based on data aggregated by participant. CIs are adjusted for within-participant designs as described by Morey (2008)."}

# Create plot RT 
plot_rt <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_rt, aes(x = stimulation, y = rt, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_rt_no_session, aes(x = stimulation, y = rt, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_rt_no_session, aes(x = stimulation, ymax = rt + ci, ymin = rt - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_rt_no_session, aes(x = stimulation, y = rt, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("#b23f00", "#ff9b64"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~response_type + stimulus_type, nrow = 1) +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "RT (ms)")


# Create plot accuracy 
plot_accuracy <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_accuracy, aes(x = stimulation, y = accuracy_numeric, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_accuracy_no_session, aes(x = stimulation, y = accuracy_numeric, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_accuracy_no_session, aes(x = stimulation, ymax = accuracy_numeric + ci, ymin = accuracy_numeric - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_accuracy_no_session, aes(x = stimulation, y = accuracy_numeric, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("#b23f00", "#ff9b64"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~stimulus_type, nrow = 1) +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "Accuracy (%)")


# Create plot PES
plot_pes <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_pes_pea_pca, aes(x = stimulation, y = pes, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_pes_no_session, aes(x = stimulation, y = pes, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_pes_no_session, aes(x = stimulation, ymax = pes + ci, ymin = pes - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_pes_no_session, aes(x = stimulation, y = pes, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("#b23f00", "#ff9b64"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "Post-error slowing (ms)")


# Create plot PEA
plot_pea <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_pes_pea_pca, aes(x = stimulation, y = pea, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_pea_no_session, aes(x = stimulation, y = pea, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_pea_no_session, aes(x = stimulation, ymax = pea + ci, ymin = pea - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_pea_no_session, aes(x = stimulation, y = pea, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("#b23f00", "#ff9b64"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "Post-error accuracy (ms)")


# Create plot PCA
plot_pca <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_pes_pea_pca, aes(x = stimulation, y = pca, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_pca_no_session, aes(x = stimulation, y = pca, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_pca_no_session, aes(x = stimulation, ymax = pca + ci, ymin = pca - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_pca_no_session, aes(x = stimulation, y = pca, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("#b23f00", "#ff9b64"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "Post-correct accuracy (ms)")



# Create common legend for plots (function from http://www.sthda.com/english/wiki/wiki.php?id_contents=7930#add-a-common-legend-for-multiple-ggplot2-graphs)
get_legend <- function(myggplot) {
  tmp      <- ggplot_gtable(ggplot_build(myggplot))
  leg      <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend   <- tmp$grobs[[leg]]
  return(legend)
}
legend    <- get_legend(plot_rt)


# Remove previous legends from plots
plot_rt       <- plot_rt       + theme(legend.position = "none")
plot_accuracy <- plot_accuracy + theme(legend.position = "none")
plot_pes      <- plot_pes      + theme(legend.position = "none")
plot_pea      <- plot_pea      + theme(legend.position = "none")
plot_pca      <- plot_pca      + theme(legend.position = "none")


# Arrange plots
figure_behav <- ggdraw() +
  draw_plot(plot_rt,       x =  0,   y = .5,  width = .65, height = .5) +
  draw_plot(plot_accuracy, x = .65,  y = .5,  width = .35, height = .5) +
  draw_plot(plot_pes,      x =  0,   y = .15, width = .33, height = .4) +
  draw_plot(plot_pea,      x = .33,  y = .15, width = .33, height = .4) +
  draw_plot(plot_pca,      x = .66,  y = .15, width = .33, height = .4) +
  draw_plot(legend,        x = .27,  y = .05, width = .5,  height = .1)   +
  draw_plot_label(c("A", "B", "C", "D", "E"), c(0, .65, 0, .33, .66), c(1, 1, .585, .585, .585), size = 15)


# Save plot
ggsave("./figures/figure_behav.tiff", width = 16, height = 15, units = "cm", dpi=600, compression = "lzw")


# Display plot
figure_behav
``` 
To quantify PES robust, we used the method proposed by Dutilh et al. (2012). See section LMM PES for details. We further quantified accuracy following incorrect and correct responses (post-error accuracy and post-correct accuracy).
<br><br>

### Plot with session

```{r descriptive-statistics-plot-including-session, fig.width = 10, fig.height = 15, fig.cap = "Note. (A) RT, (B) accuracy, (c) post-error slowing and (D) post-error accuracy, and (E) post-correct accuracy in the flanker task are shown as a function of stimulation condition, group, and session. Means and 95% confidence intervals (shown in orange/red) were calculated based on single-trial data. Boxplots are based on data aggregated by participant. CIs are adjusted for within-participant designs as described by Morey (2008)."}

# Create plot RT 
plot_rt_session <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_rt, aes(x = stimulation, y = rt, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_rt, aes(x = stimulation, y = rt, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_rt, aes(x = stimulation, ymax = rt + ci, ymin = rt - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_rt, aes(x = stimulation, y = rt, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("#b23f00", "#ff9b64"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~response_type + stimulus_type + session, nrow = 1) +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "RT (ms)")


# Create plot accuracy 
plot_accuracy_session <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_accuracy, aes(x = stimulation, y = accuracy_numeric, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_accuracy, aes(x = stimulation, y = accuracy_numeric, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_accuracy, aes(x = stimulation, ymax = accuracy_numeric + ci, ymin = accuracy_numeric - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_accuracy, aes(x = stimulation, y = accuracy_numeric, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("#b23f00", "#ff9b64"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~stimulus_type + session, nrow = 1) +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "Accuracy (%)")


# Create plot PES
plot_pes_session <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_pes_pea_pca, aes(x = stimulation, y = pes, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_pes, aes(x = stimulation, y = pes, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_pes, aes(x = stimulation, ymax = pes + ci, ymin = pes - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_pes, aes(x = stimulation, y = pes, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("#b23f00", "#ff9b64"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~session, nrow = 1) +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "Post-error slowing (ms)")


# Create plot PEA
plot_pea_session <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_pes_pea_pca, aes(x = stimulation, y = pea, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_pea, aes(x = stimulation, y = pea, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_pea, aes(x = stimulation, ymax = pea + ci, ymin = pea - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_pea, aes(x = stimulation, y = pea, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("#b23f00", "#ff9b64"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~session, nrow = 1) +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "Post-error accuracy (ms)")


# Create plot PCA
plot_pca_session <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_pes_pea_pca, aes(x = stimulation, y = pca, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_pca, aes(x = stimulation, y = pca, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_pca, aes(x = stimulation, ymax = pca + ci, ymin = pca - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_pca, aes(x = stimulation, y = pca, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("#b23f00", "#ff9b64"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~session, nrow = 1) +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "Post-correct accuracy (ms)")


# Create common legend for plots (function from http://www.sthda.com/english/wiki/wiki.php?id_contents=7930#add-a-common-legend-for-multiple-ggplot2-graphs)
get_legend <- function(myggplot) {
  tmp      <- ggplot_gtable(ggplot_build(myggplot))
  leg      <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend   <- tmp$grobs[[leg]]
  return(legend)
}
legend    <- get_legend(plot_rt_session)


# Remove previous legends from plots
plot_rt_session       <- plot_rt_session       + theme(legend.position = "none")
plot_accuracy_session <- plot_accuracy_session + theme(legend.position = "none")
plot_pes_session      <- plot_pes_session      + theme(legend.position = "none")
plot_pea_session      <- plot_pea_session      + theme(legend.position = "none")
plot_pca_session      <- plot_pca_session      + theme(legend.position = "none")


# Arrange plots
figure_behav_session <- ggdraw() +
  draw_plot(plot_rt_session,       x =  0,   y = .66,  width = 1,   height = .33) +
  draw_plot(plot_accuracy_session, x =  0,   y = .33,  width = .5,  height = .33) +
  draw_plot(legend,                x = .6,   y = .36,  width = .5,  height = .33) +
  draw_plot(plot_pes_session,      x =  0,   y = .0,   width = .33, height = .33) +
  draw_plot(plot_pea_session,      x = .33,  y = .0,   width = .33, height = .33) +
  draw_plot(plot_pca_session,      x = .66,  y = .0,   width = .33, height = .33) +
  draw_plot_label(c("A", "B", "C","D","E"), c(0, 0, 0, 0.33, 0.66), c(1, .66, .33, .33, .33), size = 15)


# Save plot
 ggsave("./figures/figure_behav_session.tiff", width = 20, height = 25, units = "cm", dpi=600, compression = "lzw")


# Display plot
figure_behav_session
```  
To quantify PES robust, we used the method proposed by Dutilh et al. (2012). See section LMM PES for details. We further quantified accuracy following incorrect and correct responses (post-error accuracy and post-correct accuracy).
<br><br>

## (G)LMM Analyses
***

RT, PES, and accuracy were modeled using two linear mixed-effects models (LMM) and a binomial generalized linear mixed-effects model (GLMM), respectively. <br><br>

**Fixed effects**

*Stimulus type (congruent, incongruent), group (HC, OCD), stimulation (verum, sham), and, where applicable (= for RT), response type (correct, incorrect)* were specified as fixed factors. Fixed effects were coded using effect coding (this equals sliding difference contrasts for two levels for factors with two levels or sum coding/2), such that the intercept reflects the grand mean across all conditions and differences in means between factor levels are tested. Fixed effects were not eliminated using model comparison techniques because they correspond to the original experimental design and a priori hypotheses. <br><br>

I specified models that predict all main effects and interactions only with the factors stimulation and group. This is the most sparse model structure for testing the effects of interest.<br><br>

**Random effects**

Participants were specified as random factors. The random-effects structure for each model was determined based on the procedure proposed by Bates, Kliegl, et al. (2015). We started with the maximal random-effects structure, including random intercepts for participants, as well as random slopes for all main effects and interactions specified as fixed effects that were justified by the design. If the model with the maximal random-effects structure would not converge, correlations of the random terms were set to zero. We performed a principal components analysis on the random-effects variance–covariance estimates to determine the number of components supported by the data and removed random effects explaining zero variance to prevent overparametrization (Matuschek et al., 2017).

```{r RT-accuracy-(G)LMM-contrast-coding}

# Define contrasts (sliding difference contrasts)
contrasts(single_trial_data_clean$stimulation)         <- contr.sdif(2)
contrasts(single_trial_data_clean$group)               <- contr.sdif(2)
contrasts(single_trial_data_clean$response_type)       <- contr.sdif(2)
contrasts(single_trial_data_clean$stimulus_type)       <- contr.sdif(2)
contrasts(single_trial_data_clean$accuracy_prev_trial) <- contr.sdif(2)
contrasts(single_trial_data_clean$session)             <- contr.sdif(2)


# Add contrasts as numerical covariates via model matrix* (specify all possible contasts for now)
model_matrix <- model.matrix(~ stimulation * group * (response_type + stimulus_type), single_trial_data_clean)


# Attach the model matrix (8 columns) to the dataframe
single_trial_data_clean[, (ncol(single_trial_data_clean) + 1):(ncol(single_trial_data_clean) + 12)] <- model_matrix


# Assign descriptive names to the contrasts
names(single_trial_data_clean)[(ncol(single_trial_data_clean) - 11):ncol(single_trial_data_clean)] <- c("Grand Mean", "verum_sham", "OCD_HC", "incorrect_correct", "incongruent_congruent", "verum_sham:OCD_HC", "verum_sham:incorrect_correct", "verum_sham:incongruent_congruent", "OCD_HC:incorrect_correct" , "OCD_HC:incongruent_congruent", "verum_sham:OCD_HC:incorrect_correct", "verum_sham:OCD_HC:incongruent_congruent")


# *Note: For the random effects, we needed to enter the separate random effect terms in the models to enable
# double-bar notation (||). This allows fitting a model that sets correlations of the random terms to zero.
```
<br><br>

### RT {.tabset}

#### LMM

Even though congruent errors were very rare (`r round(nrow(single_trial_data_clean[single_trial_data_clean$response_type == "incorrect" & single_trial_data_clean$stimulus_type == "congruent",])/nrow(single_trial_data_clean)*100, digits = 2)`% of all trials; N = `r nrow(single_trial_data_clean[single_trial_data_clean$response_type == "incorrect" & single_trial_data_clean$stimulus_type == "congruent",])` trials), I did not exclude them from LMM RT analysis. In order to not loose power, I do not want to exclude congruent errors from analyses in general (e.g. EEG, accuracy), which is ok as I will not add stimulus type as factor there. For the following reasons it should also be ok to not exclude congruent errors from LMM RT analysis:

* LMMs take into account this imbalance due to different number of observations per cell and thus different uncertainty. So, it should be not problematic that congruent errors are rare.

* Additionally, it should be ok that one cell only has few observations, as the interaction response type * stimulus type is not included in the model but only main effects. Not including this interaction in the model is ok, as it is of no interest to our research questions. 

* I thought about whether the effect of stimulus type could confound effect of response type, as errors occur mostly in incongruent trials whereas correct responses are occur in both congruent and incongruent trials. But actually, when both factors are included, the respective main effects reflect the effect of each factor when controlling for the other. 

For the aggregation-based ANOVA on RT, I will excluded congruent errors.

```{r LMM-RT, cache = knitr_cache_enabled}

# Run model with maximal random-effects structure
LMM_rt <- lmer(rt_log ~ verum_sham * OCD_HC * (incorrect_correct + incongruent_congruent) +
  (1 + verum_sham * (incorrect_correct + incongruent_congruent) | participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_rt) # Model does converge 
# isSingular(LMM_rt) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_rt)) # All terms explain variance 


# Display results (fixed effects)
tab_model(LMM_rt,
  dv.labels = "log(RT)", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>
Responses are faster in the congruent than in the incongruent condition. Incorrect responses are faster than correct responses. There is no evidence for an effect of group or stimulation. Trends for the following interactions are present: group:stimulus_congruence and stimulation:group:response_type.
<br><br>

#### Assumption checks

```{r LMM-RT-assumptions, fig.width = 20, fig.height = 15, cache = knitr_cache_enabled}

# Check model assumptions
performance::check_model(LMM_rt, panel = TRUE)

# In addition to plots, print verbal output for some assumption tests to facilitate conclusion
print("# Check for heteroscedasticity")
performance::check_heteroscedasticity(LMM_rt)

print("# Check for influential observations (Cook's distance)")
performance::check_outliers(LMM_rt, effects = "random")

print("# Check for normal distributed random effects")
performance::check_normality(LMM_rt, effects = "random")
```

* **Assumption 1: Independence of Data Points / Absence of collinearity -> Is OK**
    + Are predictors not highly correlated?
    + Multicollinearity plot shows only low correlations 

* **Assumption 2: Normality of Residuals -> Is OK???** 
    + Are residuals approximately normally distributed?
    + Q-Q plot and density plot look fine? Q-Q plot quite a bit off at the extremes 
    + It is debated whether this is problematic at all; and violation does not seem so bad, so maybe not worry about it? 

* **Assumption 3: Linearity -> Is OK** 
    + Is the dependent variable linearly related to the fixed factors, random factors, and covariates?
    + Plot of the residuals against the fitted values shows a random scatter pattern, no nonlinear or curvy pattern 

* **Assumption 4: Homogeneity of Residual Variance (Heteroscedasticity) -> Is OK???**
    + Have residuals constant variance across the range of the predicted values?
    + Plot of the residuals against the fitted values shows an even spread around the centered line; but written output says this is not ok

* **Assumption 5: Absence of Influential Data Points -> Is OK** 
    + Are there are no influential values? 
    + Cook's distance plot looks fine (for large N, Cook's distances should be below 1) and written output says there are no outliers 

* **Assumption 6: Normality of Random Effects -> Is OK**
    + Are random effects approximately normally distributed?
    + Plots look fine; written output says this is (mostly) ok
<br><br>

#### Check covariates

The purpose of including the covariates was to see how the effects change when controlling for the overall effect of the covariate. Thus, covariates were included only as fixed factor, not as random term. I first included the covariates as main effect only, not allowing any interactions with stimulation or group. However, inspecting the interactions as well might lead to new, important insights. These models including the interactions are presented below.
Note: The covariate number of errors refers to the actual number of errors committed by each participant, not the number included in the analyses.

```{r LMM-RT-covariates, cache = knitr_cache_enabled}

# RT check covariate session
LMM_rt_session <- lmer(rt_log ~ verum_sham * OCD_HC * session * (incorrect_correct + incongruent_congruent) +
  (1 + verum_sham * (incorrect_correct + incongruent_congruent) | participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_rt_session,
  dv.labels = "log(RT), covariate session", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# RT check covariate number of errors (predictor was z standardized)
LMM_rt_number_errors <- lmer(rt_log ~ verum_sham * OCD_HC * number_errors_standardized * (incorrect_correct + incongruent_congruent) +
  (1 + verum_sham * (incorrect_correct + incongruent_congruent) | participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_rt_number_errors,
  dv.labels = "log(RT), covariate number of errors", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# RT check covariate number of feedback faster (predictor was z standardized)
LMM_rt_number_feedback_faster <- lmer(rt_log ~ verum_sham * OCD_HC * number_feedback_faster_standardized * (incorrect_correct + incongruent_congruent) +
  (1 + verum_sham * (incorrect_correct + incongruent_congruent) | participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_rt_number_feedback_faster,
  dv.labels = "log(RT), covariate number of feedback faster", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>

### PES {.tabset}

#### LMM

To quantify PES robust, we used the method proposed by Dutilh et al. (2012). "A single-trial value of PES was computed by performing a pairwise comparison of correct trials around each error (RTpost-error − RTpre-error). This method ensures that post-error and post-correct trials originate from the same time periods in the data set and thus controls for global fluctuations in motivation and attention." To avoid the effects of consecutive errors on RTs, we considered only error trials that were preceded by at least two correct responses and followed by at least one correct response (i.e., sequences of CCEC trials, where ‘C’ represents correct trials and ‘E’ represents error trials). 

I decided to test the stimulation effect on PES in a separate model instead of including the factor correctness of previous trial in the RT model. Otherwise, this would have led to an overly complex LMM on RTs. Additionally, all trials would have been included then, preventing to use the robust (and preregistered) PES quantification by Dutilh et al. (2012).

I included stimulus type as covariate (hence, only as fixed, not as random effect) in this model to avoid confounding the PES effect with stimulus congruency, as stimulus congruency of the respective trial might potentially interfere with PES. The results do not change when the covariate is not included. The covariate was included only in the LMM, not in the parallel ANOVA calculated below.

```{r LMM-PES}
# Run model with maximal random-effects structure (to avoid singularity / non-convergence and rePCA value of 0, correlations betw. random terms and radom slope of stimulus_type had to be removed)
LMM_pes <- lmer(pes ~ verum_sham * OCD_HC + incongruent_congruent +
  (1 + verum_sham  || participant_id),
data = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_pes) # Model does converge
# isSingular(LMM_pes) # is also ok


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_pes)) # All terms explain variance


# Display results (fixed effects)
tab_model(LMM_pes,
  dv.labels = "PES (ms)", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>
There is PES (intercept is significant). There is no effect of congruency in the respective on PES. There is no evidence for an effect of group or stimulation. 
<br><br>

#### Assumption checks

```{r LMM-PES-assumptions, fig.width = 20, fig.height = 15}

# Check model assumptions
performance::check_model(LMM_pes, panel = TRUE)

# In addition to plots, print verbal output for some assumption tests to facilitate conclusion
print("# Check for heteroscedasticity")
performance::check_heteroscedasticity(LMM_pes)

print("# Check for influential observations (Cook's distance)")
performance::check_outliers(LMM_pes, effects = "random")

print("# Check for normal distributed random effects")
performance::check_normality(LMM_pes, effects = "random")
```

* **Assumption 1: Independence of Data Points / Absence of collinearity -> Is OK**
    + Are predictors not highly correlated?
    + Multicollinearity plot shows only low correlations 

* **Assumption 2: Normality of Residuals -> Is OK???** 
    + Are residuals approximately normally distributed?
    + Q-Q plot and density plot look fine? Q-Q plot a bit off at the extremes
    + It is debated whether this is problematic at all; and violation does not seem so bad, so maybe not worry about it?  

* **Assumption 3: Linearity -> Is OK** 
    + Is the dependent variable linearly related to the fixed factors, random factors, and covariates?
    + Plot of the residuals against the fitted values shows a random scatter pattern, no nonlinear or curvy pattern 

* **Assumption 4: Homogeneity of Residual Variance (Heteroscedasticity)  -> Is OK???**
    + Have residuals constant variance across the range of the predicted values?
    + Plot of the residuals against the fitted values shows an even spread around the centered line; but written output says this is not ok

* **Assumption 5: Absence of Influential Data Points  -> Is OK** 
    + Are there are no influential values? 
    + Cook's distance plot looks fine (for large N, Cook's distances should be below 1) and written output says there are no outliers 

* **Assumption 6: Normality of Random Effects  -> Is OK???**
    + Are random effects approximately normally distributed?
    + Written output says this is ok (but: standard errors and hence also plot not available due to || syntax) 
<br><br>

#### Check covariates

For further infos on inclusion of covariates, see notes in section "(G)LMM Analyses -> RT", tab "Check covariates".


```{r LMM-PES-covariates, cache = knitr_cache_enabled}

# PES check covariate session
LMM_pes_session <- lmer(pes ~ verum_sham * OCD_HC * session + incongruent_congruent +
  (1 + verum_sham  || participant_id),
data = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_pes_session,
  dv.labels = "PES (ms), covariate session", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# PES check covariate number of errors (predictor was z standardized)
LMM_pes_number_errors <- lmer(pes ~ verum_sham * OCD_HC * number_errors_standardized + incongruent_congruent +
  (1 + verum_sham  || participant_id),
data = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_pes_number_errors,
  dv.labels = "PES (ms), covariate number of errors", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# PES check covariate number of feedback faster (predictor was z standardized)
LMM_pes_number_feedback_faster <- lmer(pes ~ verum_sham * OCD_HC * number_feedback_faster_standardized + incongruent_congruent +
  (1 + verum_sham  || participant_id),
data = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_pes_number_feedback_faster,
  dv.labels = "PES (ms), covariate number of feedback faster", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>

### Accuracy {.tabset}

#### GLMM

I included the factor *accuracy of previous trial* in the model, to directly test the difference between post-error vs. post-correct accuracy (post-error increase of accuracy) and the effect of stimulation on it. This was not preregistered. (If preferred, I could also first run the preregistered model with only stimulation, group and stimulus type as fixed factors and then a second model, including additionally the factor *accuracy of previous trial*. This approach would also allow using all trials for the basic accuracy model, as when including the factor , I automatically lose all trials from the calculation at the start of a new task block, that have NA for the variable *accuracy of previous trial*. But the results remain the same, independent of whether I run the model with all trials or the subset). I did not add *accuracy of previous trial* as random effect here. Doing so would require specifying the model matrix separately for the subset of data where *accuracy of previous trial* is not NA.  

```{r GLMM-accuracy, cache = knitr_cache_enabled}

# Run model with maximal random-effects structure
GLMM_accuracy <- glmer(accuracy_numeric ~ verum_sham * OCD_HC * (incongruent_congruent + accuracy_prev_trial) +
  (1 + verum_sham * incongruent_congruent | participant_id),
data = single_trial_data_clean,
family = binomial,
control = glmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(GLMM_accuracy) # Model does converge
# isSingular(GLMM_accuracy) # is also ok


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(GLMM_accuracy)) # All terms explain variance


# Display results (fixed effects)
tab_model(GLMM_accuracy,
  dv.labels = "Accuracy", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "z value", 
  string.ci = "95 % CI", string.p = "p value", wrap.labels = 80, digits.re = 3
)
```
<br><br>
Accuracy is lower in the incongruent condition than in the congruent condition. Furthermore, accuracy is higher on trials following incorrect responses than on trials following correct responses (post-error increase of accuracy). There is no evidence for an effect of group or stimulation. There is a significant interaction group:stimulus_type and a trend for an interaction stimulation:accuracy_prev_trial.
<br><br>

#### Assumption checks

```{r GLMM-accuracy-assumptions}

# Check for normal distributed random effects
print("Check for normal distributed random effects")
performance::check_normality(GLMM_accuracy, effects = "random")


# Check for overdispersion
print("Check for overdispersion")
overdisp_fun <- function(model) {
  rdf <- df.residual(model)
  rp <- residuals(model,type="pearson")
  Pearson.chisq <- sum(rp^2)
  prat <- Pearson.chisq/rdf
  pval <- pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE)
  c(chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)
}
overdisp_fun(GLMM_accuracy) # If the p-value is < 0.05, the data are overdispersed. Here p > 0.05. Overdispersion is not a problem here. 
```

* **Assumption 1: Chosen Link Function is Appropriate  -> Is OK** 
  
* **Assumption 2: Normality of Random Effects -> Is OK???**
    + Are random effects approximately normally distributed?
    + Written output says this is ok (but: standard errors and hence also plot not available due to || syntax) 

* **Assumption 3: Appropriate Estimation of Variance (No Overdispersion)  -> Is OK**
    + Overdispersion is not a problem here. The empirical variance in data does not exceed the nominal variance under the presumed model. The chi-square test of the ratio of the empirical variance in data and the nominal variance under the presumed model is not significant.
<br><br>

#### Check covariates

For further infos on inclusion of covariates, see notes in section LMM Anylses RT, tab "Check covariates".

```{r LMM-accuracy-covariates, cache = knitr_cache_enabled}

# Accuracy check covariate session (not included in all interactions for reasons of convergence)
GLMM_accuracy_session <- glmer(accuracy_numeric ~ verum_sham * OCD_HC * (incongruent_congruent + accuracy_prev_trial + session) +
  (1 + verum_sham * incongruent_congruent | participant_id),
data = single_trial_data_clean,
family = binomial,
control = glmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(GLMM_accuracy_session,
  dv.labels = "Accuracy, covariate session", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "z value", 
  string.ci = "95 % CI", string.p = "p value", wrap.labels = 80, digits.re = 3
)


# Accuracy check covariate number of feedback faster (predictor was z standardized) (not included in all interactions for reasons of convergence)
GLMM_accuracy_number_feedback_faster <- glmer(accuracy_numeric ~ verum_sham * OCD_HC * (incongruent_congruent + accuracy_prev_trial + number_feedback_faster_standardized) +
  (1 + verum_sham * incongruent_congruent || participant_id),
data = single_trial_data_clean,
family = binomial,
control = glmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(GLMM_accuracy_number_feedback_faster,
  dv.labels = "Accuracy, covariate number of feedback faster", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "z value", 
  string.ci = "95 % CI", string.p = "p value", wrap.labels = 80, digits.re = 3
)
```
<br><br>

## ANOVAs {.tabset}
***
To facilitate comparison with previously reported results obtained using a similar task and aggregation-based analyses (Rainhart & Woodman 2014), RT, PES, and accuracy were additionally analyzed with Greenhouse–Geisser corrected repeated-measures analyses of variance (ANOVAs) including the within-participant factors stimulation (verum, sham), group (OCD, HC), response type (correct, incorrect)/condition (congruent correct, incongruent correct, incongruent incorrect), and accuracy previous trial (correct, incorrect). (GG correction only relevant for the ANOVA on RT, as factors in all other have only two levels, so that sphericity could not be violated). As can be seen below, the ANOVAs yielded similar results as obtained with mixed-effects modeling with respect to all main effects, interaction effects, and pairwise comparisons. I additionally analyzed whether there is an stimulation effect on the number of corrective second responses (because this was erroneously stated in the preregistration). There is none. This analysis requires aggregated data, hence I only calculated an ANOVA, no LMM.

```{r ANOVAs}

# Due to the afex package, contrasts are automatically set to effect-coding (contr.sum). Afex package 
# also checks sphericity assumptions and automatically corrects for any violations if necessary.


# Get data in correct format for ANOVAs (aggregate within participants per condition)
data_anova_rt <- single_trial_data_clean %>%
  # for RT exclude congruent errors -> use factor with 3 levels (correct congr., correct incongr., incorrect incongr.)
  dplyr::filter(
    condition != "congruent_incorrect"
    ) %>%
  # Adjust number of factor levels
  dplyr::mutate(
    condition = as.factor(condition)
    ) %>%
  dplyr::group_by(participant_id, stimulation, group, condition) %>%
  dplyr::summarize(
    rt_log = mean(rt_log, na.rm = TRUE)
  )


data_anova_pes <- single_trial_data_clean[!is.na(single_trial_data_clean$pes),] %>%
  dplyr::group_by(participant_id, stimulation, group) %>%
  dplyr::summarize(
    pes = mean(pes)
  )


data_anova_accuracy <- single_trial_data_clean[!is.na(single_trial_data_clean$accuracy_prev_trial),] %>%
  dplyr::group_by(participant_id, stimulation, group, stimulus_type, accuracy_prev_trial) %>%
  dplyr::summarize(
    percentage_correct_responses = sum(accuracy_numeric) / length(participant_id) * 100
  )


data_anova_number_2nd_resp <- single_trial_data_clean %>%
  dplyr::group_by(participant_id, group, stimulation, session) %>%
  dplyr::summarize(
    number_2nd_resp = sum(!is.na(response_type_2nd[response_type_2nd == "correct"]))
  )
      

# ANOVA RT
anova_rt <- aov_ez(
  id     = "participant_id", 
  dv     = "rt_log", 
  data   = data_anova_rt,
  within = c("stimulation", "condition"),
  between= c("group"),
  observed = c("group")
)


# ANOVA PES
anova_pes <- aov_ez(
  id     = "participant_id", 
  dv     = "pes", 
  data   = data_anova_pes,
  within = c("stimulation"),
  between= c("group"),
  observed = c("group")
)


# ANOVA accuracy
anova_accuracy <- aov_ez(
  id     = "participant_id", 
  dv     = "percentage_correct_responses", 
  data   = data_anova_accuracy,
  within = c("stimulation", "stimulus_type", "accuracy_prev_trial"),
  between= c("group"),
  observed = c("group")
)


# ANOVA number 2nd responses
anova_number_2nd_resp <- aov_ez(
  id     = "participant_id", 
  dv     = "number_2nd_resp", 
  data   = data_anova_number_2nd_resp,
  within = c("stimulation"),
  between= c("group"),
  observed = c("group")
)


# Display ANOVA results
my_table_template(nice(anova_rt,       MSE = FALSE), caption = "RT")
my_table_template(nice(anova_pes,      MSE = FALSE), caption = "PES")                  
my_table_template(nice(anova_accuracy, MSE = FALSE), caption = "Accuracy")
my_table_template(nice(anova_number_2nd_resp, MSE = FALSE), caption = "Number of corrective second responses (only correct)")
```
<br><br>

### Pairwise comparisons

Significant main effects and interactions were followed up with pairwise comparisons using the emmeans package with Holm–Bonferroni *p* value adjustments.

```{r ANOVAs-pairwise-tests}

# Use multivariate model for all follow-up tests to adequately control for violations of sphericity
afex_options(emmeans_model = "multivariate")


# Pairwise t tests
pairwise_rt  <- summary(pairs(emmeans(anova_rt, ~condition), adjust = "holm"))
pairwise_acc <- summary(pairs(emmeans(anova_accuracy, "accuracy_prev_trial", by = "stimulus_type"), adjust = "holm"))



# Add Cohen's dz (CIs for d could be added if needed, as it can be returned by the "t_to_d" function)
pairwise_rt$cohens_dz  <- round(t_to_d(pairwise_rt$t.ratio,  pairwise_rt$df,  paired = TRUE)[1], digits = 2)
pairwise_acc$cohens_dz <- round(t_to_d(pairwise_acc$t.ratio, pairwise_acc$df, paired = TRUE)[1], digits = 2)


# Display results 
my_table_template(pairwise_rt, 
                  digits = c(0, 2, 2, 0, 2, 4, 2),
                  caption = "RT: Main Effect Response Type",
                  footnote = "P values are adjusted with Holm–Bonferroni method.") 


my_table_template(pairwise_acc, 
                  digits = c(0, 0, 2, 2, 0, 2, 3, 2),
                  caption = "Accuracy: Interaction Effect Stimulus Type x Accuracy Previous Trial",
                  footnote = "P values are adjusted with Holm–Bonferroni method.") 
```
<br><br>

### Assumption checks

```{r ANOVAs-assumptions}

# Get residuals
residuals_anova_rt       <- as.data.frame(anova_rt$lm$residuals)
residuals_anova_pes      <- as.data.frame(anova_pes$lm$residuals)
residuals_anova_accuracy <- as.data.frame(anova_accuracy$lm$residuals)


# Plot residuals
ggplot(gather(residuals_anova_rt, cols, value), aes(x = value)) + 
  geom_histogram(color = my_figure_colors[2], fill = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "Histogram Residuals RT") + my_figure_theme

ggplot(gather(residuals_anova_rt, cols, value), aes(sample = value)) + 
  stat_qq(color = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "QQ-Plot Residuals RT") + my_figure_theme

ggplot(gather(residuals_anova_pes, cols, value), aes(x = value)) + 
  geom_histogram(color = my_figure_colors[2], fill = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "Histogram Residuals PES") + my_figure_theme

ggplot(gather(residuals_anova_pes, cols, value), aes(sample = value)) + 
  stat_qq(color = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "QQ-Plot Residuals PES") + my_figure_theme

ggplot(gather(residuals_anova_accuracy, cols, value), aes(x = value)) + 
  geom_histogram(color = my_figure_colors[2], fill = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "Histogram Residuals Accuracy") + my_figure_theme

ggplot(gather(residuals_anova_accuracy, cols, value), aes(sample = value)) + 
  stat_qq(color = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "QQ-Plot Residuals Accuracy") + my_figure_theme


# Test normality of residuals
print("# ANOVA RT: Normality of Residuals with Shapiro Wilk Test")
do.call(rbind, lapply(residuals_anova_rt[,], function(x) shapiro.test(x)["p.value"]))

print("# ANOVA PES: Check Normality of Residuals with Shapiro Wilk Test")
do.call(rbind, lapply(residuals_anova_pes[,], function(x) shapiro.test(x)["p.value"]))

print("# ANOVA Accuracy: Check Normality of Residuals with Shapiro Wilk Test")
do.call(rbind, lapply(residuals_anova_accuracy[,], function(x) shapiro.test(x)["p.value"]))
```

* **Assumption #1: Dependent variable interval or ratio variable -> Is OK**

* **Assumption #2: Balanced design** (each subject has to have a value in each condition) **-> Is OK**

* **Assumption #3: No dependency in the scores between participants** (dependency can exist only across scores for individuals) **-> Is OK**

* **Assumption #4: Residuals** of the dependent variable in each level of the within-subjects factor are approximately **normally distributed -> NOT OK** (see above; but for RT and PES only small deviation)**

* **Assumption #5: Sphericity** (only relevant for within-subject factors with > 2 levels) (afex package automatically corrects (Greenhouse Geisser) for any violations if necessary) **-> Is OK**
<br><br>