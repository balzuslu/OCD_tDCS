---
title: "Behavioral Performance"
output: 
  html_document

---

<!-- Set general settings -->

```{r setup, include = FALSE}

# Set general settings for markdown file
knitr::opts_chunk$set(
  message = FALSE,
  warning = FALSE,
  comment = "",
  results = "hold"
)


# Clear environment
rm(list = ls())


# Enable/disable caching of time-consuming code chunks
knitr_cache_enabled = TRUE


# Load packages
library(dplyr)      # for data manipulation
library(knitr)      # for integrating computing and reporting in markdown
library(kableExtra) # for customizing appearance of tables
library(ggplot2)    # for plotting
library(cowplot)    # for arranging plots
library(e1071)      # for functions skewness and kurtosis
library(MASS)       # for boxcox function and contrast definition
library(lme4)       # for (G)LMMs
library(lmerTest)   # for LMM p values (Satterthwaite's method for approximating dfs for the t and F tests)
library(sjPlot)     # for tab_model function to display (G)LMM results
library(performance)# for check of model various assumptions
library(influence.ME) # to calculate Cook's distance
library(emmeans)    # for pairwise comparisons
library(afex)       # for ANOVAs (convenience functions, e.g. for nice display)
library(effectsize) # for effect sizes (t_to_d function)
library(tidyr)      # for reshape function


# Load functions
source("./functions/summarySEwithinO.R")  # Function provided by R-cookbook: http://www.cookbook-r.com/Graphs/Plotting_means_and_error_bars_(ggplot2)/
source("./functions/my_table_template.R") # Function to create table template


# Turn off scientific notation
options(scipen = 999)


# Set figure theme and colors
my_figure_theme <- theme_classic(base_size = 11) +
  theme(legend.position = "bottom", 
        strip.background = element_rect(fill="grey95", linetype = "blank"),
        axis.ticks.x = element_blank(), 
        plot.title = element_text(hjust = 0.5)) 
# instad of theme_classic: + theme_apa(base_size = 11)

my_figure_colors <- c("tan1","navy","slategray3","sienna3")
```
<br><br> 

## Data Cleaning
***

```{r load-and-clean-data}

# Load data
load(file = "./data/Single_Trial_Data.rda")
load(file = "./data/Feedback_Infos.rda")


# Exclude P_02 (due to retainer) and C_02 (as preregistered: patients are excluded with their match)
single_trial_data <- single_trial_data[single_trial_data$participant_id != "P_02" & single_trial_data$participant_id != "C_02",]
feedback_infos    <- feedback_infos[feedback_infos$participant_id != "P_02" & feedback_infos$participant_id != "C_02",]


# Create some variables: numeric word accuracy, correctness of previous response, post-error accuracy (pea), and post-correct accuracy (pca)
# Important to do this before trial exclusion! Otherwise the previous trial in the table might not have been the previous trial in the task!
single_trial_data <- single_trial_data %>%
  dplyr::mutate(
    accuracy_numeric = ifelse(response_type == "correct", 1, 0),
    accuracy_prev_trial = ifelse(lag(response_type == "correct", default = TRUE) == TRUE, "correct", "incorrect"),
    pea = ifelse(lag(response_type == "correct") == FALSE & response_type == "correct", 1,
                 ifelse(lag(response_type == "correct") == FALSE & response_type == "incorrect", 0, NA)),
    pca = ifelse(lag(response_type == "correct") == TRUE & response_type == "correct", 1,
                 ifelse(lag(response_type == "correct") == TRUE & response_type == "incorrect", 0, NA))
    ) 

# For each first trial in a block, accuracy_prev_trial, pea, and pca cannot be determined; set these values to NA
single_trial_data[single_trial_data$trial == 1   | single_trial_data$trial == 81 |
                  single_trial_data$trial == 161 | single_trial_data$trial == 241| 
                  single_trial_data$trial == 321 | single_trial_data$trial == 401, c("accuracy_prev_trial", "pea", "pca")] <- NA


# Exclude missing responses, RT outliers and trials with ERP artifacts
single_trial_data_clean <- single_trial_data %>%
  dplyr::filter(
      response_type != "miss" &
      rt_invalid  == FALSE &
      !is.na(MFN_0_100_FCz)
  ) # (53093 of 53760 trials left)


# Create specific condition column (to exclude congruent errors if necessary)
single_trial_data_clean <- single_trial_data_clean %>%
  dplyr::mutate(
    condition = as.factor(paste0(stimulus_type, "_", response_type))
  ) 


# Create column with single-trial PES (RTpost-error − RTpre-error for all CCEC sequences)
single_trial_data_clean$pes <- NA
# also make sure to exclude sequences where a trial was excluded in between (or when error occurred in first or last trial)
for (i in 3:(nrow(single_trial_data_clean)-1)) {
  if (single_trial_data_clean[i,]$response_type == "incorrect" &
      single_trial_data_clean[(i+1),]$response_type == "correct" &
      single_trial_data_clean[(i-1),]$response_type == "correct" &
      single_trial_data_clean[(i-2),]$response_type == "correct" &
      single_trial_data_clean[(i+1),]$trial - single_trial_data_clean[(i-1),]$trial == 2) {
    single_trial_data_clean[i,]$pes <- (single_trial_data_clean[(i+1),]$rt) - (single_trial_data_clean[(i-1),]$rt)
  }
}


# For each last first and trial in a block, PES cannot be determined; set these values to NA
single_trial_data_clean[single_trial_data_clean$trial == 1   |
                        single_trial_data_clean$trial == 80  | single_trial_data_clean$trial == 81 |
                        single_trial_data_clean$trial == 160 | single_trial_data_clean$trial == 161| 
                        single_trial_data_clean$trial == 240 | single_trial_data_clean$trial == 241|
                        single_trial_data_clean$trial == 320 | single_trial_data_clean$trial == 321|
                        single_trial_data_clean$trial == 400 | single_trial_data_clean$trial == 401| 
                        single_trial_data_clean$trial == 480, "pes"] <- NA


# Add column for (grand mean standardized) number of errors (needed as covariate later) - this variable contains total number of errors, not only those entering analysis (preferred according to JK)
single_trial_data_clean <- single_trial_data %>% 
  dplyr::group_by(participant_id, session) %>% 
  dplyr::summarize(number_errors = sum(response_type == "incorrect")) %>%
  dplyr::ungroup(.) %>%
  dplyr::mutate(number_errors_standardized = scale(number_errors, center = TRUE, scale = TRUE)) %>%
  dplyr::left_join(single_trial_data_clean, ., by = c("participant_id", "session")) 


# Add column for (grand mean standardized) number of speeding (needed as covariate later) 
single_trial_data_clean <- feedback_infos[feedback_infos$block != 6,] %>% 
  dplyr::group_by(participant_id, session, feedback) %>%
  dplyr::count(feedback, .drop = FALSE) %>%
  dplyr::filter(feedback == " schneller") %>%
  dplyr::rename(number_feedback_faster = n) %>%
  dplyr::ungroup(.) %>%  
  dplyr::select(-feedback) %>%
  dplyr::mutate(number_feedback_faster_standardized = scale(number_feedback_faster, center = TRUE, scale = TRUE)) %>%
  dplyr::left_join(single_trial_data_clean, ., by = c("participant_id", "session")) 


# Make categorical variables factors
single_trial_data_clean$participant_id      <- as.factor(single_trial_data_clean$participant_id) 
single_trial_data_clean$group               <- as.factor(single_trial_data_clean$group)
single_trial_data_clean$session             <- as.factor(single_trial_data_clean$session)
single_trial_data_clean$stimulation         <- as.factor(single_trial_data_clean$stimulation)
single_trial_data_clean$stimulus_type       <- as.factor(single_trial_data_clean$stimulus_type)
single_trial_data_clean$response_type       <- as.factor(single_trial_data_clean$response_type)
single_trial_data_clean$response_type_2nd   <- as.factor(single_trial_data_clean$response_type_2nd)
single_trial_data_clean$accuracy_prev_trial <- as.factor(single_trial_data_clean$accuracy_prev_trial)


# Calculate aggregated data per participant for boxplots,  outlier detection, and ANOVAs
df_aggregated_per_subject_rt <- single_trial_data_clean %>%
  dplyr::group_by(participant_id, group, response_type, stimulus_type, stimulation, session) %>%
  dplyr::summarize(
    rt = mean(rt, na.rm = TRUE)
    )  %>%
  dplyr::ungroup()

df_aggregated_per_subject_accuracy <- single_trial_data_clean %>%
  dplyr::group_by(participant_id, group, stimulus_type, stimulation, session) %>%
  dplyr::summarize(
    accuracy_numeric = mean(accuracy_numeric, na.rm = TRUE)*100
    )  %>%
  dplyr::ungroup()

df_aggregated_per_subject_pes_pea_pca <- single_trial_data_clean %>%
  dplyr::group_by(participant_id, group, stimulation, session) %>%
  dplyr::summarize(
    pes = mean(pes, na.rm = TRUE),
    pea = mean(pea, na.rm = TRUE)*100,
    pca = mean(pca, na.rm = TRUE)*100
  )  %>%
  dplyr::ungroup()
```

Trials were excluded from all analyses if RT was shorter than 100 ms or longer than 800 ms or if the response in a trial was missing. We further discarded trials containing artifacts in the EEG, i.e., a voltage difference exceeding 50 μV between two consecutive sampling points or 200 μV within an epoch. Please see section "ERP Analysis" for percentage of excluded trials. 

```{r performance-outliers}

# Detect performance outliers (accuracy deviates more than 2/3 SD below/above group mean per condition) - based on raw data, not cleaned data
performance_outliers <- single_trial_data %>%
  dplyr::group_by(participant_id, group, stimulation, stimulus_type, session) %>%
  dplyr::summarize(accuracy_numeric = mean(accuracy_numeric, na.rm = TRUE)*100)  %>%
  dplyr::ungroup() %>% 
  dplyr::group_by(group, stimulus_type, stimulation) %>%
  dplyr::mutate(outlier_2_sd = case_when(abs(accuracy_numeric - mean(accuracy_numeric, na.rm = TRUE)) <=  2 * sd(accuracy_numeric, na.rm = TRUE)~ FALSE, TRUE ~ TRUE),
                outlier_3_sd = case_when(abs(accuracy_numeric - mean(accuracy_numeric, na.rm = TRUE)) <=  3 * sd(accuracy_numeric, na.rm = TRUE)~ FALSE, TRUE ~ TRUE)
                ) %>%
  dplyr::filter(outlier_2_sd == TRUE)  %>%
  dplyr::ungroup()


# Display performance outliers
my_table_template(performance_outliers, caption = "Performance outliers (accuracy > 2/3 SD below/above group mean per condition (stimulus type x stimulation))")


# Calculate number of errors
number_of_errors <- single_trial_data %>% 
  dplyr::group_by(participant_id, group, session) %>% 
  dplyr::summarize(errors_total    = sum(response_type == "incorrect"), 
                   errors_analyzed = sum(response_type == "incorrect" & !is.nan(MFN_0_100_FCz) & rt_invalid == FALSE)
  ) %>%
  dplyr::ungroup() %>%
  dplyr::group_by(group) %>%
  # Calculate M and SD of the variables
  dplyr::summarize(across(-c(participant_id, session), list(mean,sd,min,max)))  %>%
  dplyr::ungroup()
  
                                                                                                 
# Display number of errors
my_table_template(number_of_errors, 
                  caption = "Number of errors", 
                  col_names = c("Group", "M", "SD", "min", "max", "M", "SD", "min", "max"),
                  header_above_config = c(" " = 1, "In total" = 4, "In analysis" = 4)
)
```
For some participants, accuracy in the task is more than 2 SD (N sessions = `r nrow(performance_outliers)`) or even 3 SD (N sessions = `r nrow(performance_outliers[performance_outliers$outlier_3_sd == TRUE,])`) below the group mean per condition (stimulus type x stimulation). But their accuracy is still quite good (> 70% in incongruent category), so these will not be excluded. Note that these information refer to the accuracy in the raw data, not the cleaned data. Being an outlier in task performance is also no exclusion criterion specified in the preregistration. No participant committed < 6 errors. Thus, no participant will be excluded based on this criterion.
<br><br>

```{r feedback}

# Create contingency table
feedback_contingency <- table(feedback_infos$feedback, feedback_infos$group)


# Display contingency table
my_table_template(feedback_contingency, caption = "Feedback in the task", row_names = TRUE)


# Calculate Chi-squared test
chisq.test(feedback_infos$feedback, feedback_infos$group)
```
The groups differ in the feedback they received. The OCD group received more feedback emphasizing speed. This is ok and could be expected. For now, we will not report this in the manuscript.
<br><br>


## Data Inspection {.tabset}
***

### Distribution

```{r inspect-distribution, fig.width = 8, fig.height = 16}

# Plot distribution RT
hist_rt <- ggplot(single_trial_data_clean, aes(x = rt)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "tan1", size = 1) +
  stat_function(fun = dnorm, args=list(mean = mean(single_trial_data_clean$rt, na.rm = TRUE), 
                                     sd = sd(single_trial_data_clean$rt, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(rt, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram RT", x = "RT", y = "Density") + 
  my_figure_theme

qqplot_rt <- ggplot(single_trial_data_clean, aes(sample = rt)) +
  stat_qq(color = "tan1") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot RT", x = "Theoretical Quantiles", y = "Sample Quantiles") + 
  my_figure_theme


# Plot distribution log RT
hist_rt_log <- ggplot(single_trial_data_clean, aes(x = rt_log)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "tan1", size = 1) +
  stat_function(fun = dnorm, args=list(mean = mean(single_trial_data_clean$rt_log, na.rm = TRUE), 
                                     sd = sd(single_trial_data_clean$rt_log, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(rt_log, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram log(RT)", x = "log(RT)", y = "Density") + 
  my_figure_theme

qqplot_rt_log <- ggplot(single_trial_data_clean, aes(sample = rt_log)) +
  stat_qq(color = "tan1") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot log(RT)", x = "Theoretical Quantiles", y = "Sample Quantiles") + 
  my_figure_theme


# Plot distribution PES
hist_pes <- ggplot(single_trial_data_clean[!is.na(single_trial_data_clean$pes),], aes(x = pes)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "tan1", size = 1) +
  stat_function(fun = dnorm, args=list(mean = mean(single_trial_data_clean$pes, na.rm = TRUE), 
                                     sd = sd(single_trial_data_clean$pes, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(pes, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram PES", x = "PES", y = "Density") + 
  my_figure_theme

qqplot_pes <- ggplot(single_trial_data_clean[!is.na(single_trial_data_clean$pes),], aes(sample = pes)) +
  stat_qq(color = "tan1") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot PES", x = "Theoretical Quantiles", y = "Sample Quantiles") + 
  my_figure_theme


# Plot distribution accuracy (we first need the values aggregated per session and participant)
accuracy_histogram <- single_trial_data_clean %>%
  dplyr::group_by(participant_id, session) %>%
  dplyr::summarize(mean_accuracy = sum(accuracy_numeric) / length(participant_id) * 100)  %>%
  dplyr::ungroup()

hist_accuracy <- ggplot(accuracy_histogram, aes(x = mean_accuracy)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "tan1", size = 1) +
  stat_function(fun = dnorm, args = list(mean = mean(accuracy_histogram$mean_accuracy, na.rm = TRUE), 
                                     sd = sd(accuracy_histogram$mean_accuracy, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(mean_accuracy, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram Mean Accuracy", x = "Mean Accuracy", y = "Density") + 
  my_figure_theme

qqplot_accuracy <- ggplot(accuracy_histogram, aes(sample = mean_accuracy)) +
  stat_qq(color = "tan1") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot Accuracy", x = "Theoretical Quantiles", y = "Sample Quantiles") + 
  my_figure_theme


# Plot distribution PEA 
hist_pea <- ggplot(df_aggregated_per_subject_pes_pea_pca, aes(x = pea)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "tan1", size = 1) +
  stat_function(fun = dnorm, args = list(mean = mean(df_aggregated_per_subject_pes_pea_pca$pea, na.rm = TRUE), 
                                     sd = sd(df_aggregated_per_subject_pes_pea_pca$pea, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(pea, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram Mean Post-Error Accuracy", x = "Post-Error Accuracy", y = "Density") + 
  my_figure_theme

qqplot_pea <- ggplot(df_aggregated_per_subject_pes_pea_pca, aes(sample = pea)) +
  stat_qq(color = "tan1") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot Mean Post-Error Accuracy", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  my_figure_theme


# Plot distribution PCA 
hist_pca <- ggplot(df_aggregated_per_subject_pes_pea_pca, aes(x = pca)) +
  geom_histogram(aes(y = ..density..), color="gray33", fill = "tan1", size = 1) +
  stat_function(fun = dnorm, args = list(mean = mean(df_aggregated_per_subject_pes_pea_pca$pca, na.rm = TRUE), 
                                     sd = sd(df_aggregated_per_subject_pes_pea_pca$pca, na.rm = TRUE)), color = "black", size = 0.5) +
  geom_vline(aes(xintercept = mean(pca, na.rm = TRUE)), color = "black", linetype = "dashed", size = 1) +
  labs (title = "Histogram Mean Post-Correct Accuracy", x = "Post-Correct Accuracy", y = "Density") + 
  my_figure_theme

qqplot_pca <- ggplot(df_aggregated_per_subject_pes_pea_pca, aes(sample = pca)) +
  stat_qq(color = "tan1") +
  stat_qq_line() +
  labs (title = "Q-Q-Plot Mean Post-Correct Accuracy", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  my_figure_theme


ggdraw() +
  draw_plot(hist_rt,        x =  0,   y = .80, width = .5,  height = .16) +
  draw_plot(qqplot_rt,      x =  .5,  y = .80, width = .5,  height = .16) +
  draw_plot(hist_rt_log,    x =  0,   y = .64, width = .5,  height = .16) +
  draw_plot(qqplot_rt_log,  x =  .5,  y = .64, width = .5,  height = .16) +
  draw_plot(hist_pes,       x =  0,   y = .48, width = .5,  height = .16) +
  draw_plot(qqplot_pes,     x =  .5,  y = .48, width = .5,  height = .16) +
  draw_plot(hist_accuracy,  x =  0,   y = .32, width = .5,  height = .16) +
  draw_plot(qqplot_accuracy,x =  .5,  y = .32, width = .5,  height = .16) +
  draw_plot(hist_pea,       x =  0,   y = .16, width = .5,  height = .16) +
  draw_plot(qqplot_pea,     x =  .5,  y = .16, width = .5,  height = .16) +
  draw_plot(hist_pca,       x =  0,   y = 0,   width = .5,  height = .16) +
  draw_plot(qqplot_pca,     x =  .5,  y = 0,   width = .5,  height = .16) 
```
<br><br>

### RT per participant 

```{r plot-RT-per-participant-and-response-type, fig.width = 12, fig.height = 20, cache = knitr_cache_enabled}

rt_per_participant <- ggplot(single_trial_data_clean, aes(x = response_type, y = rt)) + 
  geom_point(position = "jitter", aes(color = stimulus_type)) + 
  ggtitle("RT per participant") + 
  my_figure_theme + 
  facet_wrap(~ participant_id + session, ncol = 10) +
  scale_color_manual(values = my_figure_colors) 
rt_per_participant
```
<br><br>

### Check RT Normality 

For the single-trial data, Shapiro-Wilk is not suitable, as it always returns a significant result for such large samples (additionally, it can handle only samples up to 5000). Hence, we have to rely on visual inspection (see tab "Distribution") and values of skewness and kurtosis (see below). Values for skewness and kurtosis between -2 and +2 are considered acceptable in order to prove normal univariate distribution (George & Mallery, 2010).

```{r normality-single-trial-RTs}

normality_rt <- round(data.frame(matrix(c(skewness(single_trial_data_clean$rt),
                                          kurtosis(single_trial_data_clean$rt),
                                          skewness(single_trial_data_clean$rt_log),
                                          kurtosis(single_trial_data_clean$rt_log),
                                          skewness(single_trial_data_clean[!is.na(single_trial_data_clean$pes),]$pes),
                                          kurtosis(single_trial_data_clean[!is.na(single_trial_data_clean$pes),]$pes)),
                                        nrow=2,ncol=3)),digits = 1)
rownames(normality_rt) <- c("Skewness","Kurtosis")
colnames(normality_rt) <- c("RT","log(RT)", "PES")

my_table_template(normality_rt, row_names = TRUE)
```
<br><br>

### Determine RT transformation

LMM analysis of RT will be conducted on log-transformed RT values to meet the assumption of normally distributed residuals. The appropriate transformation was determined (or rather "validated", see notes below) using the Box–Cox procedure (Box & Cox, 1964). 

```{r RT-determine-transformation, fig.width = 8, fig.height = 3}

# Arrange plots
par(mfrow = c(1, 2)) 

# Determine transformation of RT by estimating optimal lambda using Box–Cox procedure
bc_rt <- boxcox(rt ~ 1, data = single_trial_data_clean)
optlambda_rt <- bc_rt$x[which.max(bc_rt$y)]

# Determine transformation of PES by estimating optimal lambda using Box–Cox procedure
bc_pes <- boxcox(pes+1000 ~ 1, data = single_trial_data_clean[!is.na(single_trial_data_clean$pes),])
optlambda_pes <- bc_pes$x[which.max(bc_pes$y)]

# Reset plot layout
par(mfrow = c(1, 1)) 
```
For RT (left plot), the optimal lambda is `r round(optlambda_rt, digits = 2)`, suggesting that log transformation (for lambda = 0) is appropriate. Actually, for lambda = -0.5, the most appropriate transformation would be Y^-0.5 = 1/(√(Y)), but this transformation does not seem very common to me. As our lambda is not far from 0, I chose log transformation, which is more commonly used. 
For PES (right plot), the optimal lambda is `r round(optlambda_pes, digits = 2)`, suggesting that no transformation (for lambda = 1) is needed.
<br><br>

## Descriptive Statistics {.tabset}
***

### Means and CIs

```{r descriptive-statistics-table}

##### WITH SESSION AND STIMULUS TYPE - FOR PLOTS 

##### RT
descriptive_statistics_rt <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "rt",
  withinvars    = c("response_type", "stimulus_type", "stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) 


##### Accuracy
descriptive_statistics_accuracy <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "accuracy_numeric",
  withinvars    = c("stimulus_type", "stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100))


##### PES
descriptive_statistics_pes <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
  measurevar    = "pes",
  withinvars    = c("stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) 


##### PEA
descriptive_statistics_pea <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pea),],
  measurevar    = "pea",
  withinvars    = c("stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100))


##### PCA
descriptive_statistics_pca <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pca),],
  measurevar    = "pca",
  withinvars    = c("stimulation", "session"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100))




##### WITHOUT SESSION AND STIMULUS TYPE - FOR TABLES 

##### RT 
descriptive_statistics_rt_no_session_no_stim_type <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "rt",
  withinvars    = c("response_type", "stimulation"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Format confidence interval column
  dplyr::mutate(
    ci_rt = paste0("[", round(rt - ci, digits = 0), 
                  ", ", round(rt + ci, digits = 0), "]")) %>%
  # Round RT means to zero decimals
  dplyr::mutate_at("rt", round, digits = 0) %>%    
  # Select columns to be displayed
  dplyr::select(c("group", "response_type", "stimulation",  "rt", "ci_rt", "ci"))


# Split and re-merge RT table to display both groups next to each other
descriptive_statistics_rt_display <-  split(descriptive_statistics_rt_no_session_no_stim_type, descriptive_statistics_rt_no_session_no_stim_type$group)
descriptive_statistics_rt_display <-  left_join(descriptive_statistics_rt_display$HC, descriptive_statistics_rt_display$OCD, by = c("response_type", "stimulation"))


# Display descriptive statistics for RT (and select columns)
my_table_template(descriptive_statistics_rt_display[,c(2:5,8:9)],
  caption = "Behavioral Performance: RT (in ms)",
  col_names = c("Response type", "Stimulation", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 2, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)


##### Accuracy 
descriptive_statistics_accuracy_no_session_no_stim_type <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "accuracy_numeric",
  withinvars    = c("stimulation"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100)) %>%
  # Format confidence interval column
  dplyr::mutate(
    ci_accuracy = paste0("[", round(accuracy_numeric - ci, digits = 2), 
                        ", ", round(accuracy_numeric + ci, digits = 2), "]")) %>%
  # Round accuracy means to two decimals
  dplyr::mutate_at("accuracy_numeric", round, digits = 2) %>% 
  # Select columns to be displayed
  dplyr::select(c("group", "stimulation", "accuracy_numeric", "ci_accuracy", "ci"))


# Split and re-merge accuracy table to display both groups next to each other
descriptive_statistics_accuracy_display <-  split(descriptive_statistics_accuracy_no_session_no_stim_type, descriptive_statistics_accuracy_no_session_no_stim_type$group)
descriptive_statistics_accuracy_display <-  left_join(descriptive_statistics_accuracy_display$HC, descriptive_statistics_accuracy_display$OCD, by = c("stimulation"))


# Display descriptive statistics for Accuracy (and select columns)
my_table_template(descriptive_statistics_accuracy_display[,c(2:4,7:8)],
  caption = "Behavioral Performance: Accuracy (in %)",
  col_names = c("Stimulation", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 1, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)




##### WITHOUT SESSION - FOR PLOTS AND TABLES 

##### RT
descriptive_statistics_rt_no_session <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "rt",
  withinvars    = c("response_type", "stimulus_type", "stimulation"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
)


##### Accuracy
descriptive_statistics_accuracy_no_session <- summarySEwithinO(
  data          = single_trial_data_clean,
  measurevar    = "accuracy_numeric",
  withinvars    = c("stimulus_type", "stimulation"),
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100))


##### PES
descriptive_statistics_pes_no_session <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
  measurevar    = "pes",
  withinvars    = "stimulation",
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Format confidence interval column
  dplyr::mutate(
    ci_pes = paste0("[", round(pes - ci, digits = 0), 
                   ", ", round(pes + ci, digits = 0), "]")) %>%
  # Round PES means to zero decimals
  dplyr::mutate_at("pes", round, digits = 0) 


# Split and re-merge PES table to display both groups next to each other
descriptive_statistics_pes_display <-  split(descriptive_statistics_pes_no_session, descriptive_statistics_pes_no_session$group)
descriptive_statistics_pes_display <-  left_join(descriptive_statistics_pes_display$HC, descriptive_statistics_pes_display$OCD, by = c("stimulation"))


# Display descriptive statistics for PES (and select columns)
my_table_template(descriptive_statistics_pes_display[,c(2,4,9,12,17)],
  caption = "Behavioral Performance: Post-Error Slowing (in %)",
  col_names = c("Stimulation", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 1, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)

##### PEA
descriptive_statistics_pea_no_session <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pea),],
  measurevar    = "pea",
  withinvars    = "stimulation",
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100)) %>%
  # Format confidence interval column
  dplyr::mutate(
    ci_pea = paste0("[", round(pea - ci, digits = 2), 
                   ", ", round(pea + ci, digits = 2), "]")) %>%
  # Round PEA means to two decimals
  dplyr::mutate_at("pea", round, digits = 2) 

# Split and re-merge PEA table to display both groups next to each other
descriptive_statistics_pea_display <-  split(descriptive_statistics_pea_no_session, descriptive_statistics_pea_no_session$group)
descriptive_statistics_pea_display <-  left_join(descriptive_statistics_pea_display$HC, descriptive_statistics_pea_display$OCD, by = c("stimulation"))


# Display descriptive statistics for PEA (and select columns)
my_table_template(descriptive_statistics_pea_display[,c(2,4,9,12,17)],
  caption = "Behavioral Performance: Post-Error Accuracy (in %)",
  col_names = c("Stimulation", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 1, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)


##### PCA
descriptive_statistics_pca_no_session <- summarySEwithinO(
  data          = single_trial_data_clean[!is.na(single_trial_data_clean$pca),],
  measurevar    = "pca",
  withinvars    = "stimulation",
  betweenvars   = "group",
  idvar         = "participant_id", 
  conf.interval = .95
) %>%
  # Multiply numeric values by 100 to obtain values in percent
  dplyr::mutate_if(is.numeric, list(~ . * 100)) %>%
  # Format confidence interval column
  dplyr::mutate(
    ci_pca = paste0("[", round(pca - ci, digits = 2), 
                   ", ", round(pca + ci, digits = 2), "]")) %>%
  # Round PCA means to two decimals
  dplyr::mutate_at("pca", round, digits = 2) 


# Split and re-merge PCA table to display both groups next to each other
descriptive_statistics_pca_display <-  split(descriptive_statistics_pca_no_session, descriptive_statistics_pca_no_session$group)
descriptive_statistics_pca_display <-  left_join(descriptive_statistics_pca_display$HC, descriptive_statistics_pca_display$OCD, by = c("stimulation"))


# Display descriptive statistics for PCA (and select columns)
my_table_template(descriptive_statistics_pca_display[,c(2,4,9,12,17)],
  caption = "Behavioral Performance: Post-Correct Accuracy (in %)",
  col_names = c("Stimulation", "M", "95% CI", "M", "95% CI"),
  header_above_config = c(" " = 1, "HC" = 2, "OCD" = 2),
  footnote_config = c(general = "Confidence intervals are adjusted for within-participant designs as described by Morey (2008).")
)
```
<br><br>

### Plot without session {.active}

```{r descriptive-statistics-plot-rt-acc, fig.width = 8, fig.height = 7, fig.cap = "Note. (A) RT, (B) accuracy, (c) post-error slowing and (D) post-error accuracy, and (E) post-correct accuracy in the flanker task are shown as a function of stimulus type, response type, stimulation condition, and group. Means and 95% confidence intervals (shown in orange/red) were calculated based on single-trial data. Boxplots are based on data aggregated by participant. CIs are adjusted for within-participant designs as described by Morey (2008)."}

# Create plot RT 
plot_rt <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_rt, aes(x = stimulation, y = rt, fill = group), outlier.shape = NA)+
  geom_point(data = descriptive_statistics_rt_no_session_no_stim_type, aes(x = stimulation, y = rt, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_rt_no_session_no_stim_type, aes(x = stimulation, ymax = rt + ci, ymin = rt - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_rt_no_session_no_stim_type, aes(x = stimulation, y = rt, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("sienna3", "slategray3"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~response_type, nrow = 1) +
  my_figure_theme + 
  coord_cartesian(ylim = c(200, 550)) +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "\nStimulation condition", y = "RT (ms)")


# Create plot accuracy 
plot_accuracy <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_accuracy, aes(x = stimulation, y = accuracy_numeric, fill = group), outlier.shape = NA)+
  geom_point(data = descriptive_statistics_accuracy_no_session_no_stim_type, aes(x = stimulation, y = accuracy_numeric, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_accuracy_no_session_no_stim_type, aes(x = stimulation, ymax = accuracy_numeric + ci, ymin = accuracy_numeric - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_accuracy_no_session_no_stim_type, aes(x = stimulation, y = accuracy_numeric, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("sienna3", "slategray3"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
#  facet_wrap(~stimulus_type, nrow = 1) +
  coord_cartesian(ylim = c(70, 100)) +
  scale_y_continuous(expand = c(0, 0)) +
  my_figure_theme + 
  labs(x = "\n", y = "Accuracy (%)")


# Create plot PES
plot_pes <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_pes_pea_pca, aes(x = stimulation, y = pes, fill = group), outlier.shape = NA)+
  geom_point(data = descriptive_statistics_pes_no_session, aes(x = stimulation, y = pes, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_pes_no_session, aes(x = stimulation, ymax = pes + ci, ymin = pes - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_pes_no_session, aes(x = stimulation, y = pes, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("sienna3", "slategray3"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  coord_cartesian(ylim = c(-50, 100)) +
  scale_y_continuous(expand = c(0, 0)) +
  my_figure_theme + 
  labs(x = "\n", y = "Post-error slowing (ms)")


# Create plot PEA
plot_pea <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_pes_pea_pca, aes(x = stimulation, y = pea, fill = group), outlier.shape = NA)+
  geom_point(data = descriptive_statistics_pea_no_session, aes(x = stimulation, y = pea, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_pea_no_session, aes(x = stimulation, ymax = pea + ci, ymin = pea - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_pea_no_session, aes(x = stimulation, y = pea, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("sienna3", "slategray3"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "Post-error accuracy (ms)")


# Create plot PCA
plot_pca <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_pes_pea_pca, aes(x = stimulation, y = pca, fill = group), outlier.shape = NA)+
  geom_point(data = descriptive_statistics_pca_no_session, aes(x = stimulation, y = pca, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_pca_no_session, aes(x = stimulation, ymax = pca + ci, ymin = pca - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_pca_no_session, aes(x = stimulation, y = pca, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("sienna3", "slategray3"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "Post-correct accuracy (ms)")



# Create common legend for plots (function from http://www.sthda.com/english/wiki/wiki.php?id_contents=7930#add-a-common-legend-for-multiple-ggplot2-graphs)
get_legend <- function(myggplot) {
  tmp      <- ggplot_gtable(ggplot_build(myggplot))
  leg      <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend   <- tmp$grobs[[leg]]
  return(legend)
}
legend    <- get_legend(plot_rt)


# Remove previous legends from plots
plot_rt       <- plot_rt       + theme(legend.position = "none")
plot_accuracy <- plot_accuracy + theme(legend.position = "none")
plot_pes      <- plot_pes      + theme(legend.position = "none")
plot_pea      <- plot_pea      + theme(legend.position = "none")
plot_pca      <- plot_pca      + theme(legend.position = "none")


# Arrange plots
figure_behav <- ggdraw() +
  draw_plot(plot_rt,       x =  0,   y = .5,  width = .65, height = .5) +
  draw_plot(plot_accuracy, x = .65,  y = .5,  width = .35, height = .5) +
  draw_plot(plot_pes,      x =  0,   y = .15, width = .33, height = .4) +
  draw_plot(plot_pea,      x = .33,  y = .15, width = .33, height = .4) +
  draw_plot(plot_pca,      x = .66,  y = .15, width = .33, height = .4) +
  draw_plot(legend,        x = .27,  y = .05, width = .5,  height = .1)   +
  draw_plot_label(c("A", "B", "C", "D", "E"), c(0, .65, 0, .33, .66), c(1, 1, .585, .585, .585), size = 15)


# Save plot
ggsave("./figures/figure_behav.tiff", width = 16, height = 15, units = "cm", dpi=600, compression = "lzw")


# Display plot
figure_behav
``` 
To quantify PES robust, we used the method proposed by Dutilh et al. (2012). See section LMM PES for details. We further quantified accuracy following incorrect and correct responses (post-error accuracy and post-correct accuracy).
<br><br>

### Plot with session

```{r descriptive-statistics-plot-including-session, fig.width = 10, fig.height = 15, fig.cap = "Note. (A) RT, (B) accuracy, (c) post-error slowing and (D) post-error accuracy, and (E) post-correct accuracy in the flanker task are shown as a function of response type, stimulation condition, group, and session. Means and 95% confidence intervals (shown in orange/red) were calculated based on single-trial data. Boxplots are based on data aggregated by participant. CIs are adjusted for within-participant designs as described by Morey (2008)."}

# Create plot RT 
plot_rt_session <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_rt, aes(x = stimulation, y = rt, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_rt, aes(x = stimulation, y = rt, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_rt, aes(x = stimulation, ymax = rt + ci, ymin = rt - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_rt, aes(x = stimulation, y = rt, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("sienna3", "slategray3"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~response_type + stimulus_type + session, nrow = 1) +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "RT (ms)")


# Create plot accuracy 
plot_accuracy_session <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_accuracy, aes(x = stimulation, y = accuracy_numeric, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_accuracy, aes(x = stimulation, y = accuracy_numeric, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_accuracy, aes(x = stimulation, ymax = accuracy_numeric + ci, ymin = accuracy_numeric - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_accuracy, aes(x = stimulation, y = accuracy_numeric, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("sienna3", "slategray3"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~stimulus_type + session, nrow = 1) +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "Accuracy (%)")


# Create plot PES
plot_pes_session <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_pes_pea_pca, aes(x = stimulation, y = pes, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_pes, aes(x = stimulation, y = pes, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_pes, aes(x = stimulation, ymax = pes + ci, ymin = pes - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_pes, aes(x = stimulation, y = pes, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("sienna3", "slategray3"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~session, nrow = 1) +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "Post-error slowing (ms)")


# Create plot PEA
plot_pea_session <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_pes_pea_pca, aes(x = stimulation, y = pea, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_pea, aes(x = stimulation, y = pea, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_pea, aes(x = stimulation, ymax = pea + ci, ymin = pea - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_pea, aes(x = stimulation, y = pea, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("sienna3", "slategray3"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~session, nrow = 1) +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "Post-error accuracy (ms)")


# Create plot PCA
plot_pca_session <- ggplot() +
  geom_boxplot(data = df_aggregated_per_subject_pes_pea_pca, aes(x = stimulation, y = pca, fill = group), outlier.size =0.3)+
  geom_point(data = descriptive_statistics_pca, aes(x = stimulation, y = pca, colour = group), 
             position = position_dodge(width = 0.7), shape = 15, size = 1) +
  geom_errorbar(data = descriptive_statistics_pca, aes(x = stimulation, ymax = pca + ci, ymin = pca - ci, colour = group),
                position = position_dodge(width = 0.7), width = 0, size = 0.5) +
  geom_line(data = descriptive_statistics_pca, aes(x = stimulation, y = pca, group = group, color = group), 
            position = position_dodge(width = 0.7), linetype = 3, size = 0.5) +
  scale_colour_manual(values = c("sienna3", "slategray3"), name = "Group:") +
  scale_fill_manual(values = my_figure_colors, name = "Group:") +
  facet_wrap(~session, nrow = 1) +
  my_figure_theme + 
  labs(x = "\nStimulation condition", y = "Post-correct accuracy (ms)")


# Create common legend for plots (function from http://www.sthda.com/english/wiki/wiki.php?id_contents=7930#add-a-common-legend-for-multiple-ggplot2-graphs)
get_legend <- function(myggplot) {
  tmp      <- ggplot_gtable(ggplot_build(myggplot))
  leg      <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend   <- tmp$grobs[[leg]]
  return(legend)
}
legend    <- get_legend(plot_rt_session)


# Remove previous legends from plots
plot_rt_session       <- plot_rt_session       + theme(legend.position = "none")
plot_accuracy_session <- plot_accuracy_session + theme(legend.position = "none")
plot_pes_session      <- plot_pes_session      + theme(legend.position = "none")
plot_pea_session      <- plot_pea_session      + theme(legend.position = "none")
plot_pca_session      <- plot_pca_session      + theme(legend.position = "none")


# Arrange plots
figure_behav_session <- ggdraw() +
  draw_plot(plot_rt_session,       x =  0,   y = .66,  width = 1,   height = .33) +
  draw_plot(plot_accuracy_session, x =  0,   y = .33,  width = .5,  height = .33) +
  draw_plot(legend,                x = .6,   y = .36,  width = .5,  height = .33) +
  draw_plot(plot_pes_session,      x =  0,   y = .0,   width = .33, height = .33) +
  draw_plot(plot_pea_session,      x = .33,  y = .0,   width = .33, height = .33) +
  draw_plot(plot_pca_session,      x = .66,  y = .0,   width = .33, height = .33) +
  draw_plot_label(c("A", "B", "C","D","E"), c(0, 0, 0, 0.33, 0.66), c(1, .66, .33, .33, .33), size = 15)


# Save plot
 ggsave("./figures/figure_behav_session.tiff", width = 20, height = 25, units = "cm", dpi=600, compression = "lzw")


# Display plot
figure_behav_session
```  
To quantify PES robust, we used the method proposed by Dutilh et al. (2012). See section LMM PES for details. We further quantified accuracy following incorrect and correct responses (post-error accuracy and post-correct accuracy).
<br><br>

## (G)LMM Analyses
***

RT, PES, and accuracy were modeled using two linear mixed-effects models (LMM) and a binomial generalized linear mixed-effects model (GLMM), respectively. <br><br>

**Fixed effects**

*Stimulus type (congruent, incongruent), group (HC, OCD), stimulation (verum, sham), and, where applicable (= for RT), response type (correct, incorrect)* were specified as fixed factors. Fixed effects were coded using effect coding (this equals sliding difference contrasts for two levels for factors with two levels or sum coding/2), such that the intercept reflects the grand mean across all conditions and differences in means between factor levels are tested. Fixed effects were not eliminated using model comparison techniques because they correspond to the original experimental design and a priori hypotheses. <br><br>

I specified models that predict all main effects and interactions only with the factors stimulation and group. This is the most sparse model structure for testing the effects of interest.<br><br>

**Random effects**

Participants were specified as random factors. The random-effects structure for each model was determined based on the procedure proposed by Bates, Kliegl, et al. (2015). We started with the maximal random-effects structure, including random intercepts for participants, as well as random slopes for all main effects and interactions specified as fixed effects that were justified by the design. If the model with the maximal random-effects structure would not converge, correlations of the random terms were set to zero. We performed a principal components analysis on the random-effects variance–covariance estimates to determine the number of components supported by the data and removed random effects explaining zero variance to prevent overparametrization (Matuschek et al., 2017).

```{r RT-accuracy-(G)LMM-contrast-coding}

# Define contrasts (sliding difference contrasts)
contrasts(single_trial_data_clean$stimulation)         <- contr.sdif(2)
contrasts(single_trial_data_clean$group)               <- contr.sdif(2)
contrasts(single_trial_data_clean$response_type)       <- contr.sdif(2)
contrasts(single_trial_data_clean$stimulus_type)       <- contr.sdif(2)
contrasts(single_trial_data_clean$accuracy_prev_trial) <- contr.sdif(2)
contrasts(single_trial_data_clean$session)             <- contr.sdif(2)


# Add contrasts as numerical covariates via model matrix* (specify all possible contasts for now)
model_matrix <- model.matrix(~ stimulation * group * (response_type + stimulus_type), single_trial_data_clean)


# Attach the model matrix (12 columns) to the dataframe
single_trial_data_clean[, (ncol(single_trial_data_clean) + 1):(ncol(single_trial_data_clean) + 12)] <- model_matrix


# Assign descriptive names to the contrasts
names(single_trial_data_clean)[(ncol(single_trial_data_clean) - 11):ncol(single_trial_data_clean)] <- c("Grand Mean", "verum_sham", "OCD_HC", "incorrect_correct", "incongruent_congruent", "verum_sham:OCD_HC", "verum_sham:incorrect_correct", "verum_sham:incongruent_congruent", "OCD_HC:incorrect_correct" , "OCD_HC:incongruent_congruent", "verum_sham:OCD_HC:incorrect_correct", "verum_sham:OCD_HC:incongruent_congruent")


# *Note: For the random effects, we needed to enter the separate random effect terms in the models to enable
# double-bar notation (||). This allows fitting a model that sets correlations of the random terms to zero.
```
<br><br>

### RT {.tabset}

#### LMM

This model will be reported in the manuscript.

```{r LMM-RT, cache = knitr_cache_enabled}

# Run model with maximal random-effects structure
LMM_rt <- lmer(rt_log ~ verum_sham * OCD_HC * incorrect_correct +
  (1 + verum_sham * incorrect_correct | participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_rt) # Model does converge 
# isSingular(LMM_rt) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_rt)) # All terms explain variance 


# Display results (fixed effects)
tab_model(LMM_rt,
  dv.labels = "log(RT)", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)

# Display random effects
print("Random effects:")
print(VarCorr(LMM_rt), digits = 1, comp = "Std.Dev.")
```
<br><br>
Incorrect responses are faster than correct responses. There is no evidence for an effect of group or stimulation. Trend for the following interaction is present stimulation:group:response_type. 
<br><br>

#### LMM nested 

```{r LMM-RT-per-group, cache = knitr_cache_enabled}

# Run model with maximal random-effects structure
LMM_rt_group <- lmer(rt_log ~ group/stimulation * incorrect_correct  +
  (1 + verum_sham * incorrect_correct  | participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_rt_group) # Model does converge 
# isSingular(LMM_rt_group) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_rt_group)) # All terms explain variance 


# Display results (fixed effects)
tab_model(LMM_rt_group,
  dv.labels = "log(RT)", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>
For the separate groups, there is no significant effect of stimulation on RT. Trends for the following interactions are present: group * stimulus_congruence and HC:stimulation:stimulus_congruence.
<br><br>

#### LMM with stimulus type

Even though congruent errors were very rare (`r round(nrow(single_trial_data_clean[single_trial_data_clean$response_type == "incorrect" & single_trial_data_clean$stimulus_type == "congruent",])/nrow(single_trial_data_clean)*100, digits = 2)`% of all trials; N = `r nrow(single_trial_data_clean[single_trial_data_clean$response_type == "incorrect" & single_trial_data_clean$stimulus_type == "congruent",])` trials), I did not exclude them from LMM RT analysis. In order to not loose power, I do not want to exclude congruent errors from analyses in general (e.g. EEG, accuracy), which is ok as I will not add stimulus type as factor there. For the following reasons it should also be ok to not exclude congruent errors from LMM RT analysis:

* LMMs take into account this imbalance due to different number of observations per cell and thus different uncertainty. So, it should be not problematic that congruent errors are rare.

* Additionally, it should be ok that one cell only has few observations, as the interaction response type * stimulus type is not included in the model but only main effects. Not including this interaction in the model is ok, as it is of no interest to our research questions. 

* I thought about whether the effect of stimulus type could confound effect of response type, as errors occur mostly in incongruent trials whereas correct responses are occur in both congruent and incongruent trials. But actually, when both factors are included, the respective main effects reflect the effect of each factor when controlling for the other. 

For the aggregation-based ANOVA on RT, I will excluded congruent errors.

```{r LMM-RT-stim-type, cache = knitr_cache_enabled}

# Run model with maximal random-effects structure
LMM_rt_stim_type <- lmer(rt_log ~ verum_sham * OCD_HC * (incorrect_correct + incongruent_congruent) +
  (1 + verum_sham * (incorrect_correct + incongruent_congruent) | participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_rt_stim_type) # Model does converge 
# isSingular(LMM_rt_stim_type) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_rt_stim_type)) # All terms explain variance 


# Display results (fixed effects)
tab_model(LMM_rt_stim_type,
  dv.labels = "log(RT)", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>
Responses are faster in the congruent than in the incongruent condition. Incorrect responses are faster than correct responses. There is no evidence for an effect of group or stimulation. Trends for the following interactions are present: group:stimulus_congruence and stimulation:group:response_type. 
<br><br>

#### LMM nested with stimulus type

```{r LMM-RT-per-group-stim-type, cache = knitr_cache_enabled}

# Run model with maximal random-effects structure
LMM_rt_group_stim_type <- lmer(rt_log ~ group/stimulation * (incorrect_correct + incongruent_congruent) +
  (1 + verum_sham * (incorrect_correct + incongruent_congruent) | participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_rt_group_stim_type) # Model does converge 
# isSingular(LMM_rt_group_stim_type) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_rt_group_stim_type)) # All terms explain variance 


# Display results (fixed effects)
tab_model(LMM_rt_group_stim_type,
  dv.labels = "log(RT)", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>
For the separate groups, there is no significant effect of stimulation on RT. Trends for the following interactions are present: group * stimulus_congruence and HC:stimulation:stimulus_congruence.
<br><br>

#### Assumption checks

```{r LMM-RT-assumptions, fig.width = 20, fig.height = 15, cache = knitr_cache_enabled, eval = FALSE}

# Check model assumptions
performance::check_model(LMM_rt, panel = TRUE)

# In addition to plots, print verbal output for some assumption tests to facilitate conclusion
print("# Check for heteroscedasticity")
performance::check_heteroscedasticity(LMM_rt)

print("# Check for influential observations (Cook's distance)")
performance::check_outliers(LMM_rt, effects = "random")

print("# Check for normal distributed random effects")
performance::check_normality(LMM_rt, effects = "random")
```

* **Assumption 1: Independence of Data Points / Absence of collinearity -> Is OK**
    + Are predictors not highly correlated?
    + Multicollinearity plot shows only low correlations 

* **Assumption 2: Normality of Residuals -> Is OK???** 
    + Are residuals approximately normally distributed?
    + Q-Q plot and density plot look fine? Q-Q plot quite a bit off at the extremes 
    + It is debated whether this is problematic at all; and violation does not seem so bad, so maybe not worry about it? 

* **Assumption 3: Linearity -> Is OK** 
    + Is the dependent variable linearly related to the fixed factors, random factors, and covariates?
    + Plot of the residuals against the fitted values shows a random scatter pattern, no nonlinear or curvy pattern 

* **Assumption 4: Homogeneity of Residual Variance (Heteroscedasticity) -> Is OK???**
    + Have residuals constant variance across the range of the predicted values?
    + Plot of the residuals against the fitted values shows an even spread around the centered line; but written output says this is not ok

* **Assumption 5: Absence of Influential Data Points -> Is OK** 
    + Are there are no influential values? 
    + Cook's distance plot looks fine (for large N, Cook's distances should be below 1) and written output says there are no outliers 

* **Assumption 6: Normality of Random Effects -> Is OK**
    + Are random effects approximately normally distributed?
    + Plots look fine; written output says this is (mostly) ok
<br><br>

#### Check covariates

The purpose of including the covariates was to see how the effects change when controlling for the overall effect of the covariate. Thus, covariates were included only as fixed factor, not as random term. I first included the covariates as main effect only, not allowing any interactions with stimulation or group. However, inspecting the interactions as well might lead to new, important insights. These models including the interactions are presented below.
Note: The covariate number of errors refers to the actual number of errors committed by each participant, not the number included in the analyses. Continuous predictors were grand mean standardized (number of errors, number of feedback faster).

```{r LMM-RT-covariates, cache = knitr_cache_enabled}

# RT check covariate session
LMM_rt_session <- lmer(rt_log ~ verum_sham * OCD_HC * (incorrect_correct + session) +
  (1 + verum_sham * incorrect_correct | participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_rt_session,
  dv.labels = "log(RT), covariate session", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# RT check covariate number of errors (predictor was z standardized)
LMM_rt_number_errors <- lmer(rt_log ~ verum_sham * OCD_HC * (incorrect_correct + number_errors_standardized) +
  (1 + verum_sham * incorrect_correct | participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_rt_number_errors,
  dv.labels = "log(RT), covariate number of errors", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# RT check covariate number of feedback faster (predictor was z standardized)
LMM_rt_number_feedback_faster <- lmer(rt_log ~ verum_sham * OCD_HC * (incorrect_correct + number_feedback_faster_standardized) +
  (1 + verum_sham * incorrect_correct | participant_id),
data = single_trial_data_clean,
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_rt_number_feedback_faster,
  dv.labels = "log(RT), covariate number of feedback faster", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = TRUE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>

#### GLMM / GLMM nested

In the preregistration I wrote that for RT, a GLMM with gamma or inverse gaussian distribution with an identity link function on raw RT will be preferred over a LMM on transformed values, if both models allow a sufficiently complex random structure without convergence problems. So I tried a GLMM here. There are no convergence problems. <br><br>
Lo & Andrews (2015) recommend gamma or inverse gaussian distribution for RTs. For the RTs, gamma distribution fits very well and a bit better than inverse gaussian distribution. Hence, I would choose gamma distribution and the identity link funktion (recommended by Lo & Andrews, 2015). BUT: With the gamma distribution, all effects in the GLMM turn significant (since the standard error gets tiny), which seems not right to me. Using the inverse gaussian distribution (see results below), the results seem much more realisitic (effects are comparable to the LMM). <br><br>
We will report the LMM for RTs. The main reason is that we do not want to deal with the group difference in RT because we would have to include RT as covariate in all models then (the group difference also remains when stimulation type is not included in the model). Further, I am not really happy with the convergence of the models.

```{r GLMM-RT, cache = knitr_cache_enabled}

# Fit inverse gaussian distribution
# library("GeneralizedHyperbolic")
# inv_gauss <- nigFit(single_trial_data_clean$rt, plots = FALSE, printOut = FALSE) 
# plot(inv_gauss)


# Fit gamma distribution
# library("fitdistrplus")
# gamma <- fitdist(single_trial_data_clean$rt, distr = "gamma")
# plot(gamma)


# Run final model (without correlations between random terms to achieve convergence)
GLMM_rt <- glmer(rt ~ verum_sham * OCD_HC * (incorrect_correct + incongruent_congruent) +
  (1 + verum_sham * (incorrect_correct + incongruent_congruent) || participant_id),
  data = single_trial_data_clean,
  family = inverse.gaussian(link="identity"),
  control = glmerControl(optimizer="bobyqa"))


# Check model output
# summary(GLMM_rt) # Model does converge 
# isSingular(GLMM_rt) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(GLMM_rt)) # All terms explain variance 


tab_model(GLMM_rt,
  dv.labels = "RT", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "z value", 
  string.ci = "95 % CI", string.p = "p value", wrap.labels = 80, digits.re = 3
)



# Run final nested model (without correlations between random terms to achieve convergence)
GLMM_rt_group <- glmer(rt ~ group/stimulation * (incorrect_correct + incongruent_congruent) +
  (1 + verum_sham * (incorrect_correct + incongruent_congruent) || participant_id),
  data = single_trial_data_clean,
  family = inverse.gaussian(link="identity"),
  control = glmerControl(optimizer="bobyqa"))


# Check model output
# summary(GLMM_rt_group) # Model does converge 
# isSingular(GLMM_rt_group) # Check for singular model fit (i.e., dimensions of the variance-covariance matrix have been estimated as exactly zero): FALSE


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(GLMM_rt_group)) # All terms explain variance 


tab_model(GLMM_rt_group,
  dv.labels = "RT", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE,
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "z value",
  string.ci = "95 % CI", string.p = "p value", wrap.labels = 80, digits.re = 3
)
```
<br><br>
In contrast to the LMM, the GLMM shows a significant interaction stimulation * group * response type (this was a trend before). The rest is pretty much the same as in the LMM (except for an additional significant main effect of group). 
<br><br>

### PES {.tabset}

#### LMM

To quantify PES robust, we used the method proposed by Dutilh et al. (2012). "A single-trial value of PES was computed by performing a pairwise comparison of correct trials around each error (RT post-error − RT pre-error). This method ensures that post-error and post-correct trials originate from the same time periods in the data set and thus controls for global fluctuations in motivation and attention." To avoid the effects of consecutive errors on RTs, we considered only error trials that were preceded by at least two correct responses and followed by at least one correct response (i.e., sequences of CCEC trials, where ‘C’ represents correct trials and ‘E’ represents error trials). 

I decided to test the stimulation effect on PES in a separate model instead of including the factor correctness of previous trial in the RT model. Otherwise, this would have led to an overly complex LMM on RTs. Additionally, all trials would have been included then, preventing to use the robust (and preregistered) PES quantification by Dutilh et al. (2012).

```{r LMM-PES}

# Run final model 
LMM_pes <- lmer(pes ~ verum_sham * OCD_HC +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_pes) # Model does converge
# isSingular(LMM_pes) # is also ok


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_pes)) # All terms explain variance


# Display results (fixed effects)
tab_model(LMM_pes,
  dv.labels = "PES (ms)", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# Display random effects
print("Random effects:")
print(VarCorr(LMM_pes), digits = 4, comp = "Std.Dev.")
```
<br><br>
There is PES (intercept is significant). There is no evidence for an effect of group or stimulation. 
<br><br>

#### LMM nested

```{r LMM-PES-per-group}

# Run final model
LMM_pes_group <- lmer(pes ~ group/stimulation  +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(LMM_pes_group) # Model does converge
# isSingular(LMM_pes_group) # is also ok


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(LMM_pes_group)) # All terms explain variance


# Display results (fixed effects)
tab_model(LMM_pes_group,
  dv.labels = "PES (ms)", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>
For the separate groups, there is no significant effect of stimulation on PES.
<br><br>

#### Assumption checks

```{r LMM-PES-assumptions, fig.width = 20, fig.height = 15, eval = FALSE}

# Check model assumptions
performance::check_model(LMM_pes, panel = TRUE)

# In addition to plots, print verbal output for some assumption tests to facilitate conclusion
print("# Check for heteroscedasticity")
performance::check_heteroscedasticity(LMM_pes)

print("# Check for influential observations (Cook's distance)")
performance::check_outliers(LMM_pes, effects = "random")

print("# Check for normal distributed random effects")
performance::check_normality(LMM_pes, effects = "random")
```

* **Assumption 1: Independence of Data Points / Absence of collinearity -> Is OK**
    + Are predictors not highly correlated?
    + Multicollinearity plot shows only low correlations 

* **Assumption 2: Normality of Residuals -> Is OK???** 
    + Are residuals approximately normally distributed?
    + Q-Q plot and density plot look fine? Q-Q plot a bit off at the extremes
    + It is debated whether this is problematic at all; and violation does not seem so bad, so maybe not worry about it?  

* **Assumption 3: Linearity -> Is OK** 
    + Is the dependent variable linearly related to the fixed factors, random factors, and covariates?
    + Plot of the residuals against the fitted values shows a random scatter pattern, no nonlinear or curvy pattern 

* **Assumption 4: Homogeneity of Residual Variance (Heteroscedasticity)  -> Is OK???**
    + Have residuals constant variance across the range of the predicted values?
    + Plot of the residuals against the fitted values shows an even spread around the centered line; but written output says this is not ok

* **Assumption 5: Absence of Influential Data Points  -> Is OK** 
    + Are there are no influential values? 
    + Cook's distance plot looks fine (for large N, Cook's distances should be below 1) and written output says there are no outliers 

* **Assumption 6: Normality of Random Effects  -> Is OK???**
    + Are random effects approximately normally distributed?
    + Written output says this is ok (but: standard errors and hence also plot not available due to || syntax) 
<br><br>

#### Check covariates

For further infos on inclusion of covariates, see notes in section "(G)LMM Analyses -> RT", tab "Check covariates".

```{r LMM-PES-covariates, cache = knitr_cache_enabled}

# PES check covariate session
LMM_pes_session <- lmer(pes ~ verum_sham * OCD_HC * session  +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_pes_session,
  dv.labels = "PES (ms), covariate session", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# PES check covariate number of errors (predictor was z standardized)
LMM_pes_number_errors <- lmer(pes ~ verum_sham * OCD_HC * number_errors_standardized  +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_pes_number_errors,
  dv.labels = "PES (ms), covariate number of errors", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)


# PES check covariate number of feedback faster (predictor was z standardized)
LMM_pes_number_feedback_faster <- lmer(pes ~ verum_sham * OCD_HC * number_feedback_faster_standardized  +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean[!is.na(single_trial_data_clean$pes),],
REML = TRUE,
control = lmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(LMM_pes_number_feedback_faster,
  dv.labels = "PES (ms), covariate number of feedback faster", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "t value", 
  string.ci = "95 % CI", string.p = "p value",  p.val = "satterthwaite", wrap.labels = 80, digits.re = 3
)
```
<br><br>

### Accuracy {.tabset}

#### GLMM

This model will be reported in the manuscript. <br>

```{r GLMM-accuracy, cache = knitr_cache_enabled}

# Run model with maximal random-effects structure
GLMM_accuracy <- glmer(accuracy_numeric ~ stimulation * group +
  (1 + stimulation | participant_id),
data = single_trial_data_clean,
family = binomial,
control = glmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(GLMM_accuracy) # Model does converge
# isSingular(GLMM_accuracy) # is also ok


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(GLMM_accuracy)) # All terms explain variance


# Display results (fixed effects)
tab_model(GLMM_accuracy,
  dv.labels = "Accuracy", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "z value", 
  string.ci = "95 % CI", string.p = "p value", wrap.labels = 80, digits.re = 3
)


# Display random effects
print("Random effects:")
print(VarCorr(GLMM_accuracy), digits = 2, comp = "Std.Dev.")
```

```{r GLMM-accuracy-plot, cache = knitr_cache_enabled, fig.width = 4}
# Plot interaction stimulation * group
plot_acc_predicted <- plot_model(GLMM_accuracy,
                                type   = "pred",
                                terms  = c("group","stimulation"),
                                ci.lvl = .95) +
  labs(title = "Predicted probabilities of a correct response",
       x     = "Group",
       y     = "Accuracy") +
  my_figure_theme +
  scale_color_manual(name = "Stimulation condition", labels = c("sham","verum"), values = c("slategray3", "navy"))
plot_acc_predicted
```
<br><br>
There is no evidence for an effect of group or stimulation but a trend for the interaction stimulation * group. Reinhart and Woodman (2014) found that cathodal stimulation increased the error rate. We see this effect in the OCD group (for HC - if anything at all - the opposite direction), but in the nested model this effect is not significant in the OCD group.
<br><br>

#### GLMM nested 

```{r GLMM-accuracy-per-group, cache = knitr_cache_enabled}

# Run model with maximal random-effects structure
GLMM_accuracy_group <- glmer(accuracy_numeric ~ group/stimulation +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean,
family = binomial,
control = glmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(GLMM_accuracy_group) # Model does converge
# isSingular(GLMM_accuracy_group) # is also ok


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(GLMM_accuracy_group)) # All terms explain variance


# Display results (fixed effects)
tab_model(GLMM_accuracy_group,
  dv.labels = "Accuracy", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "z value", 
  string.ci = "95 % CI", string.p = "p value", wrap.labels = 80, digits.re = 3
)


# Run model with maximal random-effects structure
GLMM_accuracy_group2 <- glmer(accuracy_numeric ~ stimulation/group +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean,
family = binomial,
control = glmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(GLMM_accuracy_group2) # Model does converge
# isSingular(GLMM_accuracy_group2) # is also ok


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(GLMM_accuracy_group2)) # All terms explain variance


# Display results (fixed effects)
tab_model(GLMM_accuracy_group2,
  dv.labels = "Accuracy", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "z value", 
  string.ci = "95 % CI", string.p = "p value", wrap.labels = 80, digits.re = 3
)
```
<br><br>
For the separate groups, there is no significant effect of stimulation on accuracy. There is a significant interaction stimulation:accuracy_prev_trial in the OCD group.
<br><br>

#### GLMM with PEIA

I included the factor *accuracy of previous trial* in the model, to directly test the difference between post-error vs. post-correct accuracy (post-error increase of accuracy) and the effect of stimulation on it. This was not preregistered. (If preferred, I could also first run the preregistered model with only stimulation, group and stimulus type as fixed factors and then a second model, including additionally the factor *accuracy of previous trial*. This approach would also allow using all trials for the basic accuracy model, as when including the factor , I automatically lose all trials from the calculation at the start of a new task block, that have NA for the variable *accuracy of previous trial*. But the results remain the same, independent of whether I run the model with all trials or the subset). I did not add *accuracy of previous trial* as random effect here. Doing so would require specifying the model matrix separately for the subset of data where *accuracy of previous trial* is not NA.  

```{r GLMM-accuracy-peia, cache = knitr_cache_enabled, fig.width = 5, results = 'asis'}

# Run model with maximal random-effects structure
GLMM_accuracy_PEIA <- glmer(accuracy_numeric ~ verum_sham * OCD_HC * accuracy_prev_trial +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean,
family = binomial,
control = glmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(GLMM_accuracy_PEIA) # Model does converge
# isSingular(GLMM_accuracy_PEIA) # is also ok


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(GLMM_accuracy_PEIA)) # All terms explain variance


# Display results (fixed effects)
tab_model(GLMM_accuracy_PEIA,
  dv.labels = "Accuracy", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "z value", 
  string.ci = "95 % CI", string.p = "p value", wrap.labels = 80, digits.re = 3
)


# Plot interaction stimulation * PEIA
plot_acc_predicted <- plot_model(GLMM_accuracy_PEIA,
                                type   = "pred",
                                terms  = c("accuracy_prev_trial","verum_sham"),
                                ci.lvl = .95) +
  labs(title = "Predicted probabilities of a correct response",
       x     = "Accuracy on the previous trial",
       y     = "Accuracy") +
  my_figure_theme +
  scale_color_manual(name = "Stimulation condition", labels = c("sham","verum"), values = c("slategray3", "navy"))
plot_acc_predicted
```
<br><br>
Accuracy is higher on trials following incorrect responses than on trials following correct responses (post-error increase of accuracy; PEIA). There is no evidence for an effect of group or stimulation. There is a significant interaction stimulation:accuracy_prev_trial. There is more PEIA in the verum than in the sham condition. This is contrary to the finding reported by Reinhart & Woodman (2014) of a *reduced* probability for a correct response following an error in the cathodal relative to the sham condition. Since this would be difficult to explain and this analysis was not preregistered, we will report the simple model, without PEIA.
<br><br>

#### GLMM with stimulus type

```{r GLMM-accuracy-stim-type, cache = knitr_cache_enabled}

# Run model with maximal random-effects structure
GLMM_accuracy_stim_type <- glmer(accuracy_numeric ~ verum_sham * OCD_HC * (incongruent_congruent + accuracy_prev_trial) +
  (1 + verum_sham * incongruent_congruent | participant_id),
data = single_trial_data_clean,
family = binomial,
control = glmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(GLMM_accuracy_stim_type) # Model does converge
# isSingular(GLMM_accuracy_stim_type) # is also ok


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(GLMM_accuracy_stim_type)) # All terms explain variance


# Display results (fixed effects)
tab_model(GLMM_accuracy_stim_type,
  dv.labels = "Accuracy", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "z value", 
  string.ci = "95 % CI", string.p = "p value", wrap.labels = 80, digits.re = 3
)
```
<br><br>
Accuracy is lower in the incongruent condition than in the congruent condition. Furthermore, accuracy is higher on trials following incorrect responses than on trials following correct responses (post-error increase of accuracy). There is no evidence for an effect of group or stimulation. There is a significant interaction group:stimulus_type and a trend for an interaction stimulation:accuracy_prev_trial.
<br><br>

#### GLMM nested with stimulus type

```{r GLMM-accuracy-per-group-stim-type, cache = knitr_cache_enabled}

# Run model with maximal random-effects structure
GLMM_accuracy_group_stim_type <- glmer(accuracy_numeric ~ group/stimulation * (incongruent_congruent + accuracy_prev_trial) +
  (1 + verum_sham * incongruent_congruent | participant_id),
data = single_trial_data_clean,
family = binomial,
control = glmerControl(optimizer = "bobyqa")
)


# Check model output
# summary(GLMM_accuracy_group_stim_type) # Model does converge
# isSingular(GLMM_accuracy_group_stim_type) # is also ok


# Check PCA of random-effects variance-covariance estimates
# summary(rePCA(GLMM_accuracy_group_stim_type)) # All terms explain variance


# Display results (fixed effects)
tab_model(GLMM_accuracy_group_stim_type,
  dv.labels = "Accuracy", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "z value", 
  string.ci = "95 % CI", string.p = "p value", wrap.labels = 80, digits.re = 3
)
```
<br><br>
For the separate groups, there is no significant effect of stimulation on accuracy. There is a significant interaction stimulation:accuracy_prev_trial in the OCD group.
<br><br>

#### Assumption checks

```{r GLMM-accuracy-assumptions}

# Check for normal distributed random effects
print("Check for normal distributed random effects")
performance::check_normality(GLMM_accuracy, effects = "random")


# Check for overdispersion
print("Check for overdispersion")
overdisp_fun <- function(model) {
  rdf <- df.residual(model)
  rp <- residuals(model,type="pearson")
  Pearson.chisq <- sum(rp^2)
  prat <- Pearson.chisq/rdf
  pval <- pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE)
  c(chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)
}
overdisp_fun(GLMM_accuracy) # If the p-value is < 0.05, the data are overdispersed. Here p > 0.05. Overdispersion is not a problem here. 
```

* **Assumption 1: Chosen Link Function is Appropriate  -> Is OK** 
  
* **Assumption 2: Normality of Random Effects -> Is OK**
    + Are random effects approximately normally distributed?
    + Written output says this is ok 

* **Assumption 3: Appropriate Estimation of Variance (No Overdispersion)  -> Is OK**
    + Overdispersion is not a problem here. The empirical variance in data does not exceed the nominal variance under the presumed model. The chi-square test of the ratio of the empirical variance in data and the nominal variance under the presumed model is not significant.
<br><br>

#### Check covariates

For further infos on inclusion of covariates, see notes in section LMM Anylses RT, tab "Check covariates".

```{r GLMM-accuracy-covariates, cache = knitr_cache_enabled}

# Accuracy check covariate session (not included in all interactions for reasons of convergence)
GLMM_accuracy_session <- glmer(accuracy_numeric ~ verum_sham * OCD_HC * (accuracy_prev_trial + session) +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean,
family = binomial,
control = glmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(GLMM_accuracy_session,
  dv.labels = "Accuracy, covariate session", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "z value", 
  string.ci = "95 % CI", string.p = "p value", wrap.labels = 80, digits.re = 3
)


# Accuracy check covariate number of feedback faster (predictor was z standardized) (not included in all interactions for reasons of convergence)
GLMM_accuracy_number_feedback_faster <- glmer(accuracy_numeric ~ verum_sham * OCD_HC * (accuracy_prev_trial + number_feedback_faster_standardized) +
  (1 + verum_sham | participant_id),
data = single_trial_data_clean,
family = binomial,
control = glmerControl(optimizer = "bobyqa")
)
# Convergence, singulatrity, PCA checked, all ok
tab_model(GLMM_accuracy_number_feedback_faster,
  dv.labels = "Accuracy, covariate number of feedback faster", show.stat = TRUE, show.icc = TRUE, show.r2 = TRUE, 
  show.re.var = TRUE, show.ngroups = FALSE, string.est = "b", string.stat = "z value", 
  string.ci = "95 % CI", string.p = "p value", wrap.labels = 80, digits.re = 3
)
```
<br><br>

## ANOVAs {.tabset}
***

### ANOVAs

To facilitate comparison with previously reported results obtained using a similar task and aggregation-based analyses (Rainhart & Woodman 2014), RT, PES, and accuracy were additionally analyzed with Greenhouse–Geisser corrected repeated-measures analyses of variance (ANOVAs) including the within-participant factors stimulation (verum, sham), group (OCD, HC), response type (correct, incorrect)/condition (congruent correct, incongruent correct, incongruent incorrect), and accuracy previous trial (correct, incorrect). (GG correction only relevant for the ANOVA on RT, as factors in all other have only two levels, so that sphericity could not be violated). As can be seen below, the ANOVAs yielded similar results as obtained with mixed-effects modeling with respect to all effects. I additionally analyzed whether there is an stimulation effect on the number of corrective second responses (because this was erroneously stated in the preregistration). There is none. This analysis requires aggregated data, hence I only calculated an ANOVA, no LMM. <br><br>
The ANOVAs will not be reported in the manuscript.

```{r ANOVAs}

# Due to the afex package, contrasts are automatically set to effect-coding (contr.sum). Afex package 
# also checks sphericity assumptions and automatically corrects for any violations if necessary.


# Get data in correct format for ANOVAs (aggregate within participants per condition)
data_anova_rt <- single_trial_data_clean %>%
  # for RT exclude congruent errors -> use factor with 3 levels (correct congr., correct incongr., incorrect incongr.)
  dplyr::filter(
    condition != "congruent_incorrect"
    ) %>%
  # Adjust number of factor levels
  dplyr::mutate(
    condition = as.factor(condition)
    ) %>%
  dplyr::group_by(participant_id, stimulation, group, condition) %>%
  dplyr::summarize(
    rt_log = mean(rt_log, na.rm = TRUE)
  )  %>%
  dplyr::ungroup()


data_anova_pes <- single_trial_data_clean[!is.na(single_trial_data_clean$pes),] %>%
  dplyr::group_by(participant_id, stimulation, group) %>%
  dplyr::summarize(
    pes = mean(pes)
  )  %>%
  dplyr::ungroup()


data_anova_accuracy <- single_trial_data_clean[!is.na(single_trial_data_clean$accuracy_prev_trial),] %>%
  dplyr::group_by(participant_id, stimulation, group, stimulus_type, accuracy_prev_trial) %>%
  dplyr::summarize(
    percentage_correct_responses = sum(accuracy_numeric) / length(participant_id) * 100
  )  %>%
  dplyr::ungroup()


data_anova_number_2nd_resp <- single_trial_data_clean %>%
  dplyr::group_by(participant_id, group, stimulation, session) %>%
  dplyr::summarize(
    number_2nd_resp = sum(!is.na(response_type_2nd[response_type_2nd == "correct"]))
  )  %>%
  dplyr::ungroup()
      

# ANOVA RT
anova_rt <- aov_ez(
  id     = "participant_id", 
  dv     = "rt_log", 
  data   = data_anova_rt,
  within = c("stimulation", "condition"),
  between= c("group"),
  observed = c("group")
)


# ANOVA PES
anova_pes <- aov_ez(
  id     = "participant_id", 
  dv     = "pes", 
  data   = data_anova_pes,
  within = c("stimulation"),
  between= c("group"),
  observed = c("group")
)


# ANOVA accuracy
anova_accuracy <- aov_ez(
  id     = "participant_id", 
  dv     = "percentage_correct_responses", 
  data   = data_anova_accuracy,
  within = c("stimulation", "accuracy_prev_trial"),
  between= c("group"),
  observed = c("group")
)


# ANOVA number 2nd responses
anova_number_2nd_resp <- aov_ez(
  id     = "participant_id", 
  dv     = "number_2nd_resp", 
  data   = data_anova_number_2nd_resp,
  within = c("stimulation"),
  between= c("group"),
  observed = c("group")
)


# Display ANOVA results
my_table_template(nice(anova_rt,       MSE = FALSE), caption = "RT (log)")
my_table_template(nice(anova_pes,      MSE = FALSE), caption = "PES")                  
my_table_template(nice(anova_accuracy, MSE = FALSE), caption = "Accuracy")
my_table_template(nice(anova_number_2nd_resp, MSE = FALSE), caption = "Number of corrective second responses (only correct)")
```
<br><br>

### Pairwise comparisons

Significant main effects and interactions were followed up with pairwise comparisons using the emmeans package with Holm–Bonferroni *p* value adjustments.

```{r ANOVAs-pairwise-tests}

# Use multivariate model for all follow-up tests to adequately control for violations of sphericity
afex_options(emmeans_model = "multivariate")


# Pairwise t tests
pairwise_rt  <- summary(pairs(emmeans(anova_rt, ~condition), adjust = "holm"))


# Add Cohen's dz (CIs for d could be added if needed, as it can be returned by the "t_to_d" function)
pairwise_rt$cohens_dz  <- round(t_to_d(pairwise_rt$t.ratio,  pairwise_rt$df,  paired = TRUE)[1], digits = 2)


# Display results 
my_table_template(pairwise_rt, 
                  digits = c(0, 2, 2, 0, 2, 4, 2),
                  caption = "RT: Main Effect Response Type",
                  footnote = "P values are adjusted with Holm–Bonferroni method.") 
```
<br><br>

### Assumption checks

```{r ANOVAs-assumptions}

# Get residuals
residuals_anova_rt       <- as.data.frame(anova_rt$lm$residuals)
residuals_anova_pes      <- as.data.frame(anova_pes$lm$residuals)
residuals_anova_accuracy <- as.data.frame(anova_accuracy$lm$residuals)


# Plot residuals
ggplot(gather(residuals_anova_rt, cols, value), aes(x = value)) + 
  geom_histogram(color = my_figure_colors[2], fill = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "Histogram Residuals RT") + my_figure_theme

ggplot(gather(residuals_anova_rt, cols, value), aes(sample = value)) + 
  stat_qq(color = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "QQ-Plot Residuals RT") + my_figure_theme

ggplot(gather(residuals_anova_pes, cols, value), aes(x = value)) + 
  geom_histogram(color = my_figure_colors[2], fill = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "Histogram Residuals PES") + my_figure_theme

ggplot(gather(residuals_anova_pes, cols, value), aes(sample = value)) + 
  stat_qq(color = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "QQ-Plot Residuals PES") + my_figure_theme

ggplot(gather(residuals_anova_accuracy, cols, value), aes(x = value)) + 
  geom_histogram(color = my_figure_colors[2], fill = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "Histogram Residuals Accuracy") + my_figure_theme

ggplot(gather(residuals_anova_accuracy, cols, value), aes(sample = value)) + 
  stat_qq(color = my_figure_colors[1]) + 
  facet_wrap(.~cols) + labs(title = "QQ-Plot Residuals Accuracy") + my_figure_theme


# Test normality of residuals
print("# ANOVA RT: Check Normality of Residuals with Shapiro Wilk Test")
do.call(rbind, lapply(residuals_anova_rt[,], function(x) shapiro.test(x)["p.value"]))

print("# ANOVA PES: Check Normality of Residuals with Shapiro Wilk Test")
do.call(rbind, lapply(residuals_anova_pes[,], function(x) shapiro.test(x)["p.value"]))

print("# ANOVA Accuracy: Check Normality of Residuals with Shapiro Wilk Test")
do.call(rbind, lapply(residuals_anova_accuracy[,], function(x) shapiro.test(x)["p.value"]))
```

* **Assumption #1: Dependent variable interval or ratio variable -> Is OK**

* **Assumption #2: Balanced design** (each participant has to have a value in each condition) **-> Is OK**

* **Assumption #3: No dependency in the scores between participants** (dependency can exist only across scores for individuals) **-> Is OK**

* **Assumption #4: Residuals** of the dependent variable in each level of the within-participants factor are approximately **normally distributed -> NOT OK** (see above; but for RT and PES only small deviation)**

* **Assumption #5: Sphericity** (only relevant for within-participant factors with > 2 levels) (afex package automatically corrects (Greenhouse Geisser) for any violations if necessary) **-> Is OK**
<br><br>